#!/usr/bin/env python3
"""
Optimized STGCN++ Violence Detection Pipeline - Test Script
ìµœì í™”ëœ ì‹¸ì›€ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python test_optimized_pipeline.py --mode single --video /path/to/video.mp4
    python test_optimized_pipeline.py --mode batch --input /path/to/videos/
    python test_optimized_pipeline.py --mode benchmark --input /path/to/test_videos/
"""

import sys
import os
import os.path as osp
import time
import argparse
import json
from pathlib import Path
from typing import List, Dict

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from optimized_violence_pipeline import OptimizedSTGCNPipeline
from pipeline_config import *


def single_video_test(pipeline: OptimizedSTGCNPipeline, video_path: str, output_dir: str) -> Dict:
    """ë‹¨ì¼ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ¬ ë‹¨ì¼ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸: {osp.basename(video_path)}")
    
    start_time = time.time()
    result = pipeline.process_video_optimized(video_path, output_dir)
    total_time = time.time() - start_time
    
    if 'error' not in result:
        print(f"""
âœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼:
   - ë¹„ë””ì˜¤: {osp.basename(video_path)}
   - ì˜ˆì¸¡: {result['prediction_label']}
   - ì‹ ë¢°ë„: {result['confidence']:.4f}
   - ì´ í”„ë ˆì„: {result['total_frames']}
   - ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ
   - FPS: {result['fps']:.2f}
        """)
    else:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result['error']}")
    
    return result


def batch_video_test(pipeline: OptimizedSTGCNPipeline, input_dir: str, output_dir: str, 
                    max_videos: int = 10) -> List[Dict]:
    """ë°°ì¹˜ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸"""
    print(f"ğŸ“ ë°°ì¹˜ ë¹„ë””ì˜¤ í…ŒìŠ¤íŠ¸: {input_dir}")
    
    # ë¹„ë””ì˜¤ íŒŒì¼ ìˆ˜ì§‘
    video_paths = []
    for ext in ['*.mp4', '*.avi', '*.mov', '*.mkv']:
        video_paths.extend(Path(input_dir).rglob(ext))
    
    video_paths = [str(p) for p in video_paths[:max_videos]]
    
    if not video_paths:
        print("âŒ í…ŒìŠ¤íŠ¸í•  ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    print(f"ğŸ“‹ ì´ {len(video_paths)}ê°œ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì˜ˆì •")
    
    start_time = time.time()
    results = pipeline.process_batch_videos(video_paths, output_dir, max_workers=4)
    total_time = time.time() - start_time
    
    # ê²°ê³¼ ë¶„ì„
    successful = [r for r in results if 'error' not in r]
    failed = [r for r in results if 'error' in r]
    
    fight_videos = [r for r in successful if r['prediction'] == 1]
    nonfight_videos = [r for r in successful if r['prediction'] == 0]
    
    avg_confidence = sum(r['confidence'] for r in successful) / len(successful) if successful else 0
    avg_fps = sum(r['fps'] for r in successful) / len(successful) if successful else 0
    
    print(f"""
ğŸ“Š ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:
   - ì´ ë¹„ë””ì˜¤: {len(video_paths)}ê°œ
   - ì„±ê³µ: {len(successful)}ê°œ
   - ì‹¤íŒ¨: {len(failed)}ê°œ
   - Fight ë¶„ë¥˜: {len(fight_videos)}ê°œ
   - NonFight ë¶„ë¥˜: {len(nonfight_videos)}ê°œ
   - í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.4f}
   - í‰ê·  FPS: {avg_fps:.2f}
   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ
   - ë¹„ë””ì˜¤ë‹¹ í‰ê·  ì‹œê°„: {total_time/len(video_paths):.2f}ì´ˆ
    """)
    
    return results


def benchmark_test(pipeline: OptimizedSTGCNPipeline, test_dir: str, output_dir: str) -> Dict:
    """ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ (ì •í™•ë„ í‰ê°€)"""
    print(f"ğŸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸: {test_dir}")
    
    # Fightì™€ NonFight ë””ë ‰í† ë¦¬ ê²€ìƒ‰
    fight_dir = osp.join(test_dir, "Fight")
    nonfight_dir = osp.join(test_dir, "NonFight")
    
    fight_videos = []
    nonfight_videos = []
    
    if osp.exists(fight_dir):
        for ext in ['*.mp4', '*.avi', '*.mov', '*.mkv']:
            fight_videos.extend(Path(fight_dir).glob(ext))
    
    if osp.exists(nonfight_dir):
        for ext in ['*.mp4', '*.avi', '*.mov', '*.mkv']:
            nonfight_videos.extend(Path(nonfight_dir).glob(ext))
    
    fight_videos = [str(p) for p in fight_videos[:20]]  # ìµœëŒ€ 20ê°œì”©
    nonfight_videos = [str(p) for p in nonfight_videos[:20]]
    
    all_videos = fight_videos + nonfight_videos
    ground_truths = [1] * len(fight_videos) + [0] * len(nonfight_videos)
    
    if not all_videos:
        print("âŒ í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    print(f"ğŸ“‹ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°: Fight {len(fight_videos)}ê°œ, NonFight {len(nonfight_videos)}ê°œ")
    
    # ë°°ì¹˜ ì²˜ë¦¬
    results = pipeline.process_batch_videos(all_videos, output_dir, max_workers=4)
    
    # ì •í™•ë„ ê³„ì‚°
    successful_results = [r for r in results if 'error' not in r]
    predictions = [r['prediction'] for r in successful_results]
    confidences = [r['confidence'] for r in successful_results]
    
    if len(successful_results) != len(ground_truths):
        print(f"âš ï¸ ì¼ë¶€ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {len(ground_truths) - len(successful_results)}ê°œ")
        ground_truths = ground_truths[:len(successful_results)]
    
    # í˜¼ë™ í–‰ë ¬ ê³„ì‚°
    tp = sum(1 for p, gt in zip(predictions, ground_truths) if p == 1 and gt == 1)
    tn = sum(1 for p, gt in zip(predictions, ground_truths) if p == 0 and gt == 0)
    fp = sum(1 for p, gt in zip(predictions, ground_truths) if p == 1 and gt == 0)
    fn = sum(1 for p, gt in zip(predictions, ground_truths) if p == 0 and gt == 1)
    
    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    benchmark_result = {
        'total_videos': len(all_videos),
        'successful': len(successful_results),
        'confusion_matrix': {'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn},
        'metrics': {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score
        },
        'avg_confidence': sum(confidences) / len(confidences) if confidences else 0
    }
    
    print(f"""
ğŸ¯ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼:
   - ì´ ë¹„ë””ì˜¤: {len(all_videos)}ê°œ
   - ì²˜ë¦¬ ì„±ê³µ: {len(successful_results)}ê°œ
   
   í˜¼ë™ í–‰ë ¬:
   - True Positive (Fight â†’ Fight): {tp}
   - True Negative (NonFight â†’ NonFight): {tn}
   - False Positive (NonFight â†’ Fight): {fp}
   - False Negative (Fight â†’ NonFight): {fn}
   
   ì„±ëŠ¥ ì§€í‘œ:
   - ì •í™•ë„ (Accuracy): {accuracy:.4f}
   - ì •ë°€ë„ (Precision): {precision:.4f}
   - ì¬í˜„ìœ¨ (Recall): {recall:.4f}
   - F1 ì ìˆ˜: {f1_score:.4f}
   - í‰ê·  ì‹ ë¢°ë„: {benchmark_result['avg_confidence']:.4f}
    """)
    
    # ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
    benchmark_path = osp.join(output_dir, 'benchmark_results.json')
    with open(benchmark_path, 'w') as f:
        json.dump(benchmark_result, f, indent=2)
    
    print(f"ğŸ’¾ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {benchmark_path}")
    
    return benchmark_result


def compare_with_baseline(baseline_results_path: str, new_results: Dict):
    """ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ì„±ëŠ¥ ë¹„êµ"""
    if not osp.exists(baseline_results_path):
        print(f"âš ï¸ ê¸°ì¤€ ê²°ê³¼ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {baseline_results_path}")
        return
    
    try:
        with open(baseline_results_path, 'r') as f:
            baseline = json.load(f)
        
        print(f"""
ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ (ê¸°ì¡´ vs ê°œì„ ):
   - ì •í™•ë„: {baseline['metrics']['accuracy']:.4f} â†’ {new_results['metrics']['accuracy']:.4f} 
     ({new_results['metrics']['accuracy'] - baseline['metrics']['accuracy']:+.4f})
   - ì •ë°€ë„: {baseline['metrics']['precision']:.4f} â†’ {new_results['metrics']['precision']:.4f} 
     ({new_results['metrics']['precision'] - baseline['metrics']['precision']:+.4f})
   - ì¬í˜„ìœ¨: {baseline['metrics']['recall']:.4f} â†’ {new_results['metrics']['recall']:.4f} 
     ({new_results['metrics']['recall'] - baseline['metrics']['recall']:+.4f})
   - F1 ì ìˆ˜: {baseline['metrics']['f1_score']:.4f} â†’ {new_results['metrics']['f1_score']:.4f} 
     ({new_results['metrics']['f1_score'] - baseline['metrics']['f1_score']:+.4f})
        """)
        
    except Exception as e:
        print(f"âŒ ê¸°ì¤€ ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")


def main():
    parser = argparse.ArgumentParser(description='Optimized Violence Detection Pipeline Test')
    parser.add_argument('--mode', choices=['single', 'batch', 'benchmark'], required=True,
                       help='Test mode')
    parser.add_argument('--video', help='Single video path (for single mode)')
    parser.add_argument('--input', help='Input directory path (for batch/benchmark mode)')
    parser.add_argument('--output', default=OUTPUT_DIR, help='Output directory')
    parser.add_argument('--max-videos', type=int, default=10, 
                       help='Maximum videos to process (for batch mode)')
    parser.add_argument('--baseline', help='Baseline results file for comparison')
    
    args = parser.parse_args()
    
    # ì„¤ì • ê²€ì¦
    print("ğŸ”§ íŒŒì´í”„ë¼ì¸ ì„¤ì • ê²€ì¦ ì¤‘...")
    if not validate_paths():
        print("âŒ í•„ìˆ˜ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    check_gpu_memory()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(args.output, exist_ok=True)
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    print("ğŸš€ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    pipeline = OptimizedSTGCNPipeline(
        pose_config=POSE_CONFIG,
        pose_checkpoint=POSE_CHECKPOINT,
        gcn_config=GCN_CONFIG,
        gcn_checkpoint=GCN_CHECKPOINT,
        device=PIPELINE_CONFIG['device'],
        sequence_length=PIPELINE_CONFIG['sequence_length']
    )
    
    try:
        if args.mode == 'single':
            if not args.video:
                print("âŒ ë‹¨ì¼ ëª¨ë“œì—ì„œëŠ” --video ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return
            
            single_video_test(pipeline, args.video, args.output)
            
        elif args.mode == 'batch':
            if not args.input:
                print("âŒ ë°°ì¹˜ ëª¨ë“œì—ì„œëŠ” --input ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return
            
            batch_video_test(pipeline, args.input, args.output, args.max_videos)
            
        elif args.mode == 'benchmark':
            if not args.input:
                print("âŒ ë²¤ì¹˜ë§ˆí¬ ëª¨ë“œì—ì„œëŠ” --input ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                return
            
            benchmark_results = benchmark_test(pipeline, args.input, args.output)
            
            # ê¸°ì¡´ ì‹œìŠ¤í…œê³¼ ë¹„êµ
            if args.baseline:
                compare_with_baseline(args.baseline, benchmark_results)
        
        # ìµœì¢… ì„±ëŠ¥ í†µê³„
        stats = pipeline.get_performance_stats()
        print(f"""
âš¡ ìµœì¢… ì„±ëŠ¥ í†µê³„:
   - ì´ ì²˜ë¦¬ í”„ë ˆì„: {stats['total_frames']}
   - í‰ê·  FPS: {stats.get('fps', 0):.2f}
   - GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats.get('gpu_memory_allocated', 0) / 1024**3:.2f} GB
        """)
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        pipeline.cleanup()


if __name__ == '__main__':
    main()