# STGCN++ Violence Detection - ê³ ê¸‰ ì‚¬ìš©ì ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ì•„í‚¤í…ì²˜ ì‹¬í™”](#ì•„í‚¤í…ì²˜-ì‹¬í™”)
2. [Fight-ìš°ì„  íŠ¸ë˜í‚¹ ì•Œê³ ë¦¬ì¦˜](#fight-ìš°ì„ -íŠ¸ë˜í‚¹-ì•Œê³ ë¦¬ì¦˜)
3. [ì„±ëŠ¥ ìµœì í™”](#ì„±ëŠ¥-ìµœì í™”)
4. [ì»¤ìŠ¤í„°ë§ˆì´ì§•](#ì»¤ìŠ¤í„°ë§ˆì´ì§•)
5. [ì‹¤ì‹œê°„ ì²˜ë¦¬](#ì‹¤ì‹œê°„-ì²˜ë¦¬)
6. [ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬](#ëŒ€ìš©ëŸ‰-ë°ì´í„°-ì²˜ë¦¬)
7. [ëª¨ë¸ íŒŒì¸íŠœë‹](#ëª¨ë¸-íŒŒì¸íŠœë‹)

---

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì‹¬í™”

### ì „ì²´ ì‹œìŠ¤í…œ êµ¬ì¡°

```mermaid
graph TD
    A[ë¹„ë””ì˜¤ ì…ë ¥] --> B[RTMO í¬ì¦ˆ ì¶”ì •]
    B --> C[Fight-ìš°ì„  íŠ¸ë˜í‚¹]
    C --> D[5ì˜ì—­ ë¶„í• ]
    C --> E[ë³µí•© ì ìˆ˜ ê³„ì‚°]
    D --> F[STGCN++ ë¶„ë¥˜]
    E --> F
    F --> G[ìœˆë„ìš° ê¸°ë°˜ ì¶”ë¡ ]
    G --> H[ì‹ ë¢°ë„ ê°€ì¤‘ íˆ¬í‘œ]
    H --> I[ìµœì¢… ì˜ˆì¸¡]
    I --> J[ì„±ëŠ¥ ë©”íŠ¸ë¦­]
    I --> K[ì˜¤ë²„ë ˆì´ ìƒì„±]
```

### ë°ì´í„° í”Œë¡œìš° ë¶„ì„

#### 1. í¬ì¦ˆ ì¶”ì • ë‹¨ê³„
```python
# Input: ë¹„ë””ì˜¤ í”„ë ˆì„ (H, W, 3)
# Process: RTMO ë‹¤ì¤‘ ì¸ë¬¼ í¬ì¦ˆ ì¶”ì •
# Output: [(N, 17, 2), (N, 17)] per frame
#         N: ê²€ì¶œëœ ì¸ë¬¼ ìˆ˜, 17: COCO í‚¤í¬ì¸íŠ¸, 2: XY ì¢Œí‘œ

pose_results = []
for frame in video_frames:
    keypoints, scores = rtmo_model.predict(frame)
    pose_results.append((keypoints, scores))
```

#### 2. Fight-ìš°ì„  íŠ¸ë˜í‚¹ ë‹¨ê³„
```python
# Input: í”„ë ˆì„ë³„ í¬ì¦ˆ ê²°ê³¼
# Process: 5ì˜ì—­ ë¶„í•  + ë³µí•© ì ìˆ˜ ê³„ì‚°
# Output: ìµœê³  ìš°ì„ ìˆœìœ„ ì¸ë¬¼ ì‹œí€€ìŠ¤ (T, 17, 2)

for frame_result in pose_results:
    keypoints_list, scores_list = frame_result
    
    # ë³µí•© ì ìˆ˜ ê³„ì‚°
    composite_scores = tracker.calculate_composite_scores(
        keypoints_list, scores_list
    )
    
    # ìµœê³  ì ìˆ˜ ì¸ë¬¼ ì„ íƒ
    best_idx = np.argmax(composite_scores)
    selected_keypoints = keypoints_list[best_idx]
```

#### 3. STGCN++ ë¶„ë¥˜ ë‹¨ê³„
```python
# Input: í‚¤í¬ì¸íŠ¸ ì‹œí€€ìŠ¤ (T, 17, 2)
# Process: ìœˆë„ìš° ê¸°ë°˜ ì‹œê³µê°„ ê·¸ë˜í”„ ë¶„ì„
# Output: Fight/NonFight ì˜ˆì¸¡ + ì‹ ë¢°ë„

window_predictions = []
for start_idx in range(0, len(sequence), stride):
    window = sequence[start_idx:start_idx+window_size]
    prediction, confidence = stgcn_model.predict(window)
    window_predictions.append((prediction, confidence))

# ì‹ ë¢°ë„ ê°€ì¤‘ íˆ¬í‘œ
final_prediction = weighted_majority_vote(window_predictions)
```

---

## ğŸ¯ Fight-ìš°ì„  íŠ¸ë˜í‚¹ ì•Œê³ ë¦¬ì¦˜

### 5ì˜ì—­ ë¶„í•  ì‹œìŠ¤í…œ

#### ì˜ì—­ ì •ì˜
```python
def define_regions(frame_width, frame_height):
    w, h = frame_width, frame_height
    
    regions = {
        # ì „ì²´ 4ë¶„í•  (ì™„ì „í•œ ê³µê°„ ì»¤ë²„ë¦¬ì§€)
        'top_left': (0, 0, w//2, h//2),              # 25% ì˜ì—­
        'top_right': (w//2, 0, w, h//2),             # 25% ì˜ì—­
        'bottom_left': (0, h//2, w//2, h),           # 25% ì˜ì—­
        'bottom_right': (w//2, h//2, w, h),          # 25% ì˜ì—­
        
        # ì¤‘ì•™ ì§‘ì¤‘ ì˜ì—­ (ê°€ì¥ ì¤‘ìš”)
        'center': (w//4, h//4, 3*w//4, 3*h//4)       # ì¤‘ì•™ 50% ì˜ì—­
    }
    
    return regions
```

#### ì˜ì—­ë³„ ê°€ì¤‘ì¹˜ ì „ëµ

```python
# ê¸°ë³¸ ì „ëµ: ì¤‘ì•™ ì§‘ì¤‘
region_weights_center_focused = {
    'center': 1.0,         # ìµœê³  ìš°ì„ ìˆœìœ„
    'top_left': 0.7,       # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
    'top_right': 0.7,      # ì¤‘ê°„ ìš°ì„ ìˆœìœ„
    'bottom_left': 0.6,    # ë‚®ì€ ìš°ì„ ìˆœìœ„
    'bottom_right': 0.6    # ë‚®ì€ ìš°ì„ ìˆœìœ„
}

# ê· ë“± ì „ëµ: ì „ì²´ ì˜ì—­ ë™ë“±
region_weights_balanced = {
    'center': 0.8,
    'top_left': 0.8,
    'top_right': 0.8,
    'bottom_left': 0.8,
    'bottom_right': 0.8
}

# ìƒë‹¨ ì§‘ì¤‘: ìƒì²´ ì¤‘ì‹¬ ë¶„ì„
region_weights_upper_focused = {
    'center': 1.0,
    'top_left': 0.9,
    'top_right': 0.9,
    'bottom_left': 0.5,
    'bottom_right': 0.5
}
```

### ë³µí•© ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜

#### 1. ìœ„ì¹˜ ì ìˆ˜ (Position Score)
```python
def calculate_position_score(keypoints, regions, region_weights):
    """ì¸ë¬¼ì˜ ì˜ì—­ë³„ ìœ„ì¹˜ ì ìˆ˜ ê³„ì‚°"""
    
    # ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
    valid_points = keypoints[keypoints[:, 0] > 0]
    if len(valid_points) == 0:
        return 0.0
    
    person_center = np.mean(valid_points, axis=0)
    
    # ê° ì˜ì—­ì—ì„œì˜ ì ìˆ˜ ê³„ì‚°
    region_scores = {}
    for region_name, (x1, y1, x2, y2) in regions.items():
        if x1 <= person_center[0] <= x2 and y1 <= person_center[1] <= y2:
            # ì˜ì—­ ì¤‘ì‹¬ì—ì„œì˜ ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜
            region_center = np.array([(x1+x2)/2, (y1+y2)/2])
            distance = np.linalg.norm(person_center - region_center)
            max_distance = np.linalg.norm([x2-x1, y2-y1]) / 2
            
            # ê±°ë¦¬ ê¸°ë°˜ ì ìˆ˜ (ì¤‘ì‹¬ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ìŒ)
            distance_score = max(0.5, 1.0 - (distance / max_distance) * 0.5)
            region_scores[region_name] = distance_score * region_weights[region_name]
        else:
            region_scores[region_name] = 0.0
    
    return max(region_scores.values())
```

#### 2. ì›€ì§ì„ ì ìˆ˜ (Movement Score)
```python
def calculate_movement_score(current_keypoints, previous_positions):
    """ë™ì‘ì˜ ê²©ë ¬í•¨ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°"""
    
    if len(previous_positions) < 2:
        return 0.5  # ê¸°ë³¸ê°’
    
    # í˜„ì¬ ìœ„ì¹˜ ê³„ì‚°
    valid_points = current_keypoints[current_keypoints[:, 0] > 0]
    current_pos = np.mean(valid_points, axis=0) if len(valid_points) > 0 else np.array([0, 0])
    
    # ì´ì „ ìœ„ì¹˜ë“¤ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°
    movements = []
    for prev_pos in previous_positions[-5:]:  # ìµœê·¼ 5í”„ë ˆì„
        movement = np.linalg.norm(current_pos - prev_pos)
        movements.append(movement)
    
    # í‰ê·  ì›€ì§ì„ ì •ê·œí™” (0-1 ë²”ìœ„)
    avg_movement = np.mean(movements)
    movement_score = min(1.0, avg_movement / 50.0)  # 50í”½ì…€ì„ ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì •
    
    return movement_score
```

#### 3. ìƒí˜¸ì‘ìš© ì ìˆ˜ (Interaction Score)
```python
def calculate_interaction_score(person_keypoints, all_keypoints_list):
    """ì¸ë¬¼ ê°„ ìƒí˜¸ì‘ìš© ê°•ë„ ê³„ì‚°"""
    
    if len(all_keypoints_list) < 2:
        return 0.0  # ë‹¨ì¼ ì¸ë¬¼
    
    person_center = np.mean(person_keypoints[person_keypoints[:, 0] > 0], axis=0)
    max_interaction = 0.0
    
    for other_keypoints in all_keypoints_list:
        if np.array_equal(person_keypoints, other_keypoints):
            continue  # ìê¸° ìì‹  ì œì™¸
        
        other_center = np.mean(other_keypoints[other_keypoints[:, 0] > 0], axis=0)
        distance = np.linalg.norm(person_center - other_center)
        
        # ê±°ë¦¬ ê¸°ë°˜ ìƒí˜¸ì‘ìš© ì ìˆ˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ìŒ)
        if distance > 0:
            interaction = max(0.0, 1.0 - (distance / 150.0))  # 150í”½ì…€ ì„ê³„ê°’
            max_interaction = max(max_interaction, interaction)
    
    return max_interaction
```

#### 4. ê²€ì¶œ ì‹ ë¢°ë„ ì ìˆ˜ (Detection Score)
```python
def calculate_detection_score(keypoint_scores):
    """í‚¤í¬ì¸íŠ¸ ê²€ì¶œ í’ˆì§ˆ ì ìˆ˜"""
    
    valid_scores = keypoint_scores[keypoint_scores > 0]
    if len(valid_scores) == 0:
        return 0.0
    
    # í‰ê·  ì‹ ë¢°ë„ì™€ ìœ íš¨ í‚¤í¬ì¸íŠ¸ ë¹„ìœ¨ ì¡°í•©
    avg_confidence = np.mean(valid_scores)
    valid_ratio = len(valid_scores) / len(keypoint_scores)
    
    detection_score = (avg_confidence * 0.7) + (valid_ratio * 0.3)
    return detection_score
```

#### 5. ì‹œê°„ì  ì¼ê´€ì„± ì ìˆ˜ (Consistency Score)
```python
def calculate_consistency_score(recent_composite_scores):
    """ìµœê·¼ í”„ë ˆì„ë“¤ì—ì„œì˜ ì ìˆ˜ ì¼ê´€ì„±"""
    
    if len(recent_composite_scores) < 3:
        return 0.5  # ê¸°ë³¸ê°’
    
    # í‘œì¤€í¸ì°¨ì˜ ì—­ìˆ˜ë¡œ ì¼ê´€ì„± ì¸¡ì •
    std_dev = np.std(recent_composite_scores)
    consistency = 1.0 / (1.0 + std_dev)
    
    return consistency
```

### ìµœì¢… ë³µí•© ì ìˆ˜ í†µí•©

```python
def calculate_final_composite_score(keypoints, scores, context, weights):
    """ëª¨ë“  ì ìˆ˜ë¥¼ ê°€ì¤‘í•©ìœ¼ë¡œ í†µí•©"""
    
    position_score = calculate_position_score(keypoints, context['regions'], context['region_weights'])
    movement_score = calculate_movement_score(keypoints, context['previous_positions'])
    interaction_score = calculate_interaction_score(keypoints, context['all_keypoints'])
    detection_score = calculate_detection_score(scores)
    consistency_score = calculate_consistency_score(context['recent_scores'])
    
    final_score = (
        position_score * weights['position'] +
        movement_score * weights['movement'] +
        interaction_score * weights['interaction'] +
        detection_score * weights['detection'] +
        consistency_score * weights['consistency']
    )
    
    return final_score
```

---

## ğŸš€ ì„±ëŠ¥ ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ìµœì í™”

#### 1. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
def optimize_batch_processing():
    """GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°°ì¹˜ ì²˜ë¦¬"""
    
    # GPU ë©”ëª¨ë¦¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§
    import torch
    
    def get_gpu_memory():
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / 1024**3  # GB ë‹¨ìœ„
        return 0
    
    # ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
    initial_batch_size = 8
    current_batch_size = initial_batch_size
    
    for batch in video_batches:
        try:
            # í˜„ì¬ ë°°ì¹˜ í¬ê¸°ë¡œ ì²˜ë¦¬ ì‹œë„
            result = process_batch(batch[:current_batch_size])
            
            # ì„±ê³µ ì‹œ ë°°ì¹˜ í¬ê¸° ì ì§„ì  ì¦ê°€
            if len(batch) > current_batch_size:
                current_batch_size = min(current_batch_size + 1, 16)
                
        except RuntimeError as e:
            if "out of memory" in str(e):
                # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ê°ì†Œ
                current_batch_size = max(current_batch_size - 2, 1)
                torch.cuda.empty_cache()
                continue
            else:
                raise e
```

#### 2. ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³µìœ 
```python
class OptimizedPipeline:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        # ëª¨ë¸ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
        self.pose_model = None
        self.gcn_model = None
        
    def load_pose_model(self):
        if self.pose_model is None:
            self.pose_model = RTMOPoseEstimator(...)
            
    def unload_pose_model(self):
        if self.pose_model is not None:
            del self.pose_model
            self.pose_model = None
            torch.cuda.empty_cache()
            
    def process_with_memory_management(self, video_path):
        # 1. í¬ì¦ˆ ì¶”ì • ë‹¨ê³„
        self.load_pose_model()
        pose_results = self.pose_model.estimate_poses_from_video(video_path)
        self.unload_pose_model()
        
        # 2. íŠ¸ë˜í‚¹ ë‹¨ê³„ (CPUì—ì„œ ì§„í–‰)
        tracker_results = self.tracker.process_video_sequence(pose_results)
        
        # 3. ë¶„ë¥˜ ë‹¨ê³„
        self.load_gcn_model()
        classification_result = self.gcn_model.classify_video_sequence(tracker_results)
        self.unload_gcn_model()
        
        return classification_result
```

### CPU ë³‘ë ¬ ì²˜ë¦¬

#### 1. ë©€í‹°í”„ë¡œì„¸ì‹± í™œìš©
```python
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor

def process_video_parallel(video_paths, num_workers=4):
    """ë©€í‹°í”„ë¡œì„¸ì‹± ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬"""
    
    def process_single_video_worker(video_path):
        # ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ë…ë¦½ì ì¸ íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipeline = EndToEndPipeline(...)
        result = pipeline.process_single_video(video_path)
        pipeline.cleanup()
        return result
    
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        futures = [executor.submit(process_single_video_worker, path) 
                  for path in video_paths]
        
        results = []
        for future in futures:
            try:
                result = future.result(timeout=300)  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                results.append(result)
            except Exception as e:
                logger.error(f"ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                results.append(None)
    
    return results
```

#### 2. ìŠ¤ë ˆë“œ ê¸°ë°˜ I/O ìµœì í™”
```python
import threading
from queue import Queue

class AsyncVideoProcessor:
    """ë¹„ë™ê¸° ë¹„ë””ì˜¤ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, pipeline, max_queue_size=10):
        self.pipeline = pipeline
        self.input_queue = Queue(maxsize=max_queue_size)
        self.output_queue = Queue()
        self.processing_thread = None
        self.running = False
        
    def start_processing(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ì‹œì‘"""
        self.running = True
        self.processing_thread = threading.Thread(target=self._process_loop)
        self.processing_thread.start()
        
    def _process_loop(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ ë£¨í”„"""
        while self.running:
            try:
                video_path = self.input_queue.get(timeout=1)
                result = self.pipeline.process_single_video(video_path)
                self.output_queue.put((video_path, result))
                self.input_queue.task_done()
            except:
                continue
                
    def add_video(self, video_path):
        """ì²˜ë¦¬í•  ë¹„ë””ì˜¤ ì¶”ê°€"""
        self.input_queue.put(video_path)
        
    def get_result(self, timeout=None):
        """ì²˜ë¦¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°"""
        return self.output_queue.get(timeout=timeout)
```

---

## ğŸ”§ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### Fight-ìš°ì„  íŠ¸ë˜í‚¹ ì»¤ìŠ¤í„°ë§ˆì´ì§•

#### 1. ë„ë©”ì¸ë³„ ì˜ì—­ ê°€ì¤‘ì¹˜ ì„¤ì •

```python
# ì‹¤ë‚´ CCTV í™˜ê²½ (ì¤‘ì•™ ì§‘ì¤‘)
indoor_weights = {
    'center': 1.2,
    'top_left': 0.6,
    'top_right': 0.6,
    'bottom_left': 0.5,
    'bottom_right': 0.5
}

# ì•¼ì™¸ ê´‘ì¥ í™˜ê²½ (ê· ë“± ë¶„ì‚°)
outdoor_weights = {
    'center': 0.9,
    'top_left': 0.8,
    'top_right': 0.8,
    'bottom_left': 0.8,
    'bottom_right': 0.8
}

# ë³µë„/í†µë¡œ í™˜ê²½ (ìˆ˜ì§ ì¤‘ì‹¬)
corridor_weights = {
    'center': 1.0,
    'top_left': 0.7,
    'top_right': 0.7,
    'bottom_left': 0.7,
    'bottom_right': 0.7
}
```

#### 2. ìƒí™©ë³„ ë³µí•© ì ìˆ˜ ê°€ì¤‘ì¹˜

```python
# í­ë ¥ ì˜ˆë°© ì¤‘ì‹¬ (ë†’ì€ ë¯¼ê°ë„)
prevention_weights = {
    'position': 0.2,
    'movement': 0.3,      # ì›€ì§ì„ ì¤‘ì‹œ
    'interaction': 0.35,   # ìƒí˜¸ì‘ìš© ì¤‘ì‹œ
    'detection': 0.1,
    'consistency': 0.05
}

# ì •í™•ë„ ì¤‘ì‹¬ (ë‚®ì€ ì˜¤íƒë¥ )
accuracy_weights = {
    'position': 0.4,      # ìœ„ì¹˜ ì¤‘ì‹œ
    'movement': 0.2,
    'interaction': 0.2,
    'detection': 0.15,    # ê²€ì¶œ í’ˆì§ˆ ì¤‘ì‹œ
    'consistency': 0.05
}

# ì‹¤ì‹œê°„ ì¤‘ì‹¬ (ë¹ ë¥¸ ë°˜ì‘)
realtime_weights = {
    'position': 0.35,
    'movement': 0.35,     # ì¦‰ì‹œ ê°ì§€
    'interaction': 0.2,
    'detection': 0.05,
    'consistency': 0.05   # ì¼ê´€ì„± ëœ ì¤‘ì‹œ
}
```

### STGCN++ ë¶„ë¥˜ ì»¤ìŠ¤í„°ë§ˆì´ì§•

#### 1. ìœˆë„ìš° ì „ëµ ìµœì í™”

```python
class AdaptiveWindowClassifier:
    """ì ì‘ì  ìœˆë„ìš° í¬ê¸° ë¶„ë¥˜ê¸°"""
    
    def __init__(self, base_window_size=30):
        self.base_window_size = base_window_size
        
    def classify_with_adaptive_windows(self, keypoints, scores):
        """ì›€ì§ì„ ê°•ë„ì— ë”°ë¥¸ ì ì‘ì  ìœˆë„ìš° í¬ê¸°"""
        
        # ì›€ì§ì„ ê°•ë„ ê³„ì‚°
        movement_intensity = self.calculate_movement_intensity(keypoints)
        
        # ìœˆë„ìš° í¬ê¸° ì¡°ì •
        if movement_intensity > 0.8:
            window_size = 20  # ê²©ë ¬í•œ ì›€ì§ì„: ì§§ì€ ìœˆë„ìš°
            stride = 5
        elif movement_intensity > 0.5:
            window_size = 30  # ë³´í†µ ì›€ì§ì„: ê¸°ë³¸ ìœˆë„ìš°
            stride = 10
        else:
            window_size = 45  # ëŠë¦° ì›€ì§ì„: ê¸´ ìœˆë„ìš°
            stride = 15
            
        return self.classify_video_sequence(
            keypoints, scores, window_size, stride
        )
```

#### 2. ë‹¤ì¤‘ ëª¨ë¸ ì•™ìƒë¸”

```python
class EnsembleClassifier:
    """ë‹¤ì¤‘ STGCN++ ëª¨ë¸ ì•™ìƒë¸”"""
    
    def __init__(self, model_configs):
        self.models = []
        for config in model_configs:
            model = STGCNActionClassifier(**config)
            self.models.append(model)
            
    def ensemble_predict(self, keypoints, scores):
        """ë‹¤ì¤‘ ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼ í†µí•©"""
        
        predictions = []
        confidences = []
        
        for model in self.models:
            result = model.classify_video_sequence(keypoints, scores)
            predictions.append(result['prediction'])
            confidences.append(result['confidence'])
        
        # ì‹ ë¢°ë„ ê°€ì¤‘ íˆ¬í‘œ
        weighted_sum = sum(pred * conf for pred, conf in zip(predictions, confidences))
        total_confidence = sum(confidences)
        
        final_prediction = 1 if weighted_sum / total_confidence > 0.5 else 0
        final_confidence = total_confidence / len(self.models)
        
        return {
            'prediction': final_prediction,
            'confidence': final_confidence,
            'individual_predictions': predictions,
            'individual_confidences': confidences
        }
```

---

## âš¡ ì‹¤ì‹œê°„ ì²˜ë¦¬

### ì›¹ìº  ì‹¤ì‹œê°„ ì²˜ë¦¬

```python
import cv2
import time
from collections import deque

class RealTimeViolenceDetector:
    """ì‹¤ì‹œê°„ í­ë ¥ ê²€ì¶œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, pipeline, buffer_size=30, detection_interval=15):
        self.pipeline = pipeline
        self.buffer_size = buffer_size
        self.detection_interval = detection_interval
        
        self.frame_buffer = deque(maxlen=buffer_size)
        self.pose_buffer = deque(maxlen=buffer_size)
        self.frame_count = 0
        
    def process_webcam(self, camera_id=0):
        """ì›¹ìº  ì‹¤ì‹œê°„ ì²˜ë¦¬"""
        
        cap = cv2.VideoCapture(camera_id)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            self.frame_buffer.append(frame)
            self.frame_count += 1
            
            # í¬ì¦ˆ ì¶”ì • (ë§¤ í”„ë ˆì„)
            keypoints, scores = self.pipeline.pose_estimator.estimate_poses_single_frame(frame)
            self.pose_buffer.append((keypoints, scores))
            
            # í­ë ¥ ê²€ì¶œ (ì„¤ì •ëœ ê°„ê²©ë§ˆë‹¤)
            if self.frame_count % self.detection_interval == 0 and len(self.pose_buffer) >= self.buffer_size:
                detection_result = self.detect_violence()
                
                # ê²°ê³¼ í‘œì‹œ
                self.display_result(frame, detection_result)
            
            # í”„ë ˆì„ í‘œì‹œ
            cv2.imshow('Real-time Violence Detection', frame)
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
                
        cap.release()
        cv2.destroyAllWindows()
        
    def detect_violence(self):
        """í˜„ì¬ ë²„í¼ì—ì„œ í­ë ¥ ê²€ì¶œ"""
        
        # Fight-ìš°ì„  íŠ¸ë˜í‚¹
        selected_keypoints, selected_scores = self.pipeline.tracker.process_video_sequence(
            list(self.pose_buffer), self.buffer_size
        )
        
        # STGCN++ ë¶„ë¥˜
        result = self.pipeline.classifier.classify_video_sequence(
            selected_keypoints, selected_scores
        )
        
        return result
        
    def display_result(self, frame, result):
        """ê²°ê³¼ë¥¼ í”„ë ˆì„ì— í‘œì‹œ"""
        
        label = result['prediction_label']
        confidence = result['confidence']
        
        # ìƒ‰ìƒ ì„ íƒ (Fight: ë¹¨ê°•, NonFight: ì´ˆë¡)
        color = (0, 0, 255) if label == 'Fight' else (0, 255, 0)
        
        # í…ìŠ¤íŠ¸ í‘œì‹œ
        text = f"{label}: {confidence:.2f}"
        cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
        
        # ìœ„í—˜ë„ì— ë”°ë¥¸ ê²½ê³  í‘œì‹œ
        if label == 'Fight' and confidence > 0.8:
            cv2.rectangle(frame, (5, 5), (635, 475), (0, 0, 255), 5)
            cv2.putText(frame, "VIOLENCE DETECTED!", (150, 60), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)
```

### ìŠ¤íŠ¸ë¦¬ë° ì„œë²„ êµ¬í˜„

```python
from flask import Flask, Response, jsonify
import json
import threading

class ViolenceDetectionServer:
    """HTTP ìŠ¤íŠ¸ë¦¬ë° ì„œë²„"""
    
    def __init__(self, pipeline):
        self.app = Flask(__name__)
        self.pipeline = pipeline
        self.detector = RealTimeViolenceDetector(pipeline)
        
        self.setup_routes()
        
    def setup_routes(self):
        """API ë¼ìš°íŠ¸ ì„¤ì •"""
        
        @self.app.route('/video_feed')
        def video_feed():
            return Response(
                self.generate_frames(),
                mimetype='multipart/x-mixed-replace; boundary=frame'
            )
            
        @self.app.route('/detection_status')
        def detection_status():
            # ìµœê·¼ ê²€ì¶œ ê²°ê³¼ ë°˜í™˜
            return jsonify(self.detector.get_latest_result())
            
        @self.app.route('/start_detection')
        def start_detection():
            threading.Thread(target=self.detector.process_webcam).start()
            return jsonify({'status': 'started'})
            
    def generate_frames(self):
        """ë¹„ë””ì˜¤ í”„ë ˆì„ ìŠ¤íŠ¸ë¦¬ë°"""
        
        cap = cv2.VideoCapture(0)
        
        while True:
            success, frame = cap.read()
            if not success:
                break
                
            # í”„ë ˆì„ ì¸ì½”ë”©
            ret, buffer = cv2.imencode('.jpg', frame)
            frame_bytes = buffer.tobytes()
            
            yield (b'--frame\r\n'
                   b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')
    
    def run(self, host='0.0.0.0', port=5000):
        """ì„œë²„ ì‹¤í–‰"""
        self.app.run(host=host, port=port, threaded=True)
```

---

## ğŸ“Š ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

### ë¶„ì‚° ì²˜ë¦¬ ì‹œìŠ¤í…œ

```python
import ray
from typing import List

@ray.remote
class DistributedProcessor:
    """Rayë¥¼ í™œìš©í•œ ë¶„ì‚° ì²˜ë¦¬"""
    
    def __init__(self, pipeline_config):
        self.pipeline = EndToEndPipeline(**pipeline_config)
        
    def process_video_batch(self, video_paths: List[str]):
        """ë¹„ë””ì˜¤ ë°°ì¹˜ ì²˜ë¦¬"""
        results = []
        for video_path in video_paths:
            result = self.pipeline.process_single_video(video_path)
            results.append(result)
        return results

class LargeScaleProcessor:
    """ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, num_workers=4):
        ray.init()
        self.num_workers = num_workers
        
        # ì›Œì»¤ ìƒì„±
        self.workers = [DistributedProcessor.remote(pipeline_config) 
                       for _ in range(num_workers)]
        
    def process_large_dataset(self, video_paths: List[str], batch_size=10):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        
        # ë¹„ë””ì˜¤ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
        batches = [video_paths[i:i+batch_size] 
                  for i in range(0, len(video_paths), batch_size)]
        
        # ë°°ì¹˜ë¥¼ ì›Œì»¤ì— ë¶„ì‚°
        futures = []
        for i, batch in enumerate(batches):
            worker = self.workers[i % self.num_workers]
            future = worker.process_video_batch.remote(batch)
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        all_results = []
        for future in futures:
            batch_results = ray.get(future)
            all_results.extend(batch_results)
            
        return all_results
```

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬

```python
import gc
from pathlib import Path

class MemoryEfficientProcessor:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëŒ€ìš©ëŸ‰ ì²˜ë¦¬"""
    
    def __init__(self, pipeline_config, max_memory_gb=8):
        self.pipeline_config = pipeline_config
        self.max_memory_gb = max_memory_gb
        
    def estimate_memory_usage(self, video_path):
        """ë¹„ë””ì˜¤ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì •"""
        
        video_size_mb = Path(video_path).stat().st_size / (1024 * 1024)
        
        # ëŒ€ëµì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (ê²½í—˜ì  ê³µì‹)
        estimated_memory_gb = video_size_mb * 0.01  # 1% ì •ë„
        
        return estimated_memory_gb
        
    def process_with_memory_limit(self, video_paths):
        """ë©”ëª¨ë¦¬ ì œí•œ í•˜ì—ì„œ ì²˜ë¦¬"""
        
        current_batch = []
        current_memory = 0
        results = []
        
        for video_path in video_paths:
            estimated_memory = self.estimate_memory_usage(video_path)
            
            if current_memory + estimated_memory > self.max_memory_gb:
                # í˜„ì¬ ë°°ì¹˜ ì²˜ë¦¬
                if current_batch:
                    batch_results = self.process_batch_with_cleanup(current_batch)
                    results.extend(batch_results)
                
                # ë°°ì¹˜ ì´ˆê¸°í™”
                current_batch = [video_path]
                current_memory = estimated_memory
            else:
                current_batch.append(video_path)
                current_memory += estimated_memory
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
        if current_batch:
            batch_results = self.process_batch_with_cleanup(current_batch)
            results.extend(batch_results)
            
        return results
    
    def process_batch_with_cleanup(self, video_paths):
        """ë©”ëª¨ë¦¬ ì •ë¦¬ì™€ í•¨ê»˜ ë°°ì¹˜ ì²˜ë¦¬"""
        
        # íŒŒì´í”„ë¼ì¸ ìƒì„±
        pipeline = EndToEndPipeline(**self.pipeline_config)
        
        try:
            # ë°°ì¹˜ ì²˜ë¦¬
            results = pipeline.process_batch_videos(
                video_paths, 
                generate_overlay=False,  # ë©”ëª¨ë¦¬ ì ˆì•½
                save_individual_results=True
            )
            
            return results['individual_results']
            
        finally:
            # ëª…ì‹œì  ì •ë¦¬
            pipeline.cleanup()
            del pipeline
            gc.collect()
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
```

---

## ğŸ“ ëª¨ë¸ íŒŒì¸íŠœë‹

### ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í•™ìŠµ

```python
# MMAction2 ê¸°ë°˜ STGCN++ íŒŒì¸íŠœë‹ ì„¤ì •
custom_config = """
_base_ = ['../../_base_/models/stgcn++.py']

# ëª¨ë¸ ì„¤ì •
model = dict(
    cls_head=dict(
        num_classes=2,  # Fight, NonFight
        dropout=0.5
    )
)

# ë°ì´í„°ì…‹ ì„¤ì •
dataset_type = 'PoseDataset'
ann_file_train = 'data/custom_train.pkl'
ann_file_val = 'data/custom_val.pkl'
ann_file_test = 'data/custom_test.pkl'

train_pipeline = [
    dict(type='PoseNormalize'),
    dict(type='PoseRandomFlip', flip_ratio=0.5),
    dict(type='PoseRandomResample', keep_ratio=0.95),
    dict(type='FormatShape', input_format='NCTVM'),
    dict(type='Collect', keys=['keypoint', 'label'], meta_keys=[]),
    dict(type='ToTensor', keys=['keypoint'])
]

# í•™ìŠµ ì„¤ì •
optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=dict(max_norm=40, norm_type=2))

lr_config = dict(policy='step', step=[20, 40])
total_epochs = 50

# Fight-íŠ¹í™” ì†ì‹¤ í•¨ìˆ˜
loss_config = dict(
    type='CrossEntropyLoss',
    class_weight=[1.0, 2.0],  # Fight í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
    use_sigmoid=False
)
"""
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦

```python
class ModelValidator:
    """ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ ë„êµ¬"""
    
    def __init__(self, pipeline, validation_data):
        self.pipeline = pipeline
        self.validation_data = validation_data
        
    def validate_performance(self):
        """ì¢…í•© ì„±ëŠ¥ ê²€ì¦"""
        
        results = []
        
        for video_path, ground_truth in self.validation_data:
            result = self.pipeline.process_single_video(
                video_path, ground_truth
            )
            results.append(result)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        predictions = [r['classification']['prediction'] for r in results]
        ground_truths = [r['ground_truth_label'] for r in results]
        confidences = [r['classification']['confidence'] for r in results]
        
        metrics = self.pipeline.metrics_calculator.calculate_comprehensive_metrics(
            predictions, ground_truths, confidences
        )
        
        return metrics
    
    def analyze_failure_cases(self, threshold=0.5):
        """ì‹¤íŒ¨ ì‚¬ë¡€ ë¶„ì„"""
        
        failure_cases = []
        
        for video_path, ground_truth in self.validation_data:
            result = self.pipeline.process_single_video(video_path, ground_truth)
            
            prediction = result['classification']['prediction']
            confidence = result['classification']['confidence']
            
            # ì‹¤íŒ¨ ì¡°ê±´: ì˜ëª»ëœ ì˜ˆì¸¡ ë˜ëŠ” ë‚®ì€ ì‹ ë¢°ë„
            if prediction != ground_truth or confidence < threshold:
                failure_cases.append({
                    'video_path': video_path,
                    'ground_truth': ground_truth,
                    'prediction': prediction,
                    'confidence': confidence,
                    'failure_type': self.classify_failure_type(
                        ground_truth, prediction, confidence
                    )
                })
        
        return failure_cases
    
    def classify_failure_type(self, gt, pred, conf):
        """ì‹¤íŒ¨ ìœ í˜• ë¶„ë¥˜"""
        
        if gt == 1 and pred == 0:
            return 'False Negative' if conf > 0.5 else 'Low Confidence FN'
        elif gt == 0 and pred == 1:
            return 'False Positive' if conf > 0.5 else 'Low Confidence FP'
        else:
            return 'Low Confidence Correct'
```

---

ì´ ê³ ê¸‰ ê°€ì´ë“œëŠ” STGCN++ Violence Detection ì‹œìŠ¤í…œì˜ ê¹Šì´ ìˆëŠ” ì´í•´ì™€ ê³ ê¸‰ í™œìš©ì„ ìœ„í•œ ì¢…í•©ì ì¸ ìë£Œì…ë‹ˆë‹¤. ê° ì„¹ì…˜ì˜ ì½”ë“œì™€ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì‹œìŠ¤í…œì„ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.