#!/usr/bin/env python3
"""
Performance Analysis Test Script
ì„±ëŠ¥ ë¶„ì„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ë‹¨ê³„ë³„ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
"""

import os
import sys
import time
import logging
import numpy as np
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append('/workspace/rtmo_gcn_pipeline/inference_pipeline')

from config import INFERENCE_CONFIG, validate_config
from pose_estimator import RTMOPoseEstimator
from fight_tracker import FightPrioritizedTracker
from action_classifier import STGCNActionClassifier
from video_overlay import VideoOverlayGenerator
from performance_analyzer import PerformanceAnalyzer, StageTimer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PerformanceTestPipeline:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        # ì„±ëŠ¥ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.analyzer = PerformanceAnalyzer()
        
        # ì„¤ì • ê²€ì¦
        validate_config()
        
        # íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
        self._init_components()
    
    def _init_components(self):
        """íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”"""
        logger.info("ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì¤‘...")
        
        # 1. RTMO í¬ì¦ˆ ì¶”ì •ê¸°
        from config import POSE_CONFIG, POSE_CHECKPOINT, GCN_CONFIG, GCN_CHECKPOINT
        pose_config = POSE_CONFIG
        pose_checkpoint = POSE_CHECKPOINT
        
        with StageTimer(self.analyzer, "component_init_pose"):
            self.pose_estimator = RTMOPoseEstimator(pose_config, pose_checkpoint, 'cuda:0')
        
        # 2. Fight-ìš°ì„  íŠ¸ë˜ì»¤ (ByteTrack ê¸°ë°˜)
        bytetrack_config = {
            'high_thresh': 0.6,
            'low_thresh': 0.1,
            'max_disappeared': 30,
            'min_hits': 3
        }
        
        with StageTimer(self.analyzer, "component_init_tracker"):
            self.tracker = FightPrioritizedTracker(
                frame_width=INFERENCE_CONFIG['frame_extraction']['resize'][0],
                frame_height=INFERENCE_CONFIG['frame_extraction']['resize'][1],
                region_weights=INFERENCE_CONFIG['region_weights'],
                composite_weights=INFERENCE_CONFIG['composite_weights'],
                bytetrack_config=bytetrack_config
            )
        
        # 3. STGCN++ ë¶„ë¥˜ê¸°
        gcn_config = GCN_CONFIG
        gcn_checkpoint = GCN_CHECKPOINT
        
        with StageTimer(self.analyzer, "component_init_classifier"):
            self.classifier = STGCNActionClassifier(gcn_config, gcn_checkpoint, 'cuda:0')
        
        # 4. ë¹„ë””ì˜¤ ì˜¤ë²„ë ˆì´ ìƒì„±ê¸° (ì„ íƒì )
        with StageTimer(self.analyzer, "component_init_overlay"):
            self.overlay_generator = VideoOverlayGenerator(
                stgcn_joint_color=(0, 255, 128),
                other_joint_color=(255, 0, 0)
            )
            if hasattr(self.pose_estimator, 'model'):
                self.overlay_generator.init_visualizer_with_pose_model(self.pose_estimator.model)
        
        logger.info("ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def analyze_video_performance(self, video_path: str, generate_overlay: bool = True):
        """ë¹„ë””ì˜¤ ì„±ëŠ¥ ë¶„ì„"""
        logger.info(f"ì„±ëŠ¥ ë¶„ì„ ì‹œì‘: {os.path.basename(video_path)}")
        
        total_start_time = time.time()
        
        try:
            # 1. RTMO í¬ì¦ˆ ì¶”ì •
            with StageTimer(self.analyzer, "pose_estimation"):
                pose_results = self.pose_estimator.estimate_poses_from_video(
                    video_path, 
                    max_frames=INFERENCE_CONFIG['frame_extraction']['max_frames']
                )
            
            if not pose_results:
                logger.error("í¬ì¦ˆ ì¶”ì • ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ")
                return None
            
            total_frames = len(pose_results)
            logger.info(f"í¬ì¦ˆ ì¶”ì • ì™„ë£Œ: {total_frames}í”„ë ˆì„")
            
            # 2. Fight-ìš°ì„  íŠ¸ë˜í‚¹ ì¤€ë¹„
            with StageTimer(self.analyzer, "tracking_preparation"):
                self.tracker.reset()
                
                detections_list = []
                keypoints_list = []
                scores_list = []
                
                for keypoints, scores in pose_results:
                    if keypoints is not None and scores is not None and len(keypoints) > 0:
                        keypoints_frame_list = [keypoints[i] for i in range(len(keypoints))]
                        scores_frame_list = [scores[i] for i in range(len(scores))]
                        
                        detections = self.tracker.create_detections_from_pose_results(
                            keypoints_frame_list, scores_frame_list
                        )
                        detections_list.append(detections)
                        keypoints_list.append(keypoints_frame_list)
                        scores_list.append(scores_frame_list)
                    else:
                        detections_list.append([])
                        keypoints_list.append([])
                        scores_list.append([])
            
            # 3. Fight-ìš°ì„  íŠ¸ë˜í‚¹ (ByteTrack ê¸°ë°˜)
            with StageTimer(self.analyzer, "tracking_processing"):
                if detections_list and any(len(det) > 0 for det in detections_list):
                    selected_keypoints, selected_scores, track_ids_per_frame = \
                        self.tracker.process_video_sequence_with_detections(
                            detections_list, keypoints_list, scores_list,
                            sequence_length=INFERENCE_CONFIG['sequence_length'],
                            num_person=INFERENCE_CONFIG['num_person']
                        )
                else:
                    # ë¹ˆ ë¹„ë””ì˜¤ ì²˜ë¦¬
                    num_person = INFERENCE_CONFIG['num_person']
                    seq_len = INFERENCE_CONFIG['sequence_length'] or 30
                    selected_keypoints = np.zeros((seq_len, num_person, 17, 2))
                    selected_scores = np.zeros((seq_len, num_person, 17))
                    track_ids_per_frame = [[-1] * num_person for _ in range(seq_len)]
            
            logger.info(f"íŠ¸ë˜í‚¹ ì™„ë£Œ: {selected_keypoints.shape}")
            
            # 4. STGCN++ ë¶„ë¥˜
            with StageTimer(self.analyzer, "stgcn_classification"):
                img_shape = INFERENCE_CONFIG['frame_extraction']['resize'][::-1]
                classification_result = self.classifier.classify_video_sequence(
                    selected_keypoints, selected_scores, 
                    window_size=INFERENCE_CONFIG['sequence_length'],
                    stride=INFERENCE_CONFIG['sequence_length'] // 2,
                    img_shape=img_shape
                )
            
            logger.info(f"ë¶„ë¥˜ ì™„ë£Œ: {classification_result['prediction_label']} ({classification_result['confidence']:.3f})")
            
            # 5. ì˜¤ë²„ë ˆì´ ë¹„ë””ì˜¤ ìƒì„± (ì„ íƒì )
            overlay_path = None
            if generate_overlay:
                with StageTimer(self.analyzer, "overlay_generation"):
                    video_name = os.path.splitext(os.path.basename(video_path))[0]
                    overlay_dir = "/workspace/rtmo_gcn_pipeline/inference_pipeline/results/overlays"
                    os.makedirs(overlay_dir, exist_ok=True)
                    overlay_path = os.path.join(overlay_dir, f"{video_name}_performance_test_overlay.mp4")
                    
                    success = self.overlay_generator.create_overlay_with_bytetrack(
                        video_path=video_path,
                        pose_results=pose_results,
                        classification_result=classification_result,
                        output_path=overlay_path,
                        num_person=INFERENCE_CONFIG['num_person'],
                        tracker=self.tracker
                    )
                    
                    if not success:
                        overlay_path = None
                        logger.warning("ì˜¤ë²„ë ˆì´ ìƒì„± ì‹¤íŒ¨")
            
            # ì „ì²´ ì²˜ë¦¬ ì‹œê°„
            total_time = time.time() - total_start_time
            
            # ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ìƒì„±
            analysis = self.analyzer.analyze_performance(total_frames)
            analysis['total_processing_time'] = total_time
            analysis['overall_fps'] = total_frames / total_time
            analysis['video_info'] = {
                'path': video_path,
                'frames': total_frames,
                'classification': classification_result,
                'overlay_path': overlay_path
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ ê²½ë¡œ
    test_video = "/aivanas/raw/surveillance/action/violence/action_recognition/data/UCF_Crime_test/Fight/Fighting033_x264.mp4"
    
    if not os.path.exists(test_video):
        logger.error(f"í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {test_video}")
        return
    
    print("ğŸš€ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë¶„ì„ ì‹œì‘")
    print(f"ğŸ“¹ í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤: {os.path.basename(test_video)}")
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline = PerformanceTestPipeline()
    
    # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
    analysis = pipeline.analyze_video_performance(test_video, generate_overlay=True)
    
    if analysis:
        # ê²°ê³¼ ì¶œë ¥
        pipeline.analyzer.print_summary(analysis)
        
        # ê²°ê³¼ ì €ì¥
        output_path = "/workspace/rtmo_gcn_pipeline/inference_pipeline/results/performance_analysis.json"
        pipeline.analyzer.save_analysis(analysis, output_path)
        
        print(f"\nğŸ’¾ ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì €ì¥: {output_path}")
    else:
        print("âŒ ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()