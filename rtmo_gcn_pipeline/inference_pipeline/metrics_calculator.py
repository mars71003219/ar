#!/usr/bin/env python3
"""
Performance Metrics Calculator
ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° ëª¨ë“ˆ - TP/TN/FP/FN ê¸°ë°˜ ì •í™•ë„, ì •ë°€ë„, ìž¬í˜„ìœ¨, F1-score ê³„ì‚°
"""

import numpy as np
from typing import List, Dict, Tuple, Optional
import logging
import json
import os.path as osp

logger = logging.getLogger(__name__)

class MetricsCalculator:
    """
    ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°ê¸°
    ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ë¼ë²¨ì„ ë¹„êµí•˜ì—¬ ë¶„ë¥˜ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
    """
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.class_mapping = {
            0: 'NonFight',
            1: 'Fight'
        }
        
    def calculate_confusion_matrix(self, predictions: List[int], ground_truths: List[int]) -> Dict[str, int]:
        """
        í˜¼ë™ í–‰ë ¬ ê³„ì‚°
        
        Args:
            predictions: ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            ground_truths: ì‹¤ì œ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í˜¼ë™ í–‰ë ¬ ë”•ì…”ë„ˆë¦¬ {'TP': int, 'TN': int, 'FP': int, 'FN': int}
        """
        if len(predictions) != len(ground_truths):
            logger.error(f"ì˜ˆì¸¡ê³¼ ì‹¤ì œ ë¼ë²¨ ê°œìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤: {len(predictions)} vs {len(ground_truths)}")
            raise ValueError("ì˜ˆì¸¡ê³¼ ì‹¤ì œ ë¼ë²¨ ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        tp = sum(1 for p, gt in zip(predictions, ground_truths) if p == 1 and gt == 1)  # True Positive
        tn = sum(1 for p, gt in zip(predictions, ground_truths) if p == 0 and gt == 0)  # True Negative
        fp = sum(1 for p, gt in zip(predictions, ground_truths) if p == 1 and gt == 0)  # False Positive
        fn = sum(1 for p, gt in zip(predictions, ground_truths) if p == 0 and gt == 1)  # False Negative
        
        confusion_matrix = {
            'TP': tp,  # Fightë¥¼ Fightë¡œ ì •í™•ížˆ ë¶„ë¥˜
            'TN': tn,  # NonFightë¥¼ NonFightë¡œ ì •í™•ížˆ ë¶„ë¥˜  
            'FP': fp,  # NonFightë¥¼ Fightë¡œ ìž˜ëª» ë¶„ë¥˜
            'FN': fn   # Fightë¥¼ NonFightë¡œ ìž˜ëª» ë¶„ë¥˜
        }
        
        logger.info(f"í˜¼ë™ í–‰ë ¬: TP={tp}, TN={tn}, FP={fp}, FN={fn}")
        
        return confusion_matrix
    
    def calculate_metrics(self, confusion_matrix: Dict[str, int]) -> Dict[str, float]:
        """
        ë¶„ë¥˜ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            confusion_matrix: í˜¼ë™ í–‰ë ¬
            
        Returns:
            ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
        """
        tp = confusion_matrix['TP']
        tn = confusion_matrix['TN']
        fp = confusion_matrix['FP']
        fn = confusion_matrix['FN']
        
        total = tp + tn + fp + fn
        
        # ì •í™•ë„ (Accuracy): ì „ì²´ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•œ ë¹„ìœ¨
        accuracy = (tp + tn) / total if total > 0 else 0.0
        
        # ì •ë°€ë„ (Precision): Fightë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ Fightì¸ ë¹„ìœ¨
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        
        # ìž¬í˜„ìœ¨ (Recall/Sensitivity): ì‹¤ì œ Fight ì¤‘ ì˜¬ë°”ë¥´ê²Œ Fightë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        
        # íŠ¹ì´ë„ (Specificity): ì‹¤ì œ NonFight ì¤‘ ì˜¬ë°”ë¥´ê²Œ NonFightë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        
        # F1-Score: ì •ë°€ë„ì™€ ìž¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· 
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        
        # False Positive Rate: ì‹¤ì œ NonFight ì¤‘ Fightë¡œ ìž˜ëª» ì˜ˆì¸¡í•œ ë¹„ìœ¨
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0
        
        # False Negative Rate: ì‹¤ì œ Fight ì¤‘ NonFightë¡œ ìž˜ëª» ì˜ˆì¸¡í•œ ë¹„ìœ¨
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0.0
        
        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'specificity': specificity,
            'f1_score': f1_score,
            'false_positive_rate': fpr,
            'false_negative_rate': fnr
        }
        
        logger.info(f"ì„±ëŠ¥ ë©”íŠ¸ë¦­: ì •í™•ë„={accuracy:.4f}, ì •ë°€ë„={precision:.4f}, ìž¬í˜„ìœ¨={recall:.4f}, F1={f1_score:.4f}")
        
        return metrics
    
    def analyze_predictions_by_class(self, predictions: List[int], ground_truths: List[int], 
                                   confidences: Optional[List[float]] = None) -> Dict:
        """
        í´ëž˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„ì„
        
        Args:
            predictions: ì˜ˆì¸¡ ê²°ê³¼
            ground_truths: ì‹¤ì œ ë¼ë²¨
            confidences: ì˜ˆì¸¡ ì‹ ë¢°ë„ (ì„ íƒì‚¬í•­)
            
        Returns:
            í´ëž˜ìŠ¤ë³„ ë¶„ì„ ê²°ê³¼
        """
        analysis = {}
        
        for class_idx, class_name in self.class_mapping.items():
            # í•´ë‹¹ í´ëž˜ìŠ¤ì˜ ì‹¤ì œ ìƒ˜í”Œë“¤
            true_indices = [i for i, gt in enumerate(ground_truths) if gt == class_idx]
            # í•´ë‹¹ í´ëž˜ìŠ¤ë¡œ ì˜ˆì¸¡ëœ ìƒ˜í”Œë“¤
            pred_indices = [i for i, pred in enumerate(predictions) if pred == class_idx]
            
            # ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ëœ ìƒ˜í”Œë“¤
            correct_indices = [i for i in true_indices if predictions[i] == class_idx]
            
            class_analysis = {
                'total_true_samples': len(true_indices),
                'total_predicted_samples': len(pred_indices),
                'correctly_classified': len(correct_indices),
                'class_accuracy': len(correct_indices) / len(true_indices) if true_indices else 0.0
            }
            
            # ì‹ ë¢°ë„ ë¶„ì„ (ì œê³µëœ ê²½ìš°)
            if confidences:
                if true_indices:
                    true_confidences = [confidences[i] for i in true_indices]
                    class_analysis['avg_confidence_true'] = np.mean(true_confidences)
                    class_analysis['std_confidence_true'] = np.std(true_confidences)
                
                if correct_indices:
                    correct_confidences = [confidences[i] for i in correct_indices]
                    class_analysis['avg_confidence_correct'] = np.mean(correct_confidences)
            
            analysis[class_name] = class_analysis
        
        return analysis
    
    def calculate_comprehensive_metrics(self, predictions: List[int], ground_truths: List[int],
                                      confidences: Optional[List[float]] = None,
                                      video_names: Optional[List[str]] = None) -> Dict:
        """
        ì¢…í•©ì ì¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Args:
            predictions: ì˜ˆì¸¡ ê²°ê³¼
            ground_truths: ì‹¤ì œ ë¼ë²¨
            confidences: ì˜ˆì¸¡ ì‹ ë¢°ë„
            video_names: ë¹„ë””ì˜¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì¢…í•© ë©”íŠ¸ë¦­ ê²°ê³¼
        """
        # ê¸°ë³¸ í˜¼ë™ í–‰ë ¬ ë° ë©”íŠ¸ë¦­
        confusion_matrix = self.calculate_confusion_matrix(predictions, ground_truths)
        metrics = self.calculate_metrics(confusion_matrix)
        
        # í´ëž˜ìŠ¤ë³„ ë¶„ì„
        class_analysis = self.analyze_predictions_by_class(predictions, ground_truths, confidences)
        
        # ì „ì²´ í†µê³„
        total_samples = len(predictions)
        correct_predictions = sum(1 for p, gt in zip(predictions, ground_truths) if p == gt)
        
        # ì‹ ë¢°ë„ ë¶„ì„
        confidence_analysis = {}
        if confidences:
            confidence_analysis = {
                'mean_confidence': np.mean(confidences),
                'std_confidence': np.std(confidences),
                'min_confidence': np.min(confidences),
                'max_confidence': np.max(confidences),
                'confidence_correct': np.mean([confidences[i] for i in range(len(confidences)) 
                                              if predictions[i] == ground_truths[i]]),
                'confidence_incorrect': np.mean([confidences[i] for i in range(len(confidences)) 
                                                if predictions[i] != ground_truths[i]]) if any(predictions[i] != ground_truths[i] for i in range(len(predictions))) else 0.0
            }
        
        # ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ ë¶„ì„
        misclassified_cases = []
        for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):
            if pred != gt:
                case = {
                    'index': i,
                    'predicted': self.class_mapping[pred],
                    'ground_truth': self.class_mapping[gt],
                    'confidence': confidences[i] if confidences else None,
                    'video_name': video_names[i] if video_names else f"video_{i}"
                }
                misclassified_cases.append(case)
        
        comprehensive_results = {
            'confusion_matrix': confusion_matrix,
            'metrics': metrics,
            'class_analysis': class_analysis,
            'confidence_analysis': confidence_analysis,
            'summary': {
                'total_samples': total_samples,
                'correct_predictions': correct_predictions,
                'incorrect_predictions': total_samples - correct_predictions,
                'overall_accuracy': correct_predictions / total_samples if total_samples > 0 else 0.0
            },
            'misclassified_cases': misclassified_cases
        }
        
        return comprehensive_results
    
    def compare_with_baseline(self, current_results: Dict, baseline_results: Dict) -> Dict:
        """
        ê¸°ì¤€ì„ ê³¼ ì„±ëŠ¥ ë¹„êµ
        
        Args:
            current_results: í˜„ìž¬ ì‹œìŠ¤í…œ ê²°ê³¼
            baseline_results: ê¸°ì¤€ ì‹œìŠ¤í…œ ê²°ê³¼
            
        Returns:
            ë¹„êµ ê²°ê³¼
        """
        current_metrics = current_results['metrics']
        baseline_metrics = baseline_results['metrics']
        
        improvements = {}
        for metric_name in current_metrics:
            if metric_name in baseline_metrics:
                current_val = current_metrics[metric_name]
                baseline_val = baseline_metrics[metric_name]
                improvement = current_val - baseline_val
                improvement_pct = (improvement / baseline_val * 100) if baseline_val > 0 else 0
                
                improvements[metric_name] = {
                    'current': current_val,
                    'baseline': baseline_val,
                    'improvement': improvement,
                    'improvement_percentage': improvement_pct
                }
        
        # í˜¼ë™ í–‰ë ¬ ë¹„êµ
        cm_comparison = {}
        current_cm = current_results['confusion_matrix']
        baseline_cm = baseline_results['confusion_matrix']
        
        for cm_key in current_cm:
            if cm_key in baseline_cm:
                cm_comparison[cm_key] = {
                    'current': current_cm[cm_key],
                    'baseline': baseline_cm[cm_key],
                    'change': current_cm[cm_key] - baseline_cm[cm_key]
                }
        
        return {
            'metric_improvements': improvements,
            'confusion_matrix_comparison': cm_comparison,
            'summary': {
                'better_metrics': [m for m, data in improvements.items() if data['improvement'] > 0],
                'worse_metrics': [m for m, data in improvements.items() if data['improvement'] < 0],
                'overall_improvement': np.mean([data['improvement'] for data in improvements.values()])
            }
        }
    
    def save_results(self, results: Dict, output_path: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ìž¥"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"ë©”íŠ¸ë¦­ ê²°ê³¼ ì €ìž¥ ì™„ë£Œ: {output_path}")
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ìž¥ ì‹¤íŒ¨: {e}")
    
    def generate_report(self, results: Dict, output_path: str):
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        confusion_matrix = results['confusion_matrix']
        metrics = results['metrics']
        class_analysis = results['class_analysis']
        summary = results['summary']
        
        report = f"""# Violence Detection Performance Report

## ðŸ“Š ì „ì²´ ì„±ëŠ¥ ìš”ì•½

- **ì´ ìƒ˜í”Œ ìˆ˜**: {summary['total_samples']}
- **ì˜¬ë°”ë¥¸ ì˜ˆì¸¡**: {summary['correct_predictions']}
- **ìž˜ëª»ëœ ì˜ˆì¸¡**: {summary['incorrect_predictions']}
- **ì „ì²´ ì •í™•ë„**: {summary['overall_accuracy']:.4f}

## ðŸŽ¯ ì„±ëŠ¥ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| ì •í™•ë„ (Accuracy) | {metrics['accuracy']:.4f} |
| ì •ë°€ë„ (Precision) | {metrics['precision']:.4f} |
| ìž¬í˜„ìœ¨ (Recall) | {metrics['recall']:.4f} |
| íŠ¹ì´ë„ (Specificity) | {metrics['specificity']:.4f} |
| F1-Score | {metrics['f1_score']:.4f} |
| False Positive Rate | {metrics['false_positive_rate']:.4f} |
| False Negative Rate | {metrics['false_negative_rate']:.4f} |

## ðŸ“ˆ í˜¼ë™ í–‰ë ¬

|          | ì˜ˆì¸¡ NonFight | ì˜ˆì¸¡ Fight |
|----------|---------------|------------|
| ì‹¤ì œ NonFight | {confusion_matrix['TN']} (TN) | {confusion_matrix['FP']} (FP) |
| ì‹¤ì œ Fight | {confusion_matrix['FN']} (FN) | {confusion_matrix['TP']} (TP) |

## ðŸ” í´ëž˜ìŠ¤ë³„ ë¶„ì„

### Fight í´ëž˜ìŠ¤
- ì‹¤ì œ ìƒ˜í”Œ ìˆ˜: {class_analysis['Fight']['total_true_samples']}
- ì˜ˆì¸¡ëœ ìƒ˜í”Œ ìˆ˜: {class_analysis['Fight']['total_predicted_samples']}
- ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ëœ ìˆ˜: {class_analysis['Fight']['correctly_classified']}
- í´ëž˜ìŠ¤ ì •í™•ë„: {class_analysis['Fight']['class_accuracy']:.4f}

### NonFight í´ëž˜ìŠ¤
- ì‹¤ì œ ìƒ˜í”Œ ìˆ˜: {class_analysis['NonFight']['total_true_samples']}
- ì˜ˆì¸¡ëœ ìƒ˜í”Œ ìˆ˜: {class_analysis['NonFight']['total_predicted_samples']}
- ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜ëœ ìˆ˜: {class_analysis['NonFight']['correctly_classified']}
- í´ëž˜ìŠ¤ ì •í™•ë„: {class_analysis['NonFight']['class_accuracy']:.4f}

## ðŸ“ ê²°ë¡ 

ì´ ë³´ê³ ì„œëŠ” STGCN++ Fight ê²€ì¶œ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼ìž…ë‹ˆë‹¤.
"""

        # ì‹ ë¢°ë„ ë¶„ì„ ì¶”ê°€ (ìžˆëŠ” ê²½ìš°)
        if 'confidence_analysis' in results and results['confidence_analysis']:
            conf = results['confidence_analysis']
            report += f"""
## ðŸ” ì‹ ë¢°ë„ ë¶„ì„

- **í‰ê·  ì‹ ë¢°ë„**: {conf['mean_confidence']:.4f}
- **ì‹ ë¢°ë„ í‘œì¤€íŽ¸ì°¨**: {conf['std_confidence']:.4f}
- **ìµœì†Œ ì‹ ë¢°ë„**: {conf['min_confidence']:.4f}
- **ìµœëŒ€ ì‹ ë¢°ë„**: {conf['max_confidence']:.4f}
- **ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ í‰ê·  ì‹ ë¢°ë„**: {conf['confidence_correct']:.4f}
- **í‹€ë¦° ì˜ˆì¸¡ í‰ê·  ì‹ ë¢°ë„**: {conf['confidence_incorrect']:.4f}
"""

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {output_path}")
        except Exception as e:
            logger.error(f"ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")