#!/usr/bin/env python3
"""
Optimized RTMO Pose Estimator
ìµœì í™”ëœ RTMO í¬ì¦ˆ ì¶”ì •ê¸° - ë°°ì¹˜ ì²˜ë¦¬ ë° ì„±ëŠ¥ ê°œì„ 
"""

import numpy as np
import cv2
import torch
from typing import List, Tuple, Optional
import logging
import warnings
from concurrent.futures import ThreadPoolExecutor
import time

# OpenCV ê²½ê³  ë©”ì‹œì§€ ì–µì œ
try:
    cv2.setLogLevel(cv2.LOG_LEVEL_ERROR)
except AttributeError:
    pass
warnings.filterwarnings("ignore", category=UserWarning)

from mmpose.apis import init_model as init_pose_model, inference_bottomup

logger = logging.getLogger(__name__)

class OptimizedRTMOPoseEstimator:
    """
    ìµœì í™”ëœ RTMO-m í¬ì¦ˆ ì¶”ì •ê¸°
    - ë°°ì¹˜ ì²˜ë¦¬ë¡œ GPU í™œìš©ë„ ê·¹ëŒ€í™”
    - í•´ìƒë„ ì ì‘í˜• ì¶”ë¡ 
    - ë©€í‹°ìŠ¤ë ˆë“œ ì „ì²˜ë¦¬
    """
    
    def __init__(self, config_path: str, checkpoint_path: str, device: str = 'cuda:0',
                 batch_size: int = 8, target_fps: int = 30):
        """
        ì´ˆê¸°í™”
        
        Args:
            config_path: RTMO ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
            checkpoint_path: í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            device: ì¶”ë¡  ë””ë°”ì´ìŠ¤
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°
            target_fps: ëª©í‘œ FPS (í•´ìƒë„ ìë™ ì¡°ì •ìš©)
        """
        self.device = device
        self.config_path = config_path
        self.checkpoint_path = checkpoint_path
        self.batch_size = batch_size
        self.target_fps = target_fps
        
        # ëª¨ë¸ ë¡œë“œ
        logger.info(f"ìµœì í™”ëœ RTMO-m ëª¨ë¸ ë¡œë”© ì¤‘... (ë°°ì¹˜í¬ê¸°: {batch_size})")
        self.model = self._load_model()
        
        # GPU ì›Œë°ì—…
        self._warmup_model()
        
        logger.info("ìµœì í™”ëœ RTMO-m ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
    
    def _load_model(self):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            model = init_pose_model(
                self.config_path, 
                self.checkpoint_path, 
                device=self.device
            )
            return model
        except Exception as e:
            logger.error(f"RTMO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _warmup_model(self):
        """GPU ì›Œë°ì—… - ì²« ì¶”ë¡  ì‹œ ì§€ì—° ë°©ì§€"""
        logger.info("GPU ì›Œë°ì—… ì¤‘...")
        dummy_image = np.zeros((480, 640, 3), dtype=np.uint8)
        try:
            _ = inference_bottomup(self.model, dummy_image)
            logger.info("GPU ì›Œë°ì—… ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"GPU ì›Œë°ì—… ì‹¤íŒ¨: {e}")
    
    def _adaptive_resize(self, frame: np.ndarray, target_fps: int = 30) -> np.ndarray:
        """
        ì ì‘í˜• í•´ìƒë„ ì¡°ì •
        ëª©í‘œ FPSì— ë”°ë¼ í•´ìƒë„ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •
        """
        h, w = frame.shape[:2]
        
        # ëª©í‘œ FPSì— ë”°ë¥¸ í•´ìƒë„ ìŠ¤ì¼€ì¼ë§
        if target_fps >= 30:
            # ê³ ì„±ëŠ¥ ëª¨ë“œ: ì›ë³¸ í•´ìƒë„
            scale_factor = 1.0
        elif target_fps >= 20:
            # ê· í˜• ëª¨ë“œ: 80% í•´ìƒë„
            scale_factor = 0.8
        else:
            # íš¨ìœ¨ ëª¨ë“œ: 60% í•´ìƒë„
            scale_factor = 0.6
        
        if scale_factor < 1.0:
            new_w = int(w * scale_factor)
            new_h = int(h * scale_factor)
            # 8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì • (ëª¨ë¸ ìµœì í™”)
            new_w = (new_w // 8) * 8
            new_h = (new_h // 8) * 8
            frame = cv2.resize(frame, (new_w, new_h))
            
        return frame
    
    def _preprocess_frames_parallel(self, frames: List[np.ndarray]) -> List[np.ndarray]:
        """ë©€í‹°ìŠ¤ë ˆë“œ ì „ì²˜ë¦¬"""
        def preprocess_single(frame):
            # ì ì‘í˜• í•´ìƒë„ ì¡°ì •
            frame = self._adaptive_resize(frame, self.target_fps)
            return frame
        
        # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì „ì²˜ë¦¬
        with ThreadPoolExecutor(max_workers=4) as executor:
            processed_frames = list(executor.map(preprocess_single, frames))
        
        return processed_frames
    
    def estimate_poses_batch(self, frames: List[np.ndarray]) -> List[Tuple[np.ndarray, np.ndarray]]:
        """
        ë°°ì¹˜ í¬ì¦ˆ ì¶”ì •
        
        Args:
            frames: ì…ë ¥ í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            [(keypoints, scores), ...] í¬ì¦ˆ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not frames:
            return []
        
        # ì „ì²˜ë¦¬ (ë³‘ë ¬)
        processed_frames = self._preprocess_frames_parallel(frames)
        
        results = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(processed_frames), self.batch_size):
            batch_frames = processed_frames[i:i + self.batch_size]
            batch_results = []
            
            # GPU ë™ê¸°í™”
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            # ë°°ì¹˜ ë‚´ ê°œë³„ ì²˜ë¦¬ (MMPoseì˜ ë°°ì¹˜ ì§€ì› í•œê³„ë¡œ ì¸í•´)
            for frame in batch_frames:
                try:
                    result = inference_bottomup(self.model, frame)
                    
                    if result and len(result.pred_instances) > 0:
                        keypoints = result.pred_instances.keypoints.cpu().numpy()
                        scores = result.pred_instances.keypoint_scores.cpu().numpy()
                        batch_results.append((keypoints, scores))
                    else:
                        # ë¹ˆ ê²°ê³¼
                        batch_results.append((np.empty((0, 17, 2)), np.empty((0, 17))))
                        
                except Exception as e:
                    logger.warning(f"í”„ë ˆì„ ì¶”ë¡  ì‹¤íŒ¨: {e}")
                    batch_results.append((np.empty((0, 17, 2)), np.empty((0, 17))))
            
            results.extend(batch_results)
        
        return results
    
    def estimate_poses_from_video_optimized(self, video_path: str, 
                                          max_frames: Optional[int] = None) -> List[Tuple[np.ndarray, np.ndarray]]:
        """
        ìµœì í™”ëœ ë¹„ë””ì˜¤ í¬ì¦ˆ ì¶”ì •
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            max_frames: ìµœëŒ€ ì²˜ë¦¬ í”„ë ˆì„ ìˆ˜
            
        Returns:
            í¬ì¦ˆ ì¶”ì • ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ìµœì í™”ëœ ë¹„ë””ì˜¤ í¬ì¦ˆ ì¶”ì • ì‹œì‘: {video_path}")
        start_time = time.time()
        
        # ë¹„ë””ì˜¤ ì—´ê¸°
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
            return []
        
        # ë¹„ë””ì˜¤ ì •ë³´
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        if max_frames:
            total_frames = min(total_frames, max_frames)
        
        logger.info(f"ë¹„ë””ì˜¤ ì •ë³´: {total_frames}í”„ë ˆì„, {fps:.1f} FPS")
        
        # í”„ë ˆì„ ì½ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì²­í¬ ë‹¨ìœ„ë¡œ)
        chunk_size = self.batch_size * 4  # ë°°ì¹˜ í¬ê¸°ì˜ 4ë°°ì”© ì²˜ë¦¬
        all_results = []
        
        for chunk_start in range(0, total_frames, chunk_size):
            chunk_end = min(chunk_start + chunk_size, total_frames)
            frames = []
            
            # ì²­í¬ í”„ë ˆì„ ì½ê¸°
            for frame_idx in range(chunk_start, chunk_end):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                
                if not ret:
                    logger.warning(f"í”„ë ˆì„ {frame_idx} ì½ê¸° ì‹¤íŒ¨")
                    continue
                    
                frames.append(frame)
            
            if not frames:
                continue
            
            # ë°°ì¹˜ í¬ì¦ˆ ì¶”ì •
            chunk_results = self.estimate_poses_batch(frames)
            all_results.extend(chunk_results)
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            progress = (chunk_end / total_frames) * 100
            elapsed = time.time() - start_time
            current_fps = chunk_end / elapsed if elapsed > 0 else 0
            
            logger.info(f"ì§„í–‰ë¥ : {progress:.1f}% ({chunk_end}/{total_frames}), "
                       f"í˜„ì¬ FPS: {current_fps:.1f}")
        
        cap.release()
        
        # ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸
        total_time = time.time() - start_time
        final_fps = len(all_results) / total_time if total_time > 0 else 0
        
        logger.info(f"ìµœì í™”ëœ í¬ì¦ˆ ì¶”ì • ì™„ë£Œ: {len(all_results)}í”„ë ˆì„, "
                   f"{total_time:.2f}ì´ˆ, {final_fps:.1f} FPS")
        
        return all_results
    
    def get_valid_poses_count(self, pose_results: List[Tuple]) -> int:
        """ìœ íš¨í•œ í¬ì¦ˆ ê°œìˆ˜ ë°˜í™˜"""
        return sum(1 for keypoints, scores in pose_results if len(keypoints) > 0)

# ì„±ëŠ¥ ë¹„êµ í•¨ìˆ˜
def compare_performance(original_estimator, optimized_estimator, video_path: str):
    """ì›ë³¸ vs ìµœì í™” ë²„ì „ ì„±ëŠ¥ ë¹„êµ"""
    print("ğŸš€ í¬ì¦ˆ ì¶”ì • ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸")
    
    # ì›ë³¸ ë²„ì „
    print("1. ì›ë³¸ RTMO ì¶”ì •ê¸° í…ŒìŠ¤íŠ¸...")
    start_time = time.time()
    original_results = original_estimator.estimate_poses_from_video(video_path, max_frames=100)
    original_time = time.time() - start_time
    original_fps = len(original_results) / original_time if original_time > 0 else 0
    
    # ìµœì í™” ë²„ì „
    print("2. ìµœì í™”ëœ RTMO ì¶”ì •ê¸° í…ŒìŠ¤íŠ¸...")
    start_time = time.time()
    optimized_results = optimized_estimator.estimate_poses_from_video_optimized(video_path, max_frames=100)
    optimized_time = time.time() - start_time
    optimized_fps = len(optimized_results) / optimized_time if optimized_time > 0 else 0
    
    # ê²°ê³¼ ë¹„êµ
    speedup = optimized_fps / original_fps if original_fps > 0 else 0
    
    print(f"\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:")
    print(f"ì›ë³¸:     {original_fps:.1f} FPS ({original_time:.2f}ì´ˆ)")
    print(f"ìµœì í™”:   {optimized_fps:.1f} FPS ({optimized_time:.2f}ì´ˆ)")
    print(f"ì„±ëŠ¥ í–¥ìƒ: {speedup:.1f}x ë¹ ë¦„")
    
    return {
        'original_fps': original_fps,
        'optimized_fps': optimized_fps,
        'speedup': speedup,
        'original_time': original_time,
        'optimized_time': optimized_time
    }