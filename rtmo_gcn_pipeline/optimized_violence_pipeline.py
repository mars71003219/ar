#!/usr/bin/env python3
"""
Optimized STGCN++ Violence Detection Pipeline
ìµœì í™”ëœ ì‹¸ì›€ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸ - Fight-ìš°ì„  íŠ¸ë˜í‚¹ ì •ë ¬ ì‹œìŠ¤í…œ

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. Fight-ìš°ì„  íŠ¸ë˜í‚¹ ì •ë ¬: 5ì˜ì—­ ë³µí•© ì ìˆ˜ ê¸°ë°˜ ì¸ë¬¼ ì„ íƒ
2. ë°°ì¹˜ ì¶”ë¡  ìµœì í™”: GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í™œìš©
3. ëª¨ë¸ ì‚¬ì „ ë¡œë“œ: ì´ˆê¸°í™” ì˜¤ë²„í—¤ë“œ ì œê±°
4. ë©”ëª¨ë¦¬ í’€ë§: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í…ì„œ ë²„í¼
5. íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”: í¬ì¦ˆ ì¶”ì •ê³¼ ë¶„ë¥˜ ë™ì‹œ ì‹¤í–‰

Architecture: Video Input â†’ Pose Estimation â†’ Fight-Prioritized Tracking â†’ STGCN++ Classification â†’ Fight/NonFight Output
"""

import numpy as np
import cv2
import torch
import torch.nn.functional as F
from collections import defaultdict, deque
import time
import os
import os.path as osp
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Union
import threading
import queue
from concurrent.futures import ThreadPoolExecutor
import multiprocessing as mp
from tempfile import TemporaryDirectory
import pickle
import logging

# MMPose and MMAction2
from mmpose.apis import init_model as init_pose_model, inference_bottomup
from mmaction.apis import init_recognizer, inference_skeleton
from mmaction.utils import frame_extract

# ë©”ëª¨ë¦¬ ê´€ë¦¬
import gc
import psutil

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MemoryPool:
    """GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì  í™œìš©ì„ ìœ„í•œ í…ì„œ í’€"""
    
    def __init__(self, device='cuda:0', max_pool_size=100):
        self.device = device
        self.max_pool_size = max_pool_size
        self.tensor_pool = defaultdict(deque)
        self.pool_lock = threading.Lock()
        
    def get_tensor(self, shape: tuple, dtype=torch.float32) -> torch.Tensor:
        """í…ì„œ í’€ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í…ì„œ íšë“"""
        key = (shape, dtype)
        
        with self.pool_lock:
            if self.tensor_pool[key]:
                tensor = self.tensor_pool[key].popleft()
                tensor.zero_()
                return tensor
            
        # ìƒˆ í…ì„œ ìƒì„±
        return torch.zeros(shape, dtype=dtype, device=self.device)
    
    def return_tensor(self, tensor: torch.Tensor):
        """í…ì„œë¥¼ í’€ì— ë°˜í™˜"""
        if tensor.device.type != self.device.split(':')[0]:
            return
            
        key = (tuple(tensor.shape), tensor.dtype)
        
        with self.pool_lock:
            if len(self.tensor_pool[key]) < self.max_pool_size:
                self.tensor_pool[key].append(tensor)
    
    def clear_pool(self):
        """ë©”ëª¨ë¦¬ í’€ ì •ë¦¬"""
        with self.pool_lock:
            self.tensor_pool.clear()
            if 'cuda' in self.device:
                torch.cuda.empty_cache()


class FightPrioritizedTracker:
    """
    Fight-ìš°ì„  ì¸ë¬¼ íŠ¸ë˜í‚¹ ì‹œìŠ¤í…œ
    5ì˜ì—­ ë¶„í•  ê¸°ë°˜ ë³µí•© ì ìˆ˜ë¡œ ì‹¸ì›€ ê´€ë ¨ ì¸ë¬¼ì„ ìµœìƒìœ„ë¡œ ì •ë ¬
    """
    
    def __init__(self, frame_width=640, frame_height=480):
        self.frame_width = frame_width
        self.frame_height = frame_height
        
        # 5ì˜ì—­ ì •ì˜ (enhanced_rtmo_bytetrack_pose_extraction.py ê¸°ë°˜)
        self.regions = self._define_regions()
        
        # ì ì‘ì  ì˜ì—­ ê°€ì¤‘ì¹˜ (ì‹œê°„ì— ë”°ë¼ í•™ìŠµ)
        self.region_weights = {
            'center': 1.0,         # ì¤‘ì•™ ì˜ì—­ (ê°€ì¥ ì¤‘ìš”)
            'top_left': 0.7,       # ì¢Œìƒë‹¨
            'top_right': 0.7,      # ìš°ìƒë‹¨
            'bottom_left': 0.6,    # ì¢Œí•˜ë‹¨
            'bottom_right': 0.6    # ìš°í•˜ë‹¨
        }
        
        # íŠ¸ë˜í‚¹ íˆìŠ¤í† ë¦¬ (ì‹œê°„ì  ì¼ê´€ì„±)
        self.person_history = defaultdict(lambda: {
            'composite_scores': deque(maxlen=10),
            'positions': deque(maxlen=10),
            'last_seen': 0,
            'consistency_score': 0.0
        })
        
        self.frame_count = 0
    
    def _define_regions(self) -> Dict[str, Tuple[int, int, int, int]]:
        """5ì˜ì—­ ì •ì˜ - ì „ì²´ 4ë¶„í•  + ì¤‘ì•™ ì§‘ì¤‘ (enhanced_rtmo_bytetrack_pose_extraction.py ë°©ì‹)"""
        w, h = self.frame_width, self.frame_height
        
        return {
            # ì „ì²´ 4ë¶„í•  (ì™„ì „í•œ ê³µê°„ ì»¤ë²„ë¦¬ì§€)
            'top_left': (0, 0, w//2, h//2),              # ì¢Œìƒë‹¨ ì˜ì—­
            'top_right': (w//2, 0, w, h//2),             # ìš°ìƒë‹¨ ì˜ì—­  
            'bottom_left': (0, h//2, w//2, h),           # ì¢Œí•˜ë‹¨ ì˜ì—­
            'bottom_right': (w//2, h//2, w, h),          # ìš°í•˜ë‹¨ ì˜ì—­
            
            # ì¤‘ì•™ ì§‘ì¤‘ ì˜ì—­ (ê°€ì¥ ì¤‘ìš” - ì‹¸ì›€ì´ ì£¼ë¡œ ë°œìƒí•˜ëŠ” êµ¬ì—­)
            'center': (w//4, h//4, 3*w//4, 3*h//4)       # ì¤‘ì•™ 50% ì˜ì—­
        }
    
    def _calculate_position_score(self, keypoints: np.ndarray) -> Dict[str, float]:
        """ì¸ë¬¼ì˜ ì˜ì—­ë³„ ìœ„ì¹˜ ì ìˆ˜ ê³„ì‚°"""
        if len(keypoints.shape) == 3:
            keypoints = keypoints[0]  # (17, 2)
        
        # ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸ì˜ ì¤‘ì‹¬ì  ê³„ì‚°
        valid_points = keypoints[keypoints[:, 0] > 0]
        if len(valid_points) == 0:
            return {region: 0.0 for region in self.regions}
        
        center_x = np.mean(valid_points[:, 0])
        center_y = np.mean(valid_points[:, 1])
        
        region_scores = {}
        for region_name, (x1, y1, x2, y2) in self.regions.items():
            # ì¤‘ì‹¬ì ì´ ì˜ì—­ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
            if x1 <= center_x <= x2 and y1 <= center_y <= y2:
                # ì˜ì—­ ì¤‘ì•™ì—ì„œì˜ ê±°ë¦¬ì— ë”°ë¥¸ ì ìˆ˜ (0.5 ~ 1.0)
                region_center_x = (x1 + x2) / 2
                region_center_y = (y1 + y2) / 2
                distance = np.sqrt((center_x - region_center_x)**2 + (center_y - region_center_y)**2)
                max_distance = np.sqrt((x2 - x1)**2 + (y2 - y1)**2) / 2
                score = max(0.5, 1.0 - (distance / max_distance) * 0.5)
                region_scores[region_name] = score * self.region_weights[region_name]
            else:
                region_scores[region_name] = 0.0
        
        return region_scores
    
    def _calculate_movement_score(self, keypoints: np.ndarray, person_id: int) -> float:
        """ë™ì‘ì˜ ê²©ë ¬í•¨ ì ìˆ˜ ê³„ì‚°"""
        if person_id not in self.person_history:
            return 0.5
        
        history = self.person_history[person_id]
        if len(history['positions']) < 2:
            return 0.5
        
        # ìµœê·¼ ìœ„ì¹˜ ë³€í™”ëŸ‰ ê³„ì‚°
        current_pos = np.mean(keypoints[keypoints[:, 0] > 0], axis=0) if len(keypoints[keypoints[:, 0] > 0]) > 0 else np.array([0, 0])
        prev_pos = history['positions'][-1]
        
        movement = np.linalg.norm(current_pos - prev_pos)
        
        # ì •ê·œí™” (0.0 ~ 1.0)
        movement_score = min(1.0, movement / 50.0)  # 50í”½ì…€ ì´ë™ì„ ìµœëŒ€ê°’ìœ¼ë¡œ ì„¤ì •
        
        return movement_score
    
    def _calculate_interaction_score(self, keypoints_list: List[np.ndarray]) -> List[float]:
        """ì¸ë¬¼ ê°„ ìƒí˜¸ì‘ìš© ì ìˆ˜ ê³„ì‚°"""
        if len(keypoints_list) < 2:
            return [0.5] * len(keypoints_list)
        
        interaction_scores = []
        
        for i, keypoints_i in enumerate(keypoints_list):
            max_interaction = 0.0
            
            for j, keypoints_j in enumerate(keypoints_list):
                if i == j:
                    continue
                
                # ë‘ ì¸ë¬¼ ê°„ ê±°ë¦¬ ê³„ì‚°
                center_i = np.mean(keypoints_i[keypoints_i[:, 0] > 0], axis=0) if len(keypoints_i[keypoints_i[:, 0] > 0]) > 0 else np.array([0, 0])
                center_j = np.mean(keypoints_j[keypoints_j[:, 0] > 0], axis=0) if len(keypoints_j[keypoints_j[:, 0] > 0]) > 0 else np.array([0, 0])
                
                distance = np.linalg.norm(center_i - center_j)
                
                # ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ìƒí˜¸ì‘ìš© ì ìˆ˜ (100í”½ì…€ ì´ë‚´ì—ì„œ ìµœëŒ€ê°’)
                if distance > 0:
                    interaction = max(0.0, 1.0 - (distance / 100.0))
                    max_interaction = max(max_interaction, interaction)
            
            interaction_scores.append(max_interaction)
        
        return interaction_scores
    
    def calculate_composite_scores(self, keypoints_list: List[np.ndarray], scores_list: List[np.ndarray]) -> List[float]:
        """ë³µí•© ì ìˆ˜ ê³„ì‚° - ì‹¸ì›€ ê´€ë ¨ ì¸ë¬¼ì„ ìµœìƒìœ„ë¡œ ì •ë ¬"""
        if not keypoints_list:
            return []
        
        self.frame_count += 1
        composite_scores = []
        
        # ìƒí˜¸ì‘ìš© ì ìˆ˜ ê³„ì‚°
        interaction_scores = self._calculate_interaction_score(keypoints_list)
        
        for i, (keypoints, scores) in enumerate(zip(keypoints_list, scores_list)):
            # 1. ìœ„ì¹˜ ì ìˆ˜ (ì˜ì—­ë³„ ê°€ì¤‘ì¹˜ ì ìš©)
            position_scores = self._calculate_position_score(keypoints)
            position_score = max(position_scores.values())
            
            # 2. ì›€ì§ì„ ì ìˆ˜
            movement_score = self._calculate_movement_score(keypoints, i)
            
            # 3. ìƒí˜¸ì‘ìš© ì ìˆ˜
            interaction_score = interaction_scores[i]
            
            # 4. ê²€ì¶œ ì‹ ë¢°ë„ ì ìˆ˜
            detection_score = np.mean(scores[scores > 0]) if len(scores[scores > 0]) > 0 else 0.0
            
            # 5. ì‹œê°„ì  ì¼ê´€ì„± ì ìˆ˜
            consistency_score = self.person_history[i]['consistency_score']
            
            # ë³µí•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘ í‰ê· )
            composite_score = (
                position_score * 0.3 +          # ìœ„ì¹˜ (30%)
                movement_score * 0.25 +         # ì›€ì§ì„ (25%)
                interaction_score * 0.25 +      # ìƒí˜¸ì‘ìš© (25%)
                detection_score * 0.1 +         # ê²€ì¶œ ì‹ ë¢°ë„ (10%)
                consistency_score * 0.1         # ì‹œê°„ì  ì¼ê´€ì„± (10%)
            )
            
            composite_scores.append(composite_score)
            
            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.person_history[i]['composite_scores'].append(composite_score)
            self.person_history[i]['positions'].append(
                np.mean(keypoints[keypoints[:, 0] > 0], axis=0) if len(keypoints[keypoints[:, 0] > 0]) > 0 else np.array([0, 0])
            )
            self.person_history[i]['last_seen'] = self.frame_count
            
            # ì¼ê´€ì„± ì ìˆ˜ ì—…ë°ì´íŠ¸ (ìµœê·¼ ì ìˆ˜ë“¤ì˜ í‘œì¤€í¸ì°¨ ì—­ìˆ˜)
            if len(self.person_history[i]['composite_scores']) >= 3:
                recent_scores = list(self.person_history[i]['composite_scores'])[-3:]
                self.person_history[i]['consistency_score'] = 1.0 / (1.0 + np.std(recent_scores))
        
        return composite_scores
    
    def get_fight_prioritized_order(self, keypoints_list: List[np.ndarray], scores_list: List[np.ndarray]) -> List[int]:
        """ì‹¸ì›€ ìš°ì„  ì •ë ¬ëœ ì¸ë±ìŠ¤ ë°˜í™˜"""
        if not keypoints_list:
            return []
        
        composite_scores = self.calculate_composite_scores(keypoints_list, scores_list)
        
        # ë³µí•© ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        sorted_indices = sorted(range(len(composite_scores)), 
                              key=lambda i: composite_scores[i], reverse=True)
        
        return sorted_indices


class OptimizedSTGCNPipeline:
    """
    ìµœì í™”ëœ STGCN++ í­ë ¥ ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸
    End-to-End ì‹¤ì‹œê°„ ì¶”ë¡  ìµœì í™”
    """
    
    def __init__(self, pose_config: str, pose_checkpoint: str, 
                 gcn_config: str, gcn_checkpoint: str, 
                 device: str = 'cuda:0', sequence_length: int = 30):
        
        self.device = device
        self.sequence_length = sequence_length
        
        # ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (ì´ˆê¸°í™” ì˜¤ë²„í—¤ë“œ ì œê±°)
        logger.info("ğŸš€ ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì‹œì‘...")
        self.pose_model = self._init_pose_model(pose_config, pose_checkpoint)
        self.gcn_model = self._init_gcn_model(gcn_config, gcn_checkpoint)
        
        # Fight-ìš°ì„  íŠ¸ë˜ì»¤ ì´ˆê¸°í™”
        self.tracker = FightPrioritizedTracker()
        
        # ë©”ëª¨ë¦¬ í’€ ì´ˆê¸°í™”
        self.memory_pool = MemoryPool(device)
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í
        self.batch_queue = queue.Queue(maxsize=100)
        self.result_queue = queue.Queue()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            'total_frames': 0,
            'inference_time': 0.0,
            'preprocessing_time': 0.0,
            'postprocessing_time': 0.0
        }
        
        # íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”
        self.num_workers = min(4, mp.cpu_count())
        self.processing_pool = ThreadPoolExecutor(max_workers=self.num_workers)
        
        logger.info("âœ… ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_pose_model(self, config_path: str, checkpoint_path: str):
        """í¬ì¦ˆ ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model = init_pose_model(config_path, checkpoint_path, device=self.device)
            logger.info("âœ… í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return model
        except Exception as e:
            logger.error(f"âŒ í¬ì¦ˆ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _init_gcn_model(self, config_path: str, checkpoint_path: str):
        """GCN ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            model = init_recognizer(config_path, checkpoint_path, device=self.device)
            logger.info("âœ… GCN ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return model
        except Exception as e:
            logger.error(f"âŒ GCN ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def extract_poses_batch(self, frames: List[np.ndarray]) -> List[Tuple[np.ndarray, np.ndarray]]:
        """ë°°ì¹˜ í¬ì¦ˆ ì¶”ì • (GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì )"""
        start_time = time.time()
        
        pose_results = []
        
        # ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì • (GPU ë©”ëª¨ë¦¬ ê¸°ë°˜)
        batch_size = self._calculate_optimal_batch_size(frames)
        
        for i in range(0, len(frames), batch_size):
            batch_frames = frames[i:i+batch_size]
            batch_poses = []
            
            for frame in batch_frames:
                try:
                    result = inference_bottomup(self.pose_model, frame)
                    keypoints, scores = self._extract_pose_data(result)
                    batch_poses.append((keypoints, scores))
                except Exception as e:
                    logger.warning(f"í¬ì¦ˆ ì¶”ì • ì‹¤íŒ¨: {e}")
                    batch_poses.append((None, None))
            
            pose_results.extend(batch_poses)
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if 'cuda' in self.device:
                torch.cuda.empty_cache()
        
        self.performance_stats['preprocessing_time'] += time.time() - start_time
        return pose_results
    
    def _extract_pose_data(self, result) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:
        """í¬ì¦ˆ ë°ì´í„° ì¶”ì¶œ"""
        try:
            if isinstance(result, list) and len(result) > 0:
                result = result[0]
            
            if hasattr(result, 'pred_instances') and len(result.pred_instances) > 0:
                instances = result.pred_instances
                
                keypoints_list = []
                scores_list = []
                
                for instance in instances:
                    if hasattr(instance, 'keypoints') and hasattr(instance, 'keypoint_scores'):
                        kpts = instance.keypoints[0] if len(instance.keypoints.shape) > 2 else instance.keypoints
                        scrs = instance.keypoint_scores[0] if len(instance.keypoint_scores.shape) > 1 else instance.keypoint_scores
                        keypoints_list.append(kpts.cpu().numpy() if hasattr(kpts, 'cpu') else kpts)
                        scores_list.append(scrs.cpu().numpy() if hasattr(scrs, 'cpu') else scrs)
                
                if keypoints_list:
                    return np.array(keypoints_list), np.array(scores_list)
            
            return None, None
            
        except Exception as e:
            logger.warning(f"í¬ì¦ˆ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None, None
    
    def _calculate_optimal_batch_size(self, frames: List[np.ndarray]) -> int:
        """GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
        if 'cuda' not in self.device:
            return 4
        
        try:
            gpu_memory = torch.cuda.get_device_properties(self.device).total_memory
            available_memory = gpu_memory - torch.cuda.memory_allocated(self.device)
            
            # í”„ë ˆì„ í¬ê¸° ê¸°ë°˜ ë°°ì¹˜ í¬ê¸° ì¶”ì •
            frame_size = frames[0].nbytes if frames else 640*480*3
            estimated_batch_size = max(1, min(8, int(available_memory * 0.3 / frame_size)))
            
            return estimated_batch_size
            
        except Exception:
            return 4
    
    def fight_prioritized_selection(self, keypoints_sequence: List[List[np.ndarray]], 
                                  scores_sequence: List[List[np.ndarray]]) -> Tuple[np.ndarray, np.ndarray]:
        """Fight-ìš°ì„  ì¸ë¬¼ ì„ íƒ ë° ì‹œí€€ìŠ¤ êµ¬ì„±"""
        if not keypoints_sequence or not scores_sequence:
            return self._get_dummy_sequence()
        
        # ê° í”„ë ˆì„ì—ì„œ ìµœê³  ì ìˆ˜ ì¸ë¬¼ ì„ íƒ
        selected_keypoints = []
        selected_scores = []
        
        for frame_keypoints, frame_scores in zip(keypoints_sequence, scores_sequence):
            if frame_keypoints is None or frame_scores is None or len(frame_keypoints) == 0:
                # ë¹ˆ í”„ë ˆì„ ì²˜ë¦¬
                selected_keypoints.append(np.zeros((17, 2)))
                selected_scores.append(np.zeros(17))
                continue
            
            # Fight-ìš°ì„  ì •ë ¬
            fight_order = self.tracker.get_fight_prioritized_order(frame_keypoints, frame_scores)
            
            if fight_order:
                # ìµœê³  ì ìˆ˜ ì¸ë¬¼ ì„ íƒ
                best_idx = fight_order[0]
                selected_keypoints.append(frame_keypoints[best_idx])
                selected_scores.append(frame_scores[best_idx])
            else:
                selected_keypoints.append(np.zeros((17, 2)))
                selected_scores.append(np.zeros(17))
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ì¡°ì •
        if len(selected_keypoints) < self.sequence_length:
            # íŒ¨ë”© (ë§ˆì§€ë§‰ í”„ë ˆì„ ë°˜ë³µ)
            needed = self.sequence_length - len(selected_keypoints)
            if selected_keypoints:
                last_kpts = selected_keypoints[-1]
                last_scores = selected_scores[-1]
                for _ in range(needed):
                    selected_keypoints.append(last_kpts.copy())
                    selected_scores.append(last_scores.copy())
            else:
                # ì™„ì „íˆ ë¹ˆ ì‹œí€€ìŠ¤
                for _ in range(self.sequence_length):
                    selected_keypoints.append(np.zeros((17, 2)))
                    selected_scores.append(np.zeros(17))
        else:
            # ìµœì‹  í”„ë ˆì„ë“¤ë§Œ ì‚¬ìš©
            selected_keypoints = selected_keypoints[-self.sequence_length:]
            selected_scores = selected_scores[-self.sequence_length:]
        
        return np.array(selected_keypoints), np.array(selected_scores)
    
    def _get_dummy_sequence(self) -> Tuple[np.ndarray, np.ndarray]:
        """ë”ë¯¸ ì‹œí€€ìŠ¤ ìƒì„±"""
        dummy_keypoints = np.zeros((self.sequence_length, 17, 2))
        dummy_scores = np.zeros((self.sequence_length, 17))
        return dummy_keypoints, dummy_scores
    
    def preprocess_for_gcn(self, keypoints: np.ndarray, scores: np.ndarray) -> List[Dict]:
        """GCN ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ì „ì²˜ë¦¬"""
        pose_results = []
        
        for i in range(keypoints.shape[0]):
            frame_keypoints = keypoints[i:i+1]  # (1, 17, 2)
            frame_scores = scores[i:i+1]        # (1, 17)
            
            # ì‹ ë¢°ë„ í•„í„°ë§
            mask = frame_scores > 0.3
            frame_keypoints[~mask] = 0
            
            frame_result = {
                'keypoints': frame_keypoints,
                'keypoint_scores': frame_scores
            }
            pose_results.append(frame_result)
        
        return pose_results
    
    def batch_gcn_inference(self, pose_results_batch: List[List[Dict]]) -> List[Tuple[int, float]]:
        """ë°°ì¹˜ GCN ì¶”ë¡ """
        start_time = time.time()
        
        results = []
        img_shape = (480, 640)  # í‘œì¤€ ì´ë¯¸ì§€ í¬ê¸°
        
        for pose_results in pose_results_batch:
            try:
                result = inference_skeleton(
                    model=self.gcn_model,
                    pose_results=pose_results,
                    img_shape=img_shape
                )
                
                if hasattr(result, 'pred_score'):
                    pred_scores = result.pred_score.cpu().numpy()
                    confidence = float(np.max(pred_scores))
                    prediction = int(np.argmax(pred_scores))
                else:
                    confidence = 0.5
                    prediction = 0
                
                results.append((prediction, confidence))
                
            except Exception as e:
                logger.warning(f"GCN ì¶”ë¡  ì‹¤íŒ¨: {e}")
                results.append((0, 0.5))
        
        self.performance_stats['inference_time'] += time.time() - start_time
        return results
    
    def process_video_optimized(self, video_path: str, output_dir: Optional[str] = None) -> Dict:
        """ìµœì í™”ëœ ë¹„ë””ì˜¤ ì²˜ë¦¬"""
        logger.info(f"ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘: {osp.basename(video_path)}")
        
        start_time = time.time()
        
        try:
            # í”„ë ˆì„ ì¶”ì¶œ
            with TemporaryDirectory() as tmp_dir:
                frame_paths = frame_extract(video_path, out_dir=tmp_dir)
                if not frame_paths:
                    return {'error': 'Frame extraction failed'}
                
                # í”„ë ˆì„ ë¡œë“œ
                frames = []
                for frame_path in frame_paths:
                    frame = cv2.imread(frame_path)
                    if frame is not None:
                        frames.append(frame)
                
                if not frames:
                    return {'error': 'No valid frames found'}
                
                # ë°°ì¹˜ í¬ì¦ˆ ì¶”ì •
                pose_results = self.extract_poses_batch(frames)
                
                # Fight-ìš°ì„  ì‹œí€€ìŠ¤ êµ¬ì„±
                keypoints_sequence = []
                scores_sequence = []
                
                for keypoints, scores in pose_results:
                    if keypoints is not None and scores is not None:
                        keypoints_sequence.append(keypoints)
                        scores_sequence.append(scores)
                    else:
                        keypoints_sequence.append([])
                        scores_sequence.append([])
                
                # ìœˆë„ìš° ê¸°ë°˜ ì¶”ë¡  (overlapping windows)
                window_predictions = []
                window_confidences = []
                
                window_size = self.sequence_length
                stride = window_size // 2  # 50% ì˜¤ë²„ë©
                
                for start_idx in range(0, len(keypoints_sequence) - window_size + 1, stride):
                    end_idx = start_idx + window_size
                    
                    window_keypoints = keypoints_sequence[start_idx:end_idx]
                    window_scores = scores_sequence[start_idx:end_idx]
                    
                    # Fight-ìš°ì„  ì„ íƒ
                    selected_keypoints, selected_scores = self.fight_prioritized_selection(
                        window_keypoints, window_scores
                    )
                    
                    # GCN ì „ì²˜ë¦¬
                    pose_results_gcn = self.preprocess_for_gcn(selected_keypoints, selected_scores)
                    
                    # ë°°ì¹˜ ì¶”ë¡ 
                    batch_results = self.batch_gcn_inference([pose_results_gcn])
                    prediction, confidence = batch_results[0]
                    
                    window_predictions.append(prediction)
                    window_confidences.append(confidence)
                
                # ìµœì¢… ê²°ê³¼ ê²°ì • (majority voting + confidence weighting)
                if window_predictions:
                    weighted_votes = sum(pred * conf for pred, conf in zip(window_predictions, window_confidences))
                    total_confidence = sum(window_confidences)
                    
                    if total_confidence > 0:
                        final_score = weighted_votes / total_confidence
                        final_prediction = 1 if final_score > 0.5 else 0
                        final_confidence = total_confidence / len(window_confidences)
                    else:
                        final_prediction = 0
                        final_confidence = 0.5
                else:
                    final_prediction = 0
                    final_confidence = 0.5
                
                # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
                processing_time = time.time() - start_time
                self.performance_stats['total_frames'] += len(frames)
                
                # ê²°ê³¼ êµ¬ì„±
                result = {
                    'video_path': video_path,
                    'prediction': final_prediction,
                    'confidence': final_confidence,
                    'prediction_label': 'Fight' if final_prediction == 1 else 'NonFight',
                    'window_predictions': window_predictions,
                    'window_confidences': window_confidences,
                    'total_frames': len(frames),
                    'processing_time': processing_time,
                    'fps': len(frames) / processing_time if processing_time > 0 else 0
                }
                
                # ê²°ê³¼ ì €ì¥
                if output_dir:
                    self._save_result(result, output_dir)
                
                logger.info(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {osp.basename(video_path)} - {result['prediction_label']} ({result['confidence']:.3f})")
                
                return result
                
        except Exception as e:
            logger.error(f"âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'error': str(e), 'video_path': video_path}
    
    def _save_result(self, result: Dict, output_dir: str):
        """ê²°ê³¼ ì €ì¥"""
        os.makedirs(output_dir, exist_ok=True)
        
        # JSON ê²°ê³¼ ì €ì¥
        import json
        video_name = osp.splitext(osp.basename(result['video_path']))[0]
        result_path = osp.join(output_dir, f"{video_name}_result.json")
        
        with open(result_path, 'w') as f:
            json.dump(result, f, indent=2)
    
    def process_batch_videos(self, video_paths: List[str], output_dir: Optional[str] = None, 
                           max_workers: int = 4) -> List[Dict]:
        """ë°°ì¹˜ ë¹„ë””ì˜¤ ì²˜ë¦¬ (ë³‘ë ¬)"""
        logger.info(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(video_paths)}ê°œ ë¹„ë””ì˜¤")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_video = {
                executor.submit(self.process_video_optimized, video_path, output_dir): video_path
                for video_path in video_paths
            }
            
            for future in future_to_video:
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    video_path = future_to_video[future]
                    logger.error(f"âŒ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨: {video_path} - {e}")
                    results.append({'error': str(e), 'video_path': video_path})
        
        logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        
        return results
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        stats = self.performance_stats.copy()
        
        if stats['total_frames'] > 0:
            stats['avg_inference_time_per_frame'] = stats['inference_time'] / stats['total_frames']
            stats['avg_preprocessing_time_per_frame'] = stats['preprocessing_time'] / stats['total_frames']
            stats['fps'] = stats['total_frames'] / (stats['inference_time'] + stats['preprocessing_time'])
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        if 'cuda' in self.device:
            stats['gpu_memory_allocated'] = torch.cuda.memory_allocated(self.device)
            stats['gpu_memory_reserved'] = torch.cuda.memory_reserved(self.device)
        
        return stats
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        self.memory_pool.clear_pool()
        self.processing_pool.shutdown(wait=True)
        
        if 'cuda' in self.device:
            torch.cuda.empty_cache()
        
        gc.collect()
        
        logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Optimized STGCN++ Violence Detection Pipeline')
    parser.add_argument('--pose-config', required=True, help='Pose model config path')
    parser.add_argument('--pose-checkpoint', required=True, help='Pose model checkpoint path')
    parser.add_argument('--gcn-config', required=True, help='GCN model config path')
    parser.add_argument('--gcn-checkpoint', required=True, help='GCN model checkpoint path')
    parser.add_argument('--input', '-i', required=True, help='Input video path or directory')
    parser.add_argument('--output', '-o', help='Output directory for results')
    parser.add_argument('--device', default='cuda:0', help='Device for inference')
    parser.add_argument('--sequence-length', type=int, default=30, help='Sequence length for GCN')
    parser.add_argument('--max-workers', type=int, default=4, help='Max workers for parallel processing')
    parser.add_argument('--batch-size', type=int, default=8, help='Batch size for processing')
    
    args = parser.parse_args()
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = OptimizedSTGCNPipeline(
        pose_config=args.pose_config,
        pose_checkpoint=args.pose_checkpoint,
        gcn_config=args.gcn_config,
        gcn_checkpoint=args.gcn_checkpoint,
        device=args.device,
        sequence_length=args.sequence_length
    )
    
    try:
        # ì…ë ¥ íŒŒì¼ ìˆ˜ì§‘
        video_paths = []
        if osp.isfile(args.input):
            video_paths = [args.input]
        elif osp.isdir(args.input):
            for ext in ['*.mp4', '*.avi', '*.mov', '*.mkv']:
                video_paths.extend(Path(args.input).rglob(ext))
            video_paths = [str(p) for p in video_paths]
        else:
            logger.error(f"âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ ê²½ë¡œ: {args.input}")
            return
        
        if not video_paths:
            logger.error("âŒ ì²˜ë¦¬í•  ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ“ ì´ {len(video_paths)}ê°œ ë¹„ë””ì˜¤ ë°œê²¬")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        results = pipeline.process_batch_videos(
            video_paths=video_paths,
            output_dir=args.output,
            max_workers=args.max_workers
        )
        
        # ê²°ê³¼ ìš”ì•½
        successful = sum(1 for r in results if 'error' not in r)
        failed = len(results) - successful
        fight_count = sum(1 for r in results if 'error' not in r and r['prediction'] == 1)
        nonfight_count = successful - fight_count
        
        logger.info(f"""
ğŸ¯ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:
   - ì„±ê³µ: {successful}ê°œ
   - ì‹¤íŒ¨: {failed}ê°œ
   - Fight: {fight_count}ê°œ
   - NonFight: {nonfight_count}ê°œ
        """)
        
        # ì„±ëŠ¥ í†µê³„
        stats = pipeline.get_performance_stats()
        logger.info(f"""
âš¡ ì„±ëŠ¥ í†µê³„:
   - ì´ í”„ë ˆì„: {stats['total_frames']}
   - í‰ê·  FPS: {stats.get('fps', 0):.2f}
   - ì „ì²˜ë¦¬ ì‹œê°„: {stats['preprocessing_time']:.2f}ì´ˆ
   - ì¶”ë¡  ì‹œê°„: {stats['inference_time']:.2f}ì´ˆ
        """)
        
    finally:
        # ì •ë¦¬
        pipeline.cleanup()


if __name__ == '__main__':
    main()