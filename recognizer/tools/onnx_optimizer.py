#!/usr/bin/env python3
"""
ONNX GPU ìµœì í™” ìë™ ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬

GPUë³„ë¡œ ìµœì ì˜ ONNXRuntime ì„¤ì •ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ í™˜ê²½ì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •ì„ ìµœì í™”í•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import json
import yaml
import argparse
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import logging

# ONNXRuntime ê´€ë ¨ import
try:
    import onnxruntime as ort
    import torch
    HAS_ONNX = True
except ImportError:
    HAS_ONNX = False


@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì • êµ¬ì¡°"""
    # cuDNN ì„¤ì •
    cudnn_conv_algo_search: str  # 'DEFAULT', 'HEURISTIC', 'EXHAUSTIVE'
    do_copy_in_default_stream: bool
    cudnn_conv_use_max_workspace: bool
    tunable_op_enable: bool
    tunable_op_tuning_enable: bool
    
    # GPU ë©”ëª¨ë¦¬ ì„¤ì •
    gpu_mem_limit_gb: int  # GB ë‹¨ìœ„
    arena_extend_strategy: str  # 'kNextPowerOfTwo', 'kSameAsRequested'
    
    # ì„¸ì…˜ ì„¤ì •
    execution_mode: str  # 'ORT_SEQUENTIAL', 'ORT_PARALLEL'
    graph_optimization_level: str  # 'ORT_ENABLE_ALL', 'ORT_ENABLE_BASIC'
    enable_cpu_mem_arena: bool
    enable_mem_pattern: bool
    
    def to_provider_options(self) -> Dict[str, Any]:
        """ONNXRuntime Provider ì˜µì…˜ìœ¼ë¡œ ë³€í™˜"""
        return {
            'device_id': 0,
            'gpu_mem_limit': self.gpu_mem_limit_gb * 1024 * 1024 * 1024,
            'arena_extend_strategy': self.arena_extend_strategy,
            'cudnn_conv_algo_search': self.cudnn_conv_algo_search,
            'do_copy_in_default_stream': self.do_copy_in_default_stream,
            'cudnn_conv_use_max_workspace': self.cudnn_conv_use_max_workspace,
            'tunable_op_enable': self.tunable_op_enable,
            'tunable_op_tuning_enable': self.tunable_op_tuning_enable,
        }
    
    def to_session_options(self) -> ort.SessionOptions:
        """ONNXRuntime ì„¸ì…˜ ì˜µì…˜ìœ¼ë¡œ ë³€í™˜"""
        session_options = ort.SessionOptions()
        session_options.execution_mode = getattr(ort.ExecutionMode, self.execution_mode)
        session_options.graph_optimization_level = getattr(ort.GraphOptimizationLevel, self.graph_optimization_level)
        session_options.enable_cpu_mem_arena = self.enable_cpu_mem_arena
        session_options.enable_mem_pattern = self.enable_mem_pattern
        session_options.log_severity_level = 3  # ê²½ê³  ì¤„ì´ê¸°
        return session_options


@dataclass
class BenchmarkResult:
    """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ êµ¬ì¡°"""
    config: OptimizationConfig
    avg_time_ms: float
    std_time_ms: float
    fps: float
    success: bool
    error_message: Optional[str] = None
    gpu_provider_active: bool = False
    
    def score(self) -> float:
        """ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚° (FPS ê¸°ë°˜, ì‹¤íŒ¨ì‹œ 0)"""
        return self.fps if self.success and self.gpu_provider_active else 0.0


class ONNXOptimizerBenchmark:
    """ONNX ìµœì í™” ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, warmup_runs: int = 20, test_runs: int = 50):
        self.model_path = model_path
        self.warmup_runs = warmup_runs
        self.test_runs = test_runs
        self.logger = self._setup_logger()
        
        # í™˜ê²½ ì •ë³´ ìˆ˜ì§‘
        self.gpu_info = self._get_gpu_info()
        self.system_info = self._get_system_info()
        
    def _setup_logger(self) -> logging.Logger:
        """ë¡œê¹… ì„¤ì •"""
        logger = logging.getLogger('ONNXOptimizer')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            
        return logger
        
    def _get_gpu_info(self) -> Dict[str, Any]:
        """GPU ì •ë³´ ìˆ˜ì§‘"""
        if not torch.cuda.is_available():
            return {'available': False}
            
        gpu_props = torch.cuda.get_device_properties(0)
        return {
            'available': True,
            'name': gpu_props.name,
            'total_memory_gb': gpu_props.total_memory / 1024**3,
            'multi_processor_count': gpu_props.multi_processor_count,
            'compute_capability': torch.cuda.get_device_capability(0),
            'cuda_version': torch.version.cuda,
            'cudnn_version': torch.backends.cudnn.version(),
        }
        
    def _get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ìˆ˜ì§‘"""
        return {
            'onnxruntime_version': ort.__version__ if HAS_ONNX else 'N/A',
            'python_version': sys.version.split()[0],
            'platform': sys.platform,
        }
    
    def generate_config_combinations(self) -> List[OptimizationConfig]:
        """ìµœì í™” ì„¤ì • ì¡°í•© ìƒì„±"""
        combinations = []
        
        # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ì„¤ì •
        available_memory = self.gpu_info.get('total_memory_gb', 24)
        memory_limits = [
            min(4, int(available_memory * 0.2)),
            min(8, int(available_memory * 0.4)), 
            min(16, int(available_memory * 0.7)),
            min(24, int(available_memory * 0.9)),
        ]
        
        # ì£¼ìš” ì„¤ì • ì¡°í•©
        base_configs = [
            # ë¹ ë¥¸ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜
            {
                'cudnn_conv_algo_search': 'HEURISTIC',
                'do_copy_in_default_stream': False,
                'cudnn_conv_use_max_workspace': True,
                'tunable_op_enable': True,
                'tunable_op_tuning_enable': True,
                'execution_mode': 'ORT_SEQUENTIAL',
                'graph_optimization_level': 'ORT_ENABLE_ALL',
                'enable_cpu_mem_arena': True,
                'enable_mem_pattern': True,
                'arena_extend_strategy': 'kNextPowerOfTwo',
            },
            # ì „ìˆ˜íƒìƒ‰ ê¸°ë°˜
            {
                'cudnn_conv_algo_search': 'EXHAUSTIVE',
                'do_copy_in_default_stream': False,
                'cudnn_conv_use_max_workspace': True,
                'tunable_op_enable': False,
                'tunable_op_tuning_enable': False,
                'execution_mode': 'ORT_SEQUENTIAL',
                'graph_optimization_level': 'ORT_ENABLE_ALL',
                'enable_cpu_mem_arena': True,
                'enable_mem_pattern': True,
                'arena_extend_strategy': 'kNextPowerOfTwo',
            },
            # ë³´ìˆ˜ì  ì„¤ì •
            {
                'cudnn_conv_algo_search': 'DEFAULT',
                'do_copy_in_default_stream': True,
                'cudnn_conv_use_max_workspace': False,
                'tunable_op_enable': False,
                'tunable_op_tuning_enable': False,
                'execution_mode': 'ORT_SEQUENTIAL',
                'graph_optimization_level': 'ORT_ENABLE_BASIC',
                'enable_cpu_mem_arena': True,
                'enable_mem_pattern': True,
                'arena_extend_strategy': 'kSameAsRequested',
            },
        ]
        
        # ë©”ëª¨ë¦¬ í¬ê¸°ë³„ë¡œ ê° ì„¤ì • ì¡°í•©
        for base_config in base_configs:
            for mem_limit in memory_limits:
                config = OptimizationConfig(
                    gpu_mem_limit_gb=mem_limit,
                    **base_config
                )
                combinations.append(config)
                
        # ì¶”ê°€ ì‹¤í—˜ì  ì¡°í•©
        experimental_configs = [
            # ë³‘ë ¬ ì‹¤í–‰ ëª¨ë“œ
            OptimizationConfig(
                cudnn_conv_algo_search='HEURISTIC',
                do_copy_in_default_stream=False,
                cudnn_conv_use_max_workspace=True,
                tunable_op_enable=True,
                tunable_op_tuning_enable=True,
                gpu_mem_limit_gb=memory_limits[-1],
                arena_extend_strategy='kNextPowerOfTwo',
                execution_mode='ORT_PARALLEL',  # ë³‘ë ¬ ëª¨ë“œ
                graph_optimization_level='ORT_ENABLE_ALL',
                enable_cpu_mem_arena=False,  # ë³‘ë ¬ì—ì„œëŠ” False
                enable_mem_pattern=False,
            ),
            # ë©”ëª¨ë¦¬ ìµœì í™” ëª¨ë“œ
            OptimizationConfig(
                cudnn_conv_algo_search='HEURISTIC',
                do_copy_in_default_stream=True,
                cudnn_conv_use_max_workspace=False,
                tunable_op_enable=False,
                tunable_op_tuning_enable=False,
                gpu_mem_limit_gb=memory_limits[0],  # ìµœì†Œ ë©”ëª¨ë¦¬
                arena_extend_strategy='kSameAsRequested',
                execution_mode='ORT_SEQUENTIAL',
                graph_optimization_level='ORT_ENABLE_ALL',
                enable_cpu_mem_arena=True,
                enable_mem_pattern=True,
            ),
        ]
        
        combinations.extend(experimental_configs)
        return combinations
    
    def benchmark_single_config(self, config: OptimizationConfig) -> BenchmarkResult:
        """ë‹¨ì¼ ì„¤ì •ì— ëŒ€í•œ ë²¤ì¹˜ë§ˆí¬ ìˆ˜í–‰"""
        try:
            # ì„¸ì…˜ ìƒì„±
            session_options = config.to_session_options()
            provider_options = config.to_provider_options()
            
            providers = [
                ('CUDAExecutionProvider', provider_options),
                'CPUExecutionProvider'
            ]
            
            session = ort.InferenceSession(
                self.model_path,
                providers=providers,
                sess_options=session_options
            )
            
            # GPU ì œê³µì í™œì„±í™” í™•ì¸
            gpu_active = 'CUDAExecutionProvider' in session.get_providers()
            
            if not gpu_active:
                return BenchmarkResult(
                    config=config,
                    avg_time_ms=0,
                    std_time_ms=0,
                    fps=0,
                    success=False,
                    error_message="CUDA Provider í™œì„±í™” ì‹¤íŒ¨",
                    gpu_provider_active=False
                )
            
            # í…ŒìŠ¤íŠ¸ ì…ë ¥ ì¤€ë¹„
            input_tensor = np.random.rand(1, 3, 640, 640).astype(np.float32)
            input_name = session.get_inputs()[0].name
            
            # ì›Œë°ì—… (ì¶©ë¶„í•œ ì‹œê°„ìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ ì•ˆì •í™”)
            self.logger.info(f"ì›Œë°ì—… ì¤‘... (ì•Œê³ ë¦¬ì¦˜: {config.cudnn_conv_algo_search})")
            for _ in range(self.warmup_runs):
                session.run(None, {input_name: input_tensor})
                
            # ì„±ëŠ¥ ì¸¡ì •
            times = []
            for _ in range(self.test_runs):
                start_time = time.time()
                session.run(None, {input_name: input_tensor})
                times.append(time.time() - start_time)
            
            # í†µê³„ ê³„ì‚°
            avg_time = np.mean(times)
            std_time = np.std(times)
            avg_time_ms = avg_time * 1000
            std_time_ms = std_time * 1000
            fps = 1000 / avg_time_ms
            
            # ì„¸ì…˜ ì •ë¦¬
            session = None
            
            return BenchmarkResult(
                config=config,
                avg_time_ms=avg_time_ms,
                std_time_ms=std_time_ms,
                fps=fps,
                success=True,
                gpu_provider_active=gpu_active
            )
            
        except Exception as e:
            self.logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {str(e)}")
            return BenchmarkResult(
                config=config,
                avg_time_ms=0,
                std_time_ms=0,
                fps=0,
                success=False,
                error_message=str(e),
                gpu_provider_active=False
            )
    
    def run_full_benchmark(self) -> List[BenchmarkResult]:
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        if not HAS_ONNX:
            raise RuntimeError("ONNXRuntimeì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        if not self.gpu_info['available']:
            raise RuntimeError("CUDA GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        self.logger.info("ONNX GPU ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
        self.logger.info(f"GPU: {self.gpu_info['name']}")
        self.logger.info(f"ëª¨ë¸: {self.model_path}")
        
        # ì„¤ì • ì¡°í•© ìƒì„±
        configs = self.generate_config_combinations()
        self.logger.info(f"ì´ {len(configs)}ê°œ ì„¤ì • ì¡°í•© í…ŒìŠ¤íŠ¸ ì˜ˆì •")
        
        results = []
        
        for i, config in enumerate(configs, 1):
            self.logger.info(f"[{i}/{len(configs)}] í…ŒìŠ¤íŠ¸ ì¤‘: {config.cudnn_conv_algo_search} + {config.gpu_mem_limit_gb}GB")
            
            try:
                result = self.benchmark_single_config(config)
                results.append(result)
                
                if result.success and result.gpu_provider_active:
                    self.logger.info(f"  ê²°ê³¼: {result.avg_time_ms:.2f}ms ({result.fps:.1f} FPS)")
                else:
                    self.logger.warning(f"  ì‹¤íŒ¨: {result.error_message}")
                    
            except Exception as e:
                self.logger.error(f"  ì˜ˆì™¸ ë°œìƒ: {str(e)}")
                result = BenchmarkResult(
                    config=config,
                    avg_time_ms=0,
                    std_time_ms=0,
                    fps=0,
                    success=False,
                    error_message=str(e),
                    gpu_provider_active=False
                )
                results.append(result)
        
        return results
    
    def find_optimal_config(self, results: List[BenchmarkResult]) -> BenchmarkResult:
        """ìµœì  ì„¤ì • ì°¾ê¸°"""
        successful_results = [r for r in results if r.success and r.gpu_provider_active]
        
        if not successful_results:
            raise RuntimeError("ì„±ê³µí•œ ë²¤ì¹˜ë§ˆí¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
        # FPS ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        successful_results.sort(key=lambda x: x.fps, reverse=True)
        
        return successful_results[0]
    
    def generate_report(self, results: List[BenchmarkResult], optimal: BenchmarkResult) -> str:
        """ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ ìƒì„±"""
        successful_results = [r for r in results if r.success and r.gpu_provider_active]
        
        report = []
        report.append("# ONNX GPU ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ë³´ê³ ì„œ")
        report.append("")
        
        # ì‹œìŠ¤í…œ ì •ë³´
        report.append("## ì‹œìŠ¤í…œ ì •ë³´")
        report.append("")
        report.append(f"- **GPU**: {self.gpu_info['name']}")
        report.append(f"- **GPU ë©”ëª¨ë¦¬**: {self.gpu_info['total_memory_gb']:.1f} GB")
        report.append(f"- **CUDA**: {self.gpu_info['cuda_version']}")
        report.append(f"- **cuDNN**: {self.gpu_info['cudnn_version']}")
        report.append(f"- **ONNXRuntime**: {self.system_info['onnxruntime_version']}")
        report.append("")
        
        # ìµœì  ì„¤ì •
        report.append("## ğŸ† ìµœì  ì„¤ì •")
        report.append("")
        report.append(f"**ì„±ëŠ¥**: {optimal.avg_time_ms:.2f}ms ({optimal.fps:.1f} FPS)")
        report.append("")
        report.append("```yaml")
        report.append("# config.yamlì— ì ìš©í•  ì„¤ì •")
        report.append("models:")
        report.append("  pose_estimation:")
        report.append("    onnx:")
        optimal_dict = asdict(optimal.config)
        for key, value in optimal_dict.items():
            if isinstance(value, str):
                report.append(f"      {key}: \"{value}\"")
            else:
                report.append(f"      {key}: {value}")
        report.append("```")
        report.append("")
        
        # ì„±ëŠ¥ ë¹„êµí‘œ
        report.append("## ğŸ“Š ì„±ëŠ¥ ë¹„êµ")
        report.append("")
        report.append("| ìˆœìœ„ | ì•Œê³ ë¦¬ì¦˜ | FPS | ì‹œê°„(ms) | GPUë©”ëª¨ë¦¬(GB) | ìŠ¤íŠ¸ë¦¼ | ì‘ì—…ê³µê°„ |")
        report.append("|------|----------|-----|----------|---------------|--------|----------|")
        
        for i, result in enumerate(successful_results[:10], 1):
            config = result.config
            report.append(f"| {i} | {config.cudnn_conv_algo_search} | {result.fps:.1f} | {result.avg_time_ms:.2f} | {config.gpu_mem_limit_gb} | {config.do_copy_in_default_stream} | {config.cudnn_conv_use_max_workspace} |")
        
        report.append("")
        
        # ì„¤ì •ë³„ ë¶„ì„
        report.append("## ğŸ” ì„¤ì •ë³„ ë¶„ì„")
        report.append("")
        
        # ì•Œê³ ë¦¬ì¦˜ë³„ ì„±ëŠ¥
        algo_stats = {}
        for result in successful_results:
            algo = result.config.cudnn_conv_algo_search
            if algo not in algo_stats:
                algo_stats[algo] = []
            algo_stats[algo].append(result.fps)
        
        for algo, fps_list in algo_stats.items():
            avg_fps = np.mean(fps_list)
            max_fps = np.max(fps_list)
            report.append(f"- **{algo}**: í‰ê·  {avg_fps:.1f} FPS, ìµœê³  {max_fps:.1f} FPS")
        
        report.append("")
        
        # ê¶Œì¥ì‚¬í•­
        report.append("## ğŸ’¡ ê¶Œì¥ì‚¬í•­")
        report.append("")
        
        if optimal.config.cudnn_conv_algo_search == 'HEURISTIC':
            report.append("âœ… **HEURISTIC ì•Œê³ ë¦¬ì¦˜ ê¶Œì¥**")
            report.append("- ë¹ ë¥¸ ì´ˆê¸°í™”ì™€ ìš°ìˆ˜í•œ ì„±ëŠ¥ì˜ ê· í˜•")
            report.append("- ì‹¤ì‹œê°„ ì¶”ë¡ ì— ìµœì í™”")
            report.append("- í”„ë¡œë•ì…˜ í™˜ê²½ì— ê¶Œì¥")
        elif optimal.config.cudnn_conv_algo_search == 'EXHAUSTIVE':
            report.append("âœ… **EXHAUSTIVE ì•Œê³ ë¦¬ì¦˜ ê¶Œì¥**") 
            report.append("- ìµœì  ì„±ëŠ¥ì„ ìœ„í•œ ì „ìˆ˜íƒìƒ‰")
            report.append("- ì´ˆê¸°í™” ì‹œê°„ì´ ê¸¸ì§€ë§Œ ì•ˆì •í™” í›„ ìµœê³  ì„±ëŠ¥")
            report.append("- ë°°ì¹˜ ì²˜ë¦¬ë‚˜ ì¥ì‹œê°„ ì‹¤í–‰ì— ê¶Œì¥")
        else:
            report.append("âœ… **DEFAULT ì•Œê³ ë¦¬ì¦˜ ê¶Œì¥**")
            report.append("- ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„±ëŠ¥")
            
        report.append("")
        report.append(f"**ë©”ëª¨ë¦¬ ì„¤ì •**: {optimal.config.gpu_mem_limit_gb}GB í• ë‹¹")
        report.append(f"**ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ**: {'ê¸°ë³¸ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©' if optimal.config.do_copy_in_default_stream else 'ë³„ë„ ìŠ¤íŠ¸ë¦¼ ì‚¬ìš©'}")
        
        return "\n".join(report)
    
    def update_config_file(self, config_path: str, optimal_config: OptimizationConfig):
        """config.yaml íŒŒì¼ ìë™ ì—…ë°ì´íŠ¸"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # ONNX ì„¤ì • ì—…ë°ì´íŠ¸
            onnx_config = config['models']['pose_estimation']['onnx']
            
            # ìµœì í™” ì„¤ì • ì ìš©
            onnx_config.update({
                'cudnn_conv_algo_search': optimal_config.cudnn_conv_algo_search,
                'do_copy_in_default_stream': optimal_config.do_copy_in_default_stream,
                'cudnn_conv_use_max_workspace': optimal_config.cudnn_conv_use_max_workspace,
                'tunable_op_enable': optimal_config.tunable_op_enable,
                'tunable_op_tuning_enable': optimal_config.tunable_op_tuning_enable,
                'gpu_mem_limit_gb': optimal_config.gpu_mem_limit_gb,
                'arena_extend_strategy': optimal_config.arena_extend_strategy,
                'execution_mode': optimal_config.execution_mode,
                'graph_optimization_level': optimal_config.graph_optimization_level,
                'enable_cpu_mem_arena': optimal_config.enable_cpu_mem_arena,
                'enable_mem_pattern': optimal_config.enable_mem_pattern,
            })
            
            # ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
            onnx_config['benchmark_info'] = {
                'gpu_name': self.gpu_info['name'],
                'optimized_fps': round(optimal_config.to_provider_options().get('fps', 0), 1),  # ì´ ë¶€ë¶„ì€ ìˆ˜ì • í•„ìš”
                'benchmark_date': time.strftime('%Y-%m-%d %H:%M:%S'),
                'onnxruntime_version': self.system_info['onnxruntime_version'],
            }
            
            # íŒŒì¼ ì €ì¥
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)
                
            self.logger.info(f"ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {config_path}")
            
        except Exception as e:
            self.logger.error(f"ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ONNX GPU ìµœì í™” ë²¤ì¹˜ë§ˆí‚¹ ë„êµ¬')
    parser.add_argument('--model', required=True, help='ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--config', help='ì—…ë°ì´íŠ¸í•  config.yaml ê²½ë¡œ')
    parser.add_argument('--output', help='ë³´ê³ ì„œ ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (.md)')
    parser.add_argument('--warmup', type=int, default=20, help='ì›Œë°ì—… íšŸìˆ˜')
    parser.add_argument('--runs', type=int, default=50, help='ì¸¡ì • íšŸìˆ˜')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (ì ì€ ì¡°í•©)')
    
    args = parser.parse_args()
    
    # ì…ë ¥ ê²€ì¦
    if not os.path.exists(args.model):
        print(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.model}")
        sys.exit(1)
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    benchmark = ONNXOptimizerBenchmark(
        model_path=args.model,
        warmup_runs=args.warmup,
        test_runs=args.runs
    )
    
    print("ğŸš€ ONNX GPU ìµœì í™” ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    print(f"GPU: {benchmark.gpu_info['name']}")
    print(f"ëª¨ë¸: {args.model}")
    print()
    
    # ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    results = benchmark.run_full_benchmark()
    
    # ìµœì  ì„¤ì • ì°¾ê¸°
    try:
        optimal = benchmark.find_optimal_config(results)
        
        print("ğŸ† ìµœì  ì„¤ì • ë°œê²¬!")
        print(f"ì„±ëŠ¥: {optimal.avg_time_ms:.2f}ms ({optimal.fps:.1f} FPS)")
        print(f"ì•Œê³ ë¦¬ì¦˜: {optimal.config.cudnn_conv_algo_search}")
        print(f"GPU ë©”ëª¨ë¦¬: {optimal.config.gpu_mem_limit_gb}GB")
        
        # ë³´ê³ ì„œ ìƒì„±
        report = benchmark.generate_report(results, optimal)
        
        # ë³´ê³ ì„œ ì €ì¥
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ë³´ê³ ì„œ ì €ì¥: {args.output}")
        else:
            print("\n" + "="*60)
            print(report)
        
        # ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸
        if args.config:
            if os.path.exists(args.config):
                benchmark.update_config_file(args.config, optimal.config)
                print(f"ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸: {args.config}")
            else:
                print(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        
    except RuntimeError as e:
        print(f"ì˜¤ë¥˜: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()