#!/bin/bash
# ONNX GPU ë¹ ë¥¸ ìµœì í™” ìŠ¤í¬ë¦½íŠ¸

echo "ğŸš€ ONNX GPU ìë™ ìµœì í™” ë„êµ¬"
echo "=============================="

# CUDA í™˜ê²½ ì„¤ì •
export LD_LIBRARY_PATH=/usr/local/cuda-12.1/targets/x86_64-linux/lib:/usr/local/cuda-12.1/lib64:$LD_LIBRARY_PATH

# í˜„ì¬ GPU ì •ë³´ ì¶œë ¥
echo "í˜„ì¬ GPU í™˜ê²½:"
nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv,noheader

echo
echo "ONNX ìµœì í™” ì‹¤í–‰ ì¤‘..."

# ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH="/workspace/mmpose/checkpoints/end2end.onnx"
CONFIG_PATH="/workspace/recognizer/configs/config.yaml"
REPORT_PATH="/workspace/recognizer/gpu_optimization_$(date +%Y%m%d_%H%M%S).md"

# ìµœì í™” ì‹¤í–‰
python /workspace/recognizer/tools/onnx_optimizer.py \
    --model "$MODEL_PATH" \
    --config "$CONFIG_PATH" \
    --output "$REPORT_PATH" \
    --warmup 15 \
    --runs 30

echo
echo "âœ… ìµœì í™” ì™„ë£Œ!"
echo "ë³´ê³ ì„œ: $REPORT_PATH"
echo "ì„¤ì • íŒŒì¼ì´ ìë™ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."

echo
echo "ğŸ”§ ì ìš©ëœ ìµœì í™” í™•ì¸:"
grep -A 10 "cudnn_conv_algo_search" "$CONFIG_PATH"

echo
echo "ğŸš€ ì´ì œ ìµœì í™”ëœ ONNX ì¶”ë¡ ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!"