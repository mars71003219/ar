#!/usr/bin/env python3
"""
Recognizer ë©”ì¸ ì‹¤í–‰ê¸° - ì™„ì „ ëª¨ë“ˆí™” ë²„ì „
ëª¨ë“  ëª¨ë“œë¥¼ í†µí•© ê´€ë¦¬í•˜ëŠ” ì¼ë°˜í™”ëœ êµ¬ì¡°
"""

import argparse
import sys
import logging
from pathlib import Path

# recognizer ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent))

# ê¸°ë³¸ ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def setup_logging(log_level: str):
    """ë¡œê¹… ì„¤ì •"""
    level = getattr(logging, log_level.upper())
    logging.getLogger().setLevel(level)
    
    # ëª¨ë“  í•˜ìœ„ ëª¨ë“ˆì˜ ë¡œê¹… ë ˆë²¨ë„ ì„¤ì •
    for name in logging.Logger.manager.loggerDict:
        if isinstance(logging.Logger.manager.loggerDict[name], logging.Logger):
            logging.Logger.manager.loggerDict[name].setLevel(level)


def register_modules():
    """ëª¨ë“ˆ ë“±ë¡"""
    from utils.factory import ModuleFactory
    
    try:
        # RTMO í¬ì¦ˆ ì¶”ì •ê¸° (PyTorch)
        from pose_estimation.rtmo.rtmo_estimator import RTMOPoseEstimator
        ModuleFactory.register_pose_estimator(
            name='rtmo',
            estimator_class=RTMOPoseEstimator,
            default_config={'score_threshold': 0.3, 'device': 'cuda:0'}
        )
        
        # RTMO ONNX ì¶”ì •ê¸°
        try:
            from pose_estimation.rtmo.rtmo_onnx_estimator import RTMOONNXEstimator
            ModuleFactory.register_pose_estimator(
                name='rtmo_onnx',
                estimator_class=RTMOONNXEstimator,
                default_config={'score_threshold': 0.3, 'device': 'cuda:0'}
            )
        except ImportError as e:
            logger.warning(f"ONNX estimator not available: {e}")
        
        # RTMO TensorRT ì¶”ì •ê¸°
        try:
            from pose_estimation.rtmo.rtmo_tensorrt_estimator import RTMOTensorRTEstimator
            ModuleFactory.register_pose_estimator(
                name='rtmo_tensorrt', 
                estimator_class=RTMOTensorRTEstimator,
                default_config={'score_threshold': 0.3, 'device': 'cuda:0'}
            )
        except ImportError as e:
            logger.warning(f"TensorRT estimator not available: {e}")
        
        # ByteTracker
        from tracking.bytetrack.byte_tracker import ByteTrackerWrapper
        ModuleFactory.register_tracker(
            name='bytetrack',
            tracker_class=ByteTrackerWrapper,
            default_config={'track_thresh': 0.5}
        )
        
        # Motion-based Scorer (ì´ì „ Region-based Scorer)
        from scoring.motion_based.fight_scorer import MotionBasedScorer
        ModuleFactory.register_scorer(
            name='region_based',
            scorer_class=MotionBasedScorer,
            default_config={'distance_threshold': 100.0}
        )

        # Movement-based Scorer (same as motion_based but optimized for movement)
        ModuleFactory.register_scorer(
            name='movement_based',
            scorer_class=MotionBasedScorer,
            default_config={'distance_threshold': 100.0}
        )
        
        # Falldown Scorer (ì“°ëŸ¬ì§ ì „ìš© ì ìˆ˜ ê³„ì‚°ê¸°)
        from scoring.motion_based.falldown_scorer import FalldownScorer
        ModuleFactory.register_scorer(
            name='falldown_scorer',
            scorer_class=FalldownScorer,
            default_config={'distance_threshold': 100.0}
        )
        
        # STGCN Action Classifier
        from action_classification.stgcn.stgcn_classifier import STGCNActionClassifier
        ModuleFactory.register_classifier(
            name='stgcn',
            classifier_class=STGCNActionClassifier,
            default_config={'num_classes': 2, 'device': 'cuda:0'}
        )
        
        
        # Window Processor
        from utils.window_processor import SlidingWindowProcessor
        ModuleFactory.register_window_processor(
            name='sliding_window',
            processor_class=SlidingWindowProcessor,
            default_config={'window_size': 100, 'window_stride': 50}
        )
        
        logger.info("All modules registered successfully")
        return True
        
    except Exception as e:
        logger.error(f"Failed to register modules: {e}")
        return False


def load_config(config_file: str = None):
    """ì„¤ì • ë¡œë“œ"""
    try:
        from utils.config_loader import load_config
        
        if not config_file:
            config_file = "config.yaml"
        
        config = load_config(
            config_file=config_file,
            mode=None,
            args_dict={}
        )
        
        logger.info(f"Configuration loaded from: {config_file}")
        return config
        
    except Exception as e:
        logger.error(f"Failed to load config: {e}")
        return None


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì™„ì „ ì¼ë°˜í™”"""
    parser = argparse.ArgumentParser(description="Recognizer - Unified Mode Manager")
    parser.add_argument('--config', type=str, default='configs/config.yaml',
                       help='Configuration file path')
    parser.add_argument('--mode', type=str,
                       help='Override mode from config (e.g., inference.analysis, annotation.stage1)')
    parser.add_argument('--log-level', default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='Set logging level')
    parser.add_argument('--list-modes', action='store_true',
                       help='List all available modes')
    
    # ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ì˜µì…˜
    parser.add_argument('--multi-process', action='store_true',
                       help='Run multi-process annotation')
    parser.add_argument('--num-processes', type=int, default=8,
                       help='Number of processes for multi-process annotation (default: 8)')
    parser.add_argument('--gpus', type=str, default='0,1',
                       help='GPU assignments for multi-process (comma-separated, e.g. 0,1)')
    
    args = parser.parse_args()
    
    # ë¡œê¹… ì„¤ì •
    setup_logging(args.log_level)
    
    try:
        # ì„¤ì • ë¡œë“œ
        config = load_config(args.config)
        if not config:
            return False
        
        # ëª¨ë“œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        from core import ModeManager
        mode_manager = ModeManager(config)
        
        # ëª¨ë“œ ëª©ë¡ í‘œì‹œ
        if args.list_modes:
            modes = mode_manager.list_modes()
            logger.info("Available modes:")
            for mode_name, description in modes.items():
                logger.info(f"  {mode_name}: {description}")
            return True
        
        # ëª¨ë“œ ê²°ì • (ì¸ì ìš°ì„ , ê·¸ ë‹¤ìŒ ì„¤ì • íŒŒì¼)
        mode = args.mode or config.get('mode', 'inference.analysis')
        
        # ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì²˜ë¦¬ ì„¤ì • (annotation, inference.analysis ë˜ëŠ” evaluation ëª¨ë“œ)
        annotation_config = config.get('annotation', {})
        stage1_config = annotation_config.get('stage1', {})
        stage1_multi_process = stage1_config.get('multi_process', {})
        should_run_multi_process = args.multi_process or stage1_multi_process.get('enabled', False)

        # ë©€í‹°í”„ë¡œì„¸ìŠ¤ëŠ” stage1ë§Œ ì§€ì›í•˜ë„ë¡ ì œí•œ
        if mode.startswith('annotation.') and should_run_multi_process:
            stage1_enabled = stage1_config.get('enabled', True)

            # stage1ì´ ì•„ë‹Œ ë‹¤ë¥¸ annotation ëª¨ë“œë“¤ì€ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ë¡œ ì‹¤í–‰
            if mode in ['annotation.stage2', 'annotation.stage3', 'annotation.visualize']:
                logger.info(f"{mode} - switching to single process mode (only stage1 supports multi-process)")
                should_run_multi_process = False
            elif not stage1_enabled:
                logger.info("Stage1 disabled - switching to single process mode to use existing stage1 data")
                should_run_multi_process = False
            else:
                # stage1ì´ í™œì„±í™”ë˜ê³  ë©€í‹°í”„ë¡œì„¸ìŠ¤ ëª¨ë“œì¸ ê²½ìš°ì—ë§Œ ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
                return run_multi_process_annotation(config, args)
        elif mode == 'inference.analysis':
            # inference.analysis ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì„¤ì • í™•ì¸
            inference_config = config.get('inference', {})
            analysis_config = inference_config.get('analysis', {})
            analysis_multi_process = analysis_config.get('multi_process', {})
            analysis_should_run_multi_process = args.multi_process or analysis_multi_process.get('enabled', False)

            if analysis_should_run_multi_process:
                return run_multi_process_inference_analysis(config, args)
        elif mode == 'evaluation':
            # evaluation ëª¨ë“œì˜ ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì„¤ì •
            if args.multi_process:
                config['multi_process'] = True
                config['num_processes'] = args.num_processes
        
        # ëª¨ë“ˆ ë“±ë¡
        if not register_modules():
            return False
        
        # íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ìë™ ê°ì§€ (ì‹œê°í™” ëª¨ë“œëŠ” ì œì™¸)
        if mode.startswith('annotation.') and mode not in ['annotation.pipeline', 'annotation.visualize']:
            from core.annotation_pipeline_mode import should_use_pipeline_mode
            if should_use_pipeline_mode(config):
                mode = 'annotation.pipeline'
                logger.info("Pipeline mode detected - switching to annotation.pipeline")
        
        logger.info(f"=== EXECUTING MODE: {mode} ===")
        
        # ëª¨ë“œ ì‹¤í–‰
        success = mode_manager.execute(mode)
        
        if success:
            logger.info(f"Mode {mode} completed successfully")
        else:
            logger.error(f"Mode {mode} failed")
        
        return success
            
    except KeyboardInterrupt:
        logger.info("Execution interrupted by user")
        return False
    except Exception as e:
        logger.error(f"Execution failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def run_multi_process_annotation(config, args):
    """ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ì‹¤í–‰"""
    try:
        from utils.multi_process_splitter import run_multi_process_annotation as run_mp
        
        # configì—ì„œ stage1 multi-process ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        annotation_config = config.get('annotation', {})
        stage1_config = annotation_config.get('stage1', {})
        multi_process_config = stage1_config.get('multi_process', {})
        
        # config ìš°ì„ , command line argsëŠ” fallback
        if hasattr(args, 'num_processes') and args.num_processes != 4:
            # command lineì—ì„œ ê¸°ë³¸ê°’ì´ ì•„ë‹Œ ê°’ì´ ì„¤ì •ëœ ê²½ìš°
            num_processes = args.num_processes
        else:
            num_processes = multi_process_config.get('num_processes', 4)
        
        # GPU í• ë‹¹ ì„¤ì • - ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹
        if hasattr(args, 'gpus') and args.gpus != '0,1':
            # command lineì—ì„œ ê¸°ë³¸ê°’ì´ ì•„ë‹Œ ê°’ì´ ì„¤ì •ëœ ê²½ìš°
            available_gpus = [int(x.strip()) for x in args.gpus.split(',')]
        else:
            # configì—ì„œ GPU ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            available_gpus = multi_process_config.get('gpus', [0, 1])
        
        # ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ GPU í• ë‹¹
        gpu_assignments = [available_gpus[i % len(available_gpus)] for i in range(num_processes)]
        
        # ì„¤ì •ì—ì„œ ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        input_dir = config.get('annotation', {}).get('input', '/aivanas/raw/surveillance/action/violence/action_recognition/data/RWF-2000')
        output_dir = config.get('annotation', {}).get('output_dir', 'output')
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not Path(output_dir).is_absolute():
            output_dir = str(Path.cwd() / output_dir)
        
        # ì„¤ì • íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        config_path = args.config
        if not Path(config_path).is_absolute():
            config_path = str(Path.cwd() / config_path)
        
        logger.info("=== Multi-Process Annotation Configuration ===")
        logger.info(f"Input directory: {input_dir}")
        logger.info(f"Output directory: {output_dir}")
        logger.info(f"Config file: {config_path}")
        logger.info(f"Number of processes: {num_processes}")
        logger.info(f"GPU assignments: {gpu_assignments}")
        logger.info(f"Config source: {'Config file' if multi_process_config.get('enabled', False) else 'Command line'}")
        
        # ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ì‹¤í–‰
        success = run_mp(
            input_dir=input_dir,
            output_dir=output_dir,
            config_path=config_path,
            num_processes=num_processes,
            gpu_assignments=gpu_assignments
        )
        
        if success:
            logger.info("ğŸ‰ Multi-process annotation completed successfully!")
        else:
            logger.error("âŒ Multi-process annotation failed!")
        
        return success
        
    except Exception as e:
        logger.error(f"Multi-process annotation error: {e}")
        return False


def run_multi_process_inference_analysis(config, args):
    """ë©€í‹° í”„ë¡œì„¸ìŠ¤ inference.analysis ì‹¤í–‰"""
    try:
        from utils.multi_process_splitter import run_multi_process_inference_analysis as run_mp
        
        # configì—ì„œ inference.analysis multi-process ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        inference_config = config.get('inference', {})
        analysis_config = inference_config.get('analysis', {})
        multi_process_config = analysis_config.get('multi_process', {})
        
        # config ìš°ì„ , command line argsëŠ” fallback
        if hasattr(args, 'num_processes') and args.num_processes != 4:
            num_processes = args.num_processes
        else:
            num_processes = multi_process_config.get('num_processes', 4)
        
        # GPU í• ë‹¹ ì„¤ì •
        if hasattr(args, 'gpus') and args.gpus != '0,1':
            available_gpus = [int(x.strip()) for x in args.gpus.split(',')]
        else:
            available_gpus = multi_process_config.get('gpus', [0, 1])
        
        # ë¼ìš´ë“œ ë¡œë¹ˆìœ¼ë¡œ GPU í• ë‹¹
        gpu_assignments = [available_gpus[i % len(available_gpus)] for i in range(num_processes)]
        
        # ì„¤ì •ì—ì„œ ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        input_dir = config.get('inference', {}).get('analysis', {}).get('input')
        if not input_dir:
            # inference.realtimeì˜ inputì„ fallbackìœ¼ë¡œ ì‚¬ìš©
            input_dir = config.get('inference', {}).get('realtime', {}).get('input', '/aivanas/raw/surveillance/action/violence/action_recognition/data/UBI_demo')
        
        output_dir = config.get('inference', {}).get('analysis', {}).get('output_dir', 'output')
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not Path(output_dir).is_absolute():
            output_dir = str(Path.cwd() / output_dir)
        
        # ì„¤ì • íŒŒì¼ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        config_path = args.config
        if not Path(config_path).is_absolute():
            config_path = str(Path.cwd() / config_path)
        
        logger.info("=== Multi-Process Inference Analysis Configuration ===")
        logger.info(f"Input directory: {input_dir}")
        logger.info(f"Output directory: {output_dir}")
        logger.info(f"Config file: {config_path}")
        logger.info(f"Number of processes: {num_processes}")
        logger.info(f"GPU assignments: {gpu_assignments}")
        logger.info(f"Config source: {'Config file' if multi_process_config.get('enabled', False) else 'Command line'}")
        
        # ë©€í‹° í”„ë¡œì„¸ìŠ¤ inference.analysis ì‹¤í–‰
        success = run_mp(
            input_dir=input_dir,
            output_dir=output_dir,
            config_path=config_path,
            num_processes=num_processes,
            gpu_assignments=gpu_assignments
        )
        
        if success:
            logger.info("ğŸ‰ Multi-process inference analysis completed successfully!")
        else:
            logger.error("âŒ Multi-process inference analysis failed!")
        
        return success
        
    except Exception as e:
        logger.error(f"Multi-process inference analysis error: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)