"""
ë©€í‹° í”„ë¡œì„¸ìŠ¤ ë°ì´í„° ë¶„í•  ë° í†µí•© ì‹œìŠ¤í…œ

ì—¬ëŸ¬ ë©”ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶„í•  ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ í†µí•©
"""

import os
import time
import logging
import subprocess
from pathlib import Path
from typing import List, Dict, Any
import math
import shutil
import yaml

logger = logging.getLogger(__name__)


class VideoDataSplitter:
    """ë¹„ë””ì˜¤ ë°ì´í„° ë¶„í• ê¸°"""
    
    def __init__(self, input_dir: str, num_splits: int = 4):
        self.input_dir = Path(input_dir)
        self.num_splits = num_splits
        self.video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']
        
    def get_all_videos(self) -> List[Path]:
        """ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        video_files = []
        for ext in self.video_extensions:
            video_files.extend(self.input_dir.rglob(f"*{ext}"))
            video_files.extend(self.input_dir.rglob(f"*{ext.upper()}"))
        
        return sorted(video_files)
    
    def split_videos(self) -> List[List[Path]]:
        """ë¹„ë””ì˜¤ë¥¼ ê· ë“±í•˜ê²Œ ë¶„í• """
        all_videos = self.get_all_videos()
        total_videos = len(all_videos)
        
        if total_videos == 0:
            logger.warning("No video files found")
            return []
        
        # ê° ë¶„í• ì˜ í¬ê¸° ê³„ì‚°
        chunk_size = math.ceil(total_videos / self.num_splits)
        
        splits = []
        for i in range(self.num_splits):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, total_videos)
            
            if start_idx < total_videos:
                split_videos = all_videos[start_idx:end_idx]
                splits.append(split_videos)
        
        logger.info(f"Split {total_videos} videos into {len(splits)} groups:")
        for i, split in enumerate(splits):
            logger.info(f"  Split {i}: {len(split)} videos")
        
        return splits
    
    def create_split_directories(self, base_output_dir: str) -> List[str]:
        """ë¶„í• ë³„ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        split_dirs = []
        base_path = Path(base_output_dir)
        
        for i in range(self.num_splits):
            split_dir = base_path / f"split_{i}"
            split_dir.mkdir(parents=True, exist_ok=True)
            split_dirs.append(str(split_dir))
        
        return split_dirs


class ConfigGenerator:
    """ë¶„í• ë³„ ì„¤ì • íŒŒì¼ ìƒì„±ê¸°"""
    
    def __init__(self, base_config_path: str):
        self.base_config_path = base_config_path
        with open(base_config_path, 'r', encoding='utf-8') as f:
            self.base_config = yaml.safe_load(f)
    
    def create_split_configs(
        self, 
        video_splits: List[List[Path]], 
        split_dirs: List[str]
    ) -> List[str]:
        """ë¶„í• ë³„ ì„¤ì • íŒŒì¼ ìƒì„±"""
        config_paths = []
        
        for i, (videos, split_dir) in enumerate(zip(video_splits, split_dirs)):
            # ì„ì‹œ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_video_dir = Path(split_dir) / "videos"
            temp_video_dir.mkdir(exist_ok=True)
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± (ì›ë³¸ í´ë” êµ¬ì¡° ë³´ì¡´)
            for video in videos:
                # ì›ë³¸ í´ë”ëª… ë³´ì¡´ (fight, normal ë“±)
                original_parent = video.parent.name
                target_subdir = temp_video_dir / original_parent
                target_subdir.mkdir(exist_ok=True)
                
                link_path = target_subdir / video.name
                if not link_path.exists():
                    try:
                        os.symlink(str(video.absolute()), str(link_path))
                    except:
                        # ì‹¬ë³¼ë¦­ ë§í¬ ì‹¤íŒ¨ì‹œ ë³µì‚¬
                        shutil.copy2(str(video), str(link_path))
            
            # ë¶„í• ë³„ ì„¤ì • ìƒì„±
            split_config = self.base_config.copy()
            split_config['annotation']['input'] = str(temp_video_dir)
            split_config['annotation']['output_dir'] = split_dir

            # ëª¨ë“œë¥¼ stage1ìœ¼ë¡œ ëª…ì‹œì  ì„¤ì •
            split_config['mode'] = 'annotation.stage1'
            
            # ë©€í‹° í”„ë¡œì„¸ìŠ¤ ë¹„í™œì„±í™” (subprocessì—ì„œëŠ” ë‹¨ì¼ ì²˜ë¦¬ë§Œ)
            if 'multi_process' in split_config:
                split_config['multi_process']['enabled'] = False
            
            # annotation í•˜ìœ„ì˜ ë©€í‹°í”„ë¡œì„¸ìŠ¤ ì„¤ì •ë„ ë¹„í™œì„±í™” (í˜¸í™˜ì„±)
            if 'annotation' in split_config and 'multi_process' in split_config['annotation']:
                split_config['annotation']['multi_process']['enabled'] = False
            
            # pipeline_mode ë¹„í™œì„±í™” ë° stage1ë§Œ í™œì„±í™” (ë¬´í•œë£¨í”„ ë°©ì§€)
            if 'annotation' in split_config:
                split_config['annotation']['pipeline_mode'] = False

                # stage1ë§Œ í™œì„±í™”í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë¹„í™œì„±í™”
                if 'stage1' in split_config['annotation']:
                    split_config['annotation']['stage1']['enabled'] = True
                    # stage1 ë©€í‹°í”„ë¡œì„¸ìŠ¤ë„ ë¹„í™œì„±í™” (subprocessì—ì„œëŠ” ë‹¨ì¼ ì²˜ë¦¬)
                    if 'multi_process' in split_config['annotation']['stage1']:
                        split_config['annotation']['stage1']['multi_process']['enabled'] = False
                    split_config['annotation']['stage1']['enable_parallel'] = False

                if 'stage2' in split_config['annotation']:
                    split_config['annotation']['stage2']['enabled'] = False
                    split_config['annotation']['stage2']['enable_parallel'] = False

                if 'stage3' in split_config['annotation']:
                    split_config['annotation']['stage3']['enabled'] = False
            
            # ì„¤ì • íŒŒì¼ ì €ì¥
            config_path = Path(split_dir) / f"config_split_{i}.yaml"
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.safe_dump(split_config, f, allow_unicode=True)
            
            config_paths.append(str(config_path))
            logger.info(f"Created config for split {i}: {len(videos)} videos -> {config_path}")
        
        return config_paths


class MultiProcessRunner:
    """ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ê¸°"""
    
    def __init__(self, main_script_path: str = "/workspace/recognizer/main.py"):
        self.main_script_path = main_script_path
        self.processes = []
    
    def run_split_processes(self, config_paths: List[str], gpu_assignments: List[int] = None) -> List[subprocess.Popen]:
        """ë¶„í• ë³„ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        processes = []
        
        if gpu_assignments is None:
            gpu_assignments = [i % 2 for i in range(len(config_paths))]  # GPU 0, 1 ìˆœí™˜
        
        for i, config_path in enumerate(config_paths):
            gpu_id = gpu_assignments[i]
            
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            env = os.environ.copy()
            env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
            
            # í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            cmd = [
                'python3', 
                self.main_script_path,
                '--config', config_path
            ]
            
            logger.info(f"Starting split {i} on GPU {gpu_id}: {' '.join(cmd)}")
            
            process = subprocess.Popen(
                cmd,
                env=env,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                universal_newlines=True,
                bufsize=1
            )
            
            processes.append(process)
            time.sleep(2)  # í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ê°„ê²©
        
        self.processes = processes
        return processes
    
    def monitor_processes(self, processes: List[subprocess.Popen]) -> Dict[int, int]:
        """í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ëª¨ë‹ˆí„°ë§ - ì•ˆì „í•œ ì¢…ë£Œ ë³´ì¥"""
        import threading
        import queue
        import signal
        
        results = {}
        result_queue = queue.Queue()
        monitor_threads = []
        
        def monitor_single_process(i, process):
            """ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ"""
            try:
                logger.info(f"Monitoring split {i} (PID: {process.pid})")
                
                # ì‹¤ì‹œê°„ ì¶œë ¥ ëª¨ë‹ˆí„°ë§
                try:
                    for line in process.stdout:
                        if line:  # ë¹ˆ ë¼ì¸ ì œì™¸
                            logger.info(f"Split {i}: {line.strip()}")
                except Exception as e:
                    logger.warning(f"Output monitoring error for split {i}: {e}")
                
                # í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ëŒ€ê¸°
                return_code = process.wait()
                
                if return_code == 0:
                    logger.info(f"Split {i} completed successfully")
                else:
                    logger.error(f"Split {i} failed with return code {return_code}")
                
                result_queue.put((i, return_code))
                
            except Exception as e:
                logger.error(f"Error monitoring split {i}: {e}")
                result_queue.put((i, -1))
            finally:
                # í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
                try:
                    if process.poll() is None:  # ì•„ì§ ì‹¤í–‰ ì¤‘ì´ë©´
                        process.terminate()
                        process.wait(timeout=5)
                except Exception:
                    pass
        
        def cleanup_handler(signum, frame):
            """ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ - ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì •ë¦¬"""
            logger.warning(f"Received signal {signum}, cleaning up processes...")
            for i, process in enumerate(processes):
                if process.poll() is None:
                    try:
                        logger.info(f"Terminating split {i} (PID: {process.pid})")
                        process.terminate()
                        process.wait(timeout=3)
                    except Exception as e:
                        logger.error(f"Error terminating split {i}: {e}")
                        try:
                            process.kill()
                        except Exception:
                            pass
        
        # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
        original_sigint_handler = signal.signal(signal.SIGINT, cleanup_handler)
        original_sigterm_handler = signal.signal(signal.SIGTERM, cleanup_handler)
        
        try:
            # ê° í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
            for i, process in enumerate(processes):
                thread = threading.Thread(target=monitor_single_process, args=(i, process))
                thread.daemon = True
                thread.start()
                monitor_threads.append(thread)
            
            # ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ í¬í•¨)
            collected_results = 0
            timeout_per_result = 300  # ê° ê²°ê³¼ë‹¹ 5ë¶„ íƒ€ì„ì•„ì›ƒ
            
            while collected_results < len(processes):
                try:
                    i, return_code = result_queue.get(timeout=timeout_per_result)
                    results[i] = return_code
                    collected_results += 1
                    logger.info(f"Collected result for split {i}: return_code={return_code}")
                except queue.Empty:
                    logger.error("Timeout waiting for process results")
                    break
            
            # ë‚¨ì€ í”„ë¡œì„¸ìŠ¤ë“¤ ê°•ì œ ì¢…ë£Œ
            for i, process in enumerate(processes):
                if i not in results and process.poll() is None:
                    logger.warning(f"Force terminating split {i}")
                    try:
                        process.terminate()
                        process.wait(timeout=5)
                        results[i] = -1
                    except Exception:
                        try:
                            process.kill()
                            results[i] = -1
                        except Exception:
                            pass
            
            # ëª¨ë“  ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸° (íƒ€ì„ì•„ì›ƒ í¬í•¨)
            for thread in monitor_threads:
                thread.join(timeout=10)
                if thread.is_alive():
                    logger.warning("Monitor thread did not terminate gracefully")
        
        finally:
            # ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë³µì›
            signal.signal(signal.SIGINT, original_sigint_handler)
            signal.signal(signal.SIGTERM, original_sigterm_handler)
        
        return results


class ResultMerger:
    """ê²°ê³¼ í†µí•©ê¸°"""
    
    def __init__(self, final_output_dir: str):
        self.final_output_dir = Path(final_output_dir)
        self.final_output_dir.mkdir(parents=True, exist_ok=True)
    
    def merge_stage_results(self, split_dirs: List[str], stage: str) -> bool:
        """ìŠ¤í…Œì´ì§€ë³„ ê²°ê³¼ í†µí•© - Stage3ëŠ” íŠ¹ë³„ ì²˜ë¦¬"""
        try:
            if stage == 'stage3':
                return self._merge_stage3_results(split_dirs, str(self.final_output_dir))
            else:
                return self._merge_regular_stage_results(split_dirs, stage)

        except Exception as e:
            logger.error(f"Error merging {stage} results: {e}")
            return False

    def _merge_stage3_results(self, split_dirs: List[str], output_dir: str) -> bool:
        """Stage3 ì„ì‹œ íŒŒì¼ë“¤ì„ í†µí•©í•˜ì—¬ ìµœì¢… ë°ì´í„°ì…‹ ìƒì„±"""
        try:
            from pipelines.dual_service.dual_pipeline import DualServicePipeline
            import yaml
            import pickle
            import random

            # Stage3 ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ì°¾ê¸°
            temp_dirs = []
            for split_dir in split_dirs:
                split_path = Path(split_dir)
                # temp_stage3ê³¼ stage3_dataset ë””ë ‰í† ë¦¬ ëª¨ë‘ í™•ì¸
                temp_stage3_dirs = list(split_path.rglob("**/temp_stage3"))
                stage3_dataset_dirs = list(split_path.rglob("**/stage3_dataset"))
                temp_dirs.extend([str(d) for d in temp_stage3_dirs])
                temp_dirs.extend([str(d) for d in stage3_dataset_dirs])

            if not temp_dirs:
                logger.warning("No stage3 temp directories found")
                return False

            logger.info(f"Found {len(temp_dirs)} stage3 temp directories")

            # ì„¤ì •ì—ì„œ split_ratios ê°€ì ¸ì˜¤ê¸°
            split_ratios = {'train': 0.7, 'val': 0.2, 'test': 0.1}
            try:
                # ì²« ë²ˆì§¸ split ë””ë ‰í† ë¦¬ì—ì„œ ì„¤ì • íŒŒì¼ ì°¾ê¸°
                first_split = Path(split_dirs[0])
                config_files = list(first_split.rglob("config_split_*.yaml"))
                if config_files:
                    with open(config_files[0], 'r', encoding='utf-8') as f:
                        config = yaml.safe_load(f)
                        split_ratios = config.get('annotation', {}).get('stage3', {}).get('split_ratios', split_ratios)
            except Exception as e:
                logger.warning(f"Failed to load split_ratios from config, using defaults: {e}")

            # ëª¨ë“  ì„ì‹œ íŒŒì¼ê³¼ stage3 ë°ì´í„°ì…‹ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•˜ì—¬ í†µí•©
            all_temp_files = []
            all_stage3_files = {'train': [], 'val': [], 'test': []}

            for temp_dir in temp_dirs:
                temp_path = Path(temp_dir)
                if temp_path.exists():
                    # ê¸°ì¡´ temp íŒŒì¼ë“¤ ì°¾ê¸°
                    temp_files = list(temp_path.glob("*_temp.pkl"))
                    all_temp_files.extend(temp_files)
                    if temp_files:
                        logger.info(f"Found {len(temp_files)} temp files in {temp_dir}")

                    # stage3_dataset ë‚´ì˜ train/val/test íŒŒì¼ë“¤ ì°¾ê¸°
                    for split_type in ['train', 'val', 'test']:
                        split_files = list(temp_path.rglob(f"**/{split_type}.pkl"))
                        all_stage3_files[split_type].extend(split_files)
                        if split_files:
                            logger.info(f"Found {len(split_files)} {split_type} files in {temp_dir}")

            # temp íŒŒì¼ ë˜ëŠ” stage3 ë°ì´í„°ì…‹ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            has_temp_files = len(all_temp_files) > 0
            has_stage3_files = any(len(files) > 0 for files in all_stage3_files.values())

            if not has_temp_files and not has_stage3_files:
                logger.warning("No stage3 temp files or dataset files found for merging")
                return False

            # Stage3 ë°ì´í„°ì…‹ íŒŒì¼ë“¤ì´ ìˆëŠ” ê²½ìš° (ìƒˆë¡œìš´ ë°©ì‹)
            if has_stage3_files:
                return self._merge_stage3_dataset_files(all_stage3_files, output_dir, split_ratios)

            # ê¸°ì¡´ temp íŒŒì¼ ì²˜ë¦¬ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
            if not all_temp_files:
                logger.warning("No stage3 temp files found for merging")
                return False

            logger.info(f"Total stage3 temp files to merge: {len(all_temp_files)}")

            # ëª¨ë“  ë°ì´í„°ë¥¼ í•œ ë²ˆì— ìˆ˜ì§‘
            all_entries = []
            video_count = 0
            label_counts = {}

            for temp_file in all_temp_files:
                try:
                    with open(temp_file, 'rb') as f:
                        temp_data = pickle.load(f)

                    entries = temp_data.get('dataset_entries', [])
                    label = temp_data.get('label', 0)
                    video_name = temp_data.get('video_name', 'unknown')

                    all_entries.extend(entries)
                    video_count += 1

                    label_counts[label] = label_counts.get(label, 0) + len(entries)

                    logger.info(f"Merged {video_name}: {len(entries)} entries (label: {label})")

                except Exception as e:
                    logger.error(f"Error reading temp file {temp_file}: {e}")
                    continue

            if not all_entries:
                logger.warning("No valid entries found in temp files")
                return False

            # ë°ì´í„° ì…”í”Œ
            random.shuffle(all_entries)

            # Train/Val/Test ë¶„í• 
            total = len(all_entries)
            train_end = int(total * split_ratios['train'])
            val_end = int(total * (split_ratios['train'] + split_ratios['val']))

            train_data = all_entries[:train_end]
            val_data = all_entries[train_end:val_end]
            test_data = all_entries[val_end:]

            # ìµœì¢… ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
            stage3_output_dir = self.final_output_dir / "stage3_dataset"
            stage3_output_dir.mkdir(parents=True, exist_ok=True)

            # ìµœì¢… íŒŒì¼ ì €ì¥
            train_file = stage3_output_dir / "train.pkl"
            val_file = stage3_output_dir / "val.pkl"
            test_file = stage3_output_dir / "test.pkl"

            with open(train_file, 'wb') as f:
                pickle.dump(train_data, f)

            with open(val_file, 'wb') as f:
                pickle.dump(val_data, f)

            with open(test_file, 'wb') as f:
                pickle.dump(test_data, f)

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for temp_file in all_temp_files:
                try:
                    temp_file.unlink()
                except Exception as e:
                    logger.warning(f"Failed to remove temp file {temp_file}: {e}")

            total_merged = total

            logger.info(f"Stage3 merge completed: {total_merged} total entries")
            return total_merged > 0

        except Exception as e:
            logger.error(f"Error in stage3 merge: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _merge_stage3_dataset_files(self, all_stage3_files: Dict, output_dir: str, split_ratios: Dict) -> bool:
        """Stage3 ë°ì´í„°ì…‹ íŒŒì¼ë“¤ì„ ë³‘í•©"""
        try:
            import pickle

            logger.info("Merging Stage3 dataset files...")

            # ìµœì¢… ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
            final_output_dir = Path(output_dir) / "stage3_dataset"
            final_output_dir.mkdir(parents=True, exist_ok=True)

            # ê° splitë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘ ë° ë³‘í•©
            merged_data = {'train': [], 'val': [], 'test': []}
            total_counts = {'train': 0, 'val': 0, 'test': 0}

            for split_type in ['train', 'val', 'test']:
                split_files = all_stage3_files[split_type]
                logger.info(f"Processing {len(split_files)} {split_type} files")

                for split_file in split_files:
                    try:
                        with open(split_file, 'rb') as f:
                            data = pickle.load(f)

                        if isinstance(data, list):
                            merged_data[split_type].extend(data)
                            total_counts[split_type] += len(data)
                            logger.info(f"Merged {split_file.name}: {len(data)} entries")
                        else:
                            logger.warning(f"Unexpected data format in {split_file}: {type(data)}")

                    except Exception as e:
                        logger.error(f"Error processing {split_file}: {e}")
                        continue

                # ë³‘í•©ëœ ë°ì´í„° ì €ì¥
                if merged_data[split_type]:
                    output_file = final_output_dir / f"{split_type}.pkl"
                    with open(output_file, 'wb') as f:
                        pickle.dump(merged_data[split_type], f)
                    logger.info(f"Saved merged {split_type}.pkl: {len(merged_data[split_type])} entries")

            # ìš”ì•½ ì •ë³´ ì €ì¥
            total_entries = sum(total_counts.values())
            summary = {
                'train_count': total_counts['train'],
                'val_count': total_counts['val'],
                'test_count': total_counts['test'],
                'total_entries': total_entries,
                'split_ratios': split_ratios,
                'output_dir': str(final_output_dir)
            }

            summary_file = final_output_dir / "dataset_summary.json"
            with open(summary_file, 'w') as f:
                import json
                json.dump(summary, f, indent=2)

            logger.info(f"âœ… Stage3 dataset merge completed")
            logger.info(f"  - Train: {total_counts['train']} entries")
            logger.info(f"  - Val: {total_counts['val']} entries")
            logger.info(f"  - Test: {total_counts['test']} entries")
            logger.info(f"  - Total: {total_entries} entries")
            logger.info(f"  - Output: {final_output_dir}")

            return True

        except Exception as e:
            logger.error(f"Error in stage3 dataset merge: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _merge_regular_stage_results(self, split_dirs: List[str], stage: str) -> bool:
        """ì¼ë°˜ ìŠ¤í…Œì´ì§€ (Stage1, Stage2) ê²°ê³¼ í†µí•©"""
        try:
            # ìµœì¢… ì¶œë ¥ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
            stage_patterns = {
                'stage1': '**/stage1_poses/**/*.pkl',
                'stage2': '**/stage2_tracking/**/*.pkl'
            }

            pattern = stage_patterns.get(stage, f'**/{stage}**/*.pkl')
            merged_count = 0
            duplicate_count = 0

            for split_dir in split_dirs:
                split_path = Path(split_dir)

                # í•´ë‹¹ ìŠ¤í…Œì´ì§€ ê²°ê³¼ íŒŒì¼ë“¤ ì°¾ê¸°
                result_files = list(split_path.rglob(pattern))

                for result_file in result_files:
                    # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°í•˜ë˜ videos/ ë¶€ë¶„ì„ ì œê±°
                    try:
                        rel_path = result_file.relative_to(split_path)

                        # videos/ ê²½ë¡œë¥¼ ì œê±°í•˜ê³  ì¬êµ¬ì„±
                        path_parts = list(rel_path.parts)
                        if path_parts and path_parts[0] == 'videos':
                            # videos/stage1_poses/... -> stage1_poses/...
                            new_rel_path = Path(*path_parts[1:])
                        else:
                            new_rel_path = rel_path

                        final_path = self.final_output_dir / new_rel_path

                        # ì¤‘ë³µ íŒŒì¼ ì²˜ë¦¬
                        if final_path.exists():
                            duplicate_count += 1
                            logger.warning(f"Duplicate file found, skipping: {final_path}")
                            continue

                        # ë””ë ‰í† ë¦¬ ìƒì„±
                        final_path.parent.mkdir(parents=True, exist_ok=True)

                        # íŒŒì¼ ì´ë™ (ì•ˆì „í•œ ì´ë™)
                        try:
                            shutil.move(str(result_file), str(final_path))
                            merged_count += 1
                        except Exception as move_error:
                            # ì´ë™ ì‹¤íŒ¨ì‹œ ë³µì‚¬ ì‹œë„
                            logger.warning(f"Move failed, trying copy: {move_error}")
                            shutil.copy2(str(result_file), str(final_path))
                            os.remove(str(result_file))  # ì›ë³¸ íŒŒì¼ ì‚­ì œ
                            merged_count += 1

                    except Exception as e:
                        logger.warning(f"Failed to merge {result_file}: {e}")

            if duplicate_count > 0:
                logger.warning(f"Found {duplicate_count} duplicate files for {stage}")

            logger.info(f"Merged {merged_count} files for {stage}")
            return merged_count > 0

        except Exception as e:
            logger.error(f"Error merging {stage} results: {e}")
            return False
    
    def cleanup_split_dirs(self, split_dirs: List[str]):
        """ë¶„í•  ë””ë ‰í† ë¦¬ ì™„ì „ ì •ë¦¬"""
        # temp_splits í´ë” ì „ì²´ ì‚­ì œ
        if split_dirs:
            temp_base = Path(split_dirs[0]).parent
            try:
                if temp_base.name.endswith("_temp_splits"):
                    shutil.rmtree(str(temp_base))
                    logger.info(f"Cleaned up temp_splits directory: {temp_base}")
                    return
            except Exception as e:
                logger.warning(f"Failed to cleanup temp_splits: {e}")
        
        # ê°œë³„ ë¶„í•  ë””ë ‰í† ë¦¬ ì‚­ì œ
        for split_dir in split_dirs:
            try:
                shutil.rmtree(split_dir)
                logger.info(f"Cleaned up split directory: {split_dir}")
            except Exception as e:
                logger.warning(f"Failed to cleanup {split_dir}: {e}")


class MultiProcessAnnotationManager:
    """ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ê´€ë¦¬ì"""
    
    def __init__(
        self,
        input_dir: str,
        output_dir: str,
        config_path: str,
        num_processes: int = 4,
        gpu_assignments: List[int] = None
    ):
        self.input_dir = input_dir
        self.config_path = config_path
        self.num_processes = num_processes
        self.gpu_assignments = gpu_assignments or [i % 2 for i in range(num_processes)]
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì…ë ¥ í´ë”ëª…ìœ¼ë¡œ ì„¤ì •
        input_folder_name = Path(input_dir).name
        self.output_dir = str(Path(output_dir) / input_folder_name)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.splitter = VideoDataSplitter(input_dir, num_processes)
        self.config_generator = ConfigGenerator(config_path)
        self.runner = MultiProcessRunner()
        self.merger = ResultMerger(self.output_dir)
    
    def run_full_pipeline(self) -> bool:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            logger.info(f"Starting multi-process annotation with {self.num_processes} processes")
            logger.info(f"Input: {self.input_dir}")
            logger.info(f"Output: {self.output_dir}")
            logger.info(f"GPU assignments: {self.gpu_assignments}")
            
            # 1. ë¹„ë””ì˜¤ ë¶„í• 
            logger.info("=== Step 1: Splitting videos ===")
            video_splits = self.splitter.split_videos()
            if not video_splits:
                logger.error("No videos to process")
                return False
            
            # 2. ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (ì…ë ¥ ë””ë ‰í† ë¦¬ ê¸°ë°˜ìœ¼ë¡œ)
            input_parent = Path(self.input_dir).parent
            input_folder_name = Path(self.input_dir).name
            temp_base = input_parent / f"{input_folder_name}_temp_splits"
            split_dirs = self.splitter.create_split_directories(str(temp_base))
            
            # 3. ë¶„í• ë³„ ì„¤ì • íŒŒì¼ ìƒì„±
            logger.info("=== Step 2: Creating split configurations ===")
            config_paths = self.config_generator.create_split_configs(video_splits, split_dirs)
            
            # 4. ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            logger.info("=== Step 3: Running split processes ===")
            processes = self.runner.run_split_processes(config_paths, self.gpu_assignments)
            
            # 5. í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§
            logger.info("=== Step 4: Monitoring processes ===")
            results = self.runner.monitor_processes(processes)
            
            # 6. ê²°ê³¼ í†µí•©
            logger.info("=== Step 5: Merging results ===")
            process_success_count = sum(1 for code in results.values() if code == 0)
            logger.info(f"Completed processes: {process_success_count}/{len(processes)}")

            # ê° ìŠ¤í…Œì´ì§€ë³„ ê²°ê³¼ í†µí•©
            stage_results = {}
            for stage in ['stage1', 'stage2', 'stage3']:
                stage_results[stage] = self.merger.merge_stage_results(split_dirs, stage)

            # 7. ì •ë¦¬
            logger.info("=== Step 6: Cleanup ===")
            self.merger.cleanup_split_dirs(split_dirs)

            # ì‹¤ì œ ì²˜ë¦¬ ì„±ê³µë¥  ê³„ì‚°
            total_videos = sum(len(videos) for videos in video_splits)
            processed_videos = 0

            # Stage1 poses íŒŒì¼ ìˆ˜ë¡œ ì‹¤ì œ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ ìˆ˜ ê³„ì‚°
            stage1_dir = Path(self.output_dir) / "stage1_poses"
            if stage1_dir.exists():
                poses_files = list(stage1_dir.rglob("*_poses.pkl"))
                processed_videos = len(poses_files)

            actual_success_rate = (processed_videos / total_videos * 100) if total_videos > 0 else 0

            total_time = time.time() - start_time
            logger.info(f"Multi-process annotation completed in {total_time:.2f}s")
            logger.info(f"Process success rate: {process_success_count}/{len(processes)}")
            logger.info(f"Video processing success rate: {processed_videos}/{total_videos} ({actual_success_rate:.1f}%)")

            # Stage3 ê²°ê³¼ í™•ì¸
            if stage_results.get('stage3', False):
                stage3_files = [
                    Path(self.output_dir) / "stage3_dataset" / "train.pkl",
                    Path(self.output_dir) / "stage3_dataset" / "val.pkl",
                    Path(self.output_dir) / "stage3_dataset" / "test.pkl"
                ]
                stage3_success = all(f.exists() for f in stage3_files)
                logger.info(f"Stage3 dataset generation: {'âœ… Success' if stage3_success else 'âŒ Failed'}")

            # Stage3 ì„±ê³µ ì—¬ë¶€ í™•ì¸
            stage3_success = stage_results.get('stage3', False)

            if stage3_success:
                logger.info("âœ… Stage3 dataset generation completed successfully!")
            else:
                logger.warning("âš ï¸ Stage3 dataset generation failed")

            # stage1 ë©€í‹°í”„ë¡œì„¸ìŠ¤ë§Œ ì‹¤í–‰í•˜ë¯€ë¡œ stage1 ì„±ê³µë¥ ë¡œ íŒë‹¨
            overall_success = actual_success_rate >= 80.0

            if overall_success:
                logger.info("ğŸ‰ Multi-process stage1: OVERALL SUCCESS!")
            else:
                logger.warning("âš ï¸ Multi-process stage1: Partial success or failure")

            return overall_success
            
        except Exception as e:
            logger.error(f"Multi-process annotation failed: {e}")
            return False


# í¸ì˜ í•¨ìˆ˜
def run_multi_process_annotation(
    input_dir: str,
    output_dir: str,
    config_path: str,
    num_processes: int = 4,
    gpu_assignments: List[int] = None
) -> bool:
    """ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì–´ë…¸í…Œì´ì…˜ ì‹¤í–‰"""
    
    manager = MultiProcessAnnotationManager(
        input_dir=input_dir,
        output_dir=output_dir,
        config_path=config_path,
        num_processes=num_processes,
        gpu_assignments=gpu_assignments
    )
    
    return manager.run_full_pipeline()


class MultiProcessInferenceAnalysisManager:
    """ë©€í‹° í”„ë¡œì„¸ìŠ¤ inference.analysis ê´€ë¦¬ì"""
    
    def __init__(
        self,
        input_dir: str,
        output_dir: str,
        config_path: str,
        num_processes: int = 4,
        gpu_assignments: List[int] = None
    ):
        self.input_dir = input_dir
        self.config_path = config_path
        self.num_processes = num_processes
        self.gpu_assignments = gpu_assignments or [i % 2 for i in range(num_processes)]
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì…ë ¥ í´ë”ëª…ìœ¼ë¡œ ì„¤ì •
        input_folder_name = Path(input_dir).name
        self.output_dir = str(Path(output_dir) / input_folder_name)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.splitter = VideoDataSplitter(input_dir, num_processes)
        self.config_generator = InferenceAnalysisConfigGenerator(config_path)
        self.runner = MultiProcessRunner()
        self.merger = InferenceResultMerger(self.output_dir)
    
    def run_full_pipeline(self) -> bool:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            logger.info(f"Starting multi-process inference analysis with {self.num_processes} processes")
            logger.info(f"Input: {self.input_dir}")
            logger.info(f"Output: {self.output_dir}")
            logger.info(f"GPU assignments: {self.gpu_assignments}")
            
            # 1. ë¹„ë””ì˜¤ ë¶„í• 
            logger.info("=== Step 1: Splitting videos ===")
            video_splits = self.splitter.split_videos()
            if not video_splits:
                logger.error("No videos to process")
                return False
            
            # 2. ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (ì…ë ¥ ë””ë ‰í† ë¦¬ ê¸°ë°˜ìœ¼ë¡œ)
            input_parent = Path(self.input_dir).parent
            input_folder_name = Path(self.input_dir).name
            temp_base = input_parent / f"{input_folder_name}_temp_splits"
            split_dirs = self.splitter.create_split_directories(str(temp_base))
            
            # 3. ë¶„í• ë³„ ì„¤ì • íŒŒì¼ ìƒì„±
            logger.info("=== Step 2: Creating split configurations ===")
            config_paths = self.config_generator.create_split_configs(video_splits, split_dirs)
            
            # 4. ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            logger.info("=== Step 3: Running split processes ===")
            processes = self.runner.run_split_processes(config_paths, self.gpu_assignments)
            
            # 5. í”„ë¡œì„¸ìŠ¤ ëª¨ë‹ˆí„°ë§
            logger.info("=== Step 4: Monitoring processes ===")
            results = self.runner.monitor_processes(processes)
            
            # 6. ê²°ê³¼ í†µí•©
            logger.info("=== Step 5: Merging results ===")
            success_count = sum(1 for code in results.values() if code == 0)
            logger.info(f"Completed processes: {success_count}/{len(processes)}")
            
            # inference.analysis ê²°ê³¼ í†µí•©
            self.merger.merge_inference_results(split_dirs)
            
            # 7. ì •ë¦¬
            logger.info("=== Step 6: Cleanup ===")
            self.merger.cleanup_split_dirs(split_dirs)
            
            total_time = time.time() - start_time
            logger.info(f"Multi-process inference analysis completed in {total_time:.2f}s")
            logger.info(f"Success rate: {success_count}/{len(processes)}")
            
            return success_count == len(processes)
            
        except Exception as e:
            logger.error(f"Multi-process inference analysis failed: {e}")
            return False


class InferenceAnalysisConfigGenerator:
    """inference.analysisìš© ì„¤ì • íŒŒì¼ ìƒì„±ê¸°"""
    
    def __init__(self, base_config_path: str):
        self.base_config_path = base_config_path
        with open(base_config_path, 'r', encoding='utf-8') as f:
            self.base_config = yaml.safe_load(f)
    
    def create_split_configs(
        self, 
        video_splits: List[List[Path]], 
        split_dirs: List[str]
    ) -> List[str]:
        """ë¶„í• ë³„ ì„¤ì • íŒŒì¼ ìƒì„±"""
        config_paths = []
        
        for i, (videos, split_dir) in enumerate(zip(video_splits, split_dirs)):
            # ì„ì‹œ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_video_dir = Path(split_dir) / "videos"
            temp_video_dir.mkdir(exist_ok=True)
            
            # ë¹„ë””ì˜¤ íŒŒì¼ ì‹¬ë³¼ë¦­ ë§í¬ ìƒì„± (ì›ë³¸ í´ë” êµ¬ì¡° ë³´ì¡´)
            for video in videos:
                # ì›ë³¸ í´ë”ëª… ë³´ì¡´ (fight, normal ë“±)
                original_parent = video.parent.name
                target_subdir = temp_video_dir / original_parent
                target_subdir.mkdir(exist_ok=True)
                
                link_path = target_subdir / video.name
                if not link_path.exists():
                    try:
                        os.symlink(str(video.absolute()), str(link_path))
                    except:
                        # ì‹¬ë³¼ë¦­ ë§í¬ ì‹¤íŒ¨ì‹œ ë³µì‚¬
                        shutil.copy2(str(video), str(link_path))
            
            # ë¶„í• ë³„ ì„¤ì • ìƒì„±
            split_config = self.base_config.copy()
            
            # inference.analysis ëª¨ë“œë¡œ ì„¤ì •
            split_config['mode'] = 'inference.analysis'
            split_config['inference']['analysis']['input'] = str(temp_video_dir)
            split_config['inference']['analysis']['output_dir'] = split_dir
            
            # ë©€í‹° í”„ë¡œì„¸ìŠ¤ ë¹„í™œì„±í™” (subprocessì—ì„œëŠ” ë‹¨ì¼ ì²˜ë¦¬ë§Œ)
            split_config['multi_process']['enabled'] = False
            
            # ì„±ëŠ¥í‰ê°€ëŠ” ë§ˆì§€ë§‰ì—ë§Œ (ì²« ë²ˆì§¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì‹¤í–‰)
            split_config['inference']['analysis']['enable_evaluation'] = (i == 0)
            
            # ì„¤ì • íŒŒì¼ ì €ì¥
            config_path = Path(split_dir) / f"config_split_{i}.yaml"
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.safe_dump(split_config, f, allow_unicode=True)
            
            config_paths.append(str(config_path))
            logger.info(f"Created config for split {i}: {len(videos)} videos -> {config_path}")
        
        return config_paths


class InferenceResultMerger:
    """inference.analysis ê²°ê³¼ í†µí•©ê¸°"""
    
    def __init__(self, final_output_dir: str):
        self.final_output_dir = Path(final_output_dir)
        self.final_output_dir.mkdir(parents=True, exist_ok=True)
    
    def merge_inference_results(self, split_dirs: List[str]) -> bool:
        """inference.analysis ê²°ê³¼ í†µí•© (JSON + PKL íŒŒì¼)"""
        try:
            merged_count = 0
            pkl_merged_count = 0
            
            for split_dir in split_dirs:
                split_path = Path(split_dir)
                
                # JSON ê²°ê³¼ íŒŒì¼ë“¤ ì°¾ê¸°
                json_files = list(split_path.rglob("**/*_results.json"))
                
                for json_file in json_files:
                    try:
                        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°í•˜ë˜ videos/ ë¶€ë¶„ì„ ì œê±°
                        rel_path = json_file.relative_to(split_path)
                        
                        # videos/ ê²½ë¡œë¥¼ ì œê±°í•˜ê³  ì¬êµ¬ì„±
                        path_parts = list(rel_path.parts)
                        if path_parts and path_parts[0] == 'videos':
                            # videos/F_20_1_1_0_0/F_20_1_1_0_0_results.json -> F_20_1_1_0_0/F_20_1_1_0_0_results.json
                            new_rel_path = Path(*path_parts[1:])
                        else:
                            new_rel_path = rel_path
                        
                        final_path = self.final_output_dir / new_rel_path
                        
                        # ë””ë ‰í† ë¦¬ ìƒì„±
                        final_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # íŒŒì¼ ì´ë™ (ë˜ëŠ” ë³µì‚¬)
                        shutil.move(str(json_file), str(final_path))
                        merged_count += 1
                        
                    except Exception as e:
                        logger.warning(f"Failed to merge {json_file}: {e}")
                
                # PKL íŒŒì¼ë“¤ ì°¾ê¸° ë° í†µí•© (analysis ëª¨ë“œì´ë¯€ë¡œ í•„ìˆ˜)
                pkl_files = list(split_path.rglob("**/*.pkl"))
                
                for pkl_file in pkl_files:
                    try:
                        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚°í•˜ë˜ videos/ ë¶€ë¶„ì„ ì œê±°
                        rel_path = pkl_file.relative_to(split_path)
                        
                        # videos/ ê²½ë¡œë¥¼ ì œê±°í•˜ê³  ì¬êµ¬ì„±
                        path_parts = list(rel_path.parts)
                        if path_parts and path_parts[0] == 'videos':
                            new_rel_path = Path(*path_parts[1:])
                        else:
                            new_rel_path = rel_path
                        
                        final_path = self.final_output_dir / new_rel_path
                        
                        # ë””ë ‰í† ë¦¬ ìƒì„±
                        final_path.parent.mkdir(parents=True, exist_ok=True)
                        
                        # íŒŒì¼ ì´ë™ (ë˜ëŠ” ë³µì‚¬)
                        shutil.move(str(pkl_file), str(final_path))
                        pkl_merged_count += 1
                        
                    except Exception as e:
                        logger.warning(f"Failed to merge {pkl_file}: {e}")
                
                # evaluation ê²°ê³¼ê°€ ìˆìœ¼ë©´ í†µí•©
                eval_dir = split_path / "evaluation"
                if eval_dir.exists():
                    final_eval_dir = self.final_output_dir / "evaluation"
                    if final_eval_dir.exists():
                        shutil.rmtree(str(final_eval_dir))
                    shutil.move(str(eval_dir), str(final_eval_dir))
                    logger.info("Merged evaluation results")
            
            logger.info(f"Merged {merged_count} inference result files")
            logger.info(f"Merged {pkl_merged_count} PKL files")
            return merged_count > 0
            
        except Exception as e:
            logger.error(f"Error merging inference results: {e}")
            return False
    
    def cleanup_split_dirs(self, split_dirs: List[str]):
        """ë¶„í•  ë””ë ‰í† ë¦¬ ì™„ì „ ì •ë¦¬"""
        # temp_splits í´ë” ì „ì²´ ì‚­ì œ
        if split_dirs:
            temp_base = Path(split_dirs[0]).parent
            try:
                if temp_base.name.endswith("_temp_splits"):
                    shutil.rmtree(str(temp_base))
                    logger.info(f"Cleaned up temp_splits directory: {temp_base}")
                    return
            except Exception as e:
                logger.warning(f"Failed to cleanup temp_splits: {e}")
        
        # ê°œë³„ ë¶„í•  ë””ë ‰í† ë¦¬ ì‚­ì œ
        for split_dir in split_dirs:
            try:
                shutil.rmtree(split_dir)
                logger.info(f"Cleaned up split directory: {split_dir}")
            except Exception as e:
                logger.warning(f"Failed to cleanup {split_dir}: {e}")


def run_multi_process_inference_analysis(
    input_dir: str,
    output_dir: str,
    config_path: str,
    num_processes: int = 4,
    gpu_assignments: List[int] = None
) -> bool:
    """ë©€í‹° í”„ë¡œì„¸ìŠ¤ inference.analysis ì‹¤í–‰"""
    
    manager = MultiProcessInferenceAnalysisManager(
        input_dir=input_dir,
        output_dir=output_dir,
        config_path=config_path,
        num_processes=num_processes,
        gpu_assignments=gpu_assignments
    )
    
    return manager.run_full_pipeline()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = run_multi_process_annotation(
        input_dir="/aivanas/raw/surveillance/action/violence/action_recognition/data/RWF-2000",
        output_dir="/workspace/recognizer/output/RWF-2000",
        config_path="/workspace/recognizer/configs/config.yaml",
        num_processes=4,
        gpu_assignments=[0, 1, 0, 1]  # GPU 0, 1 ìˆœí™˜ í• ë‹¹
    )
    
    if success:
        print("Multi-process annotation completed successfully!")
    else:
        print("Multi-process annotation failed!")