# Recognizer ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ì„œ (Part 3/3)

## ë¬¸ì„œ ì •ë³´

| í•­ëª© | ë‚´ìš© |
|------|------|
| **í”„ë¡œì íŠ¸ëª…** | Recognizer - ì‹¤ì‹œê°„ ë™ì‘ ì¸ì‹ ë° ë¶„ì„ ì‹œìŠ¤í…œ |
| **ë¬¸ì„œ ìœ í˜•** | Software Design Document (SDD) |
| **ë²„ì „** | v2.0 |
| **ì‘ì„±ì¼** | 2025ë…„ |
| **ìŠ¹ì¸ì** | ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸ |
| **ë¶„ë¥˜** | ê¸°ìˆ  ì„¤ê³„ ë¬¸ì„œ |
| **ë¬¸ì„œ ë²”ìœ„** | Part 3: í’ˆì§ˆë³´ì¦, ë°°í¬/ìš´ì˜, í™•ì¥ì„±, ìœ„í—˜ê´€ë¦¬ |

## ëª©ì°¨ (Part 3)

9. [í’ˆì§ˆ ë³´ì¦ ë° í…ŒìŠ¤íŠ¸ ì „ëµ](#9-í’ˆì§ˆ-ë³´ì¦-ë°-í…ŒìŠ¤íŠ¸-ì „ëµ)
10. [ë°°í¬ ë° ìš´ì˜ ì„¤ê³„](#10-ë°°í¬-ë°-ìš´ì˜-ì„¤ê³„)
11. [í™•ì¥ì„± ë° ìœ ì§€ë³´ìˆ˜ ì„¤ê³„](#11-í™•ì¥ì„±-ë°-ìœ ì§€ë³´ìˆ˜-ì„¤ê³„)
12. [ìœ„í—˜ ê´€ë¦¬ ë° ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš](#12-ìœ„í—˜-ê´€ë¦¬-ë°-ë§ˆì´ê·¸ë ˆì´ì…˜-ê³„íš)

---

## 9. í’ˆì§ˆ ë³´ì¦ ë° í…ŒìŠ¤íŠ¸ ì „ëµ

### 9.1 í’ˆì§ˆ ë³´ì¦ í”„ë ˆì„ì›Œí¬

#### 9.1.1 í’ˆì§ˆ ë³´ì¦ ì•„í‚¤í…ì²˜

```mermaid
graph TD
    subgraph "Quality Assurance Framework"
        QAF1[Test Strategy]
        QAF2[Test Automation]
        QAF3[Quality Metrics]
        QAF4[Continuous Integration]
        QAF5[Quality Gates]
    end

    subgraph "Testing Layers"
        TL1[Unit Testing<br/>85% Coverage]
        TL2[Integration Testing<br/>70% Coverage]
        TL3[System Testing<br/>90% E2E Scenarios]
        TL4[Performance Testing<br/>Load & Stress]
        TL5[Security Testing<br/>OWASP Top 10]
    end

    subgraph "Quality Metrics"
        QM1[Code Quality<br/>SonarQube]
        QM2[Test Coverage<br/>pytest-cov]
        QM3[Performance Metrics<br/>Benchmarking]
        QM4[Security Scan<br/>Bandit, Safety]
        QM5[AI Model Quality<br/>Accuracy, F1-Score]
    end

    QAF1 --> TL1
    QAF2 --> TL2
    QAF3 --> TL3
    QAF4 --> TL4
    QAF5 --> TL5

    TL1 --> QM1
    TL2 --> QM2
    TL3 --> QM3
    TL4 --> QM4
    TL5 --> QM5
```

#### 9.1.2 í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ êµ¬í˜„

```python
import pytest
import numpy as np
import torch
from unittest.mock import Mock, patch
from typing import List, Dict, Any

class TestPyramid:
    """í…ŒìŠ¤íŠ¸ í”¼ë¼ë¯¸ë“œ êµ¬í˜„"""

    def __init__(self):
        self.unit_tests = UnitTestSuite()
        self.integration_tests = IntegrationTestSuite()
        self.system_tests = SystemTestSuite()
        self.performance_tests = PerformanceTestSuite()

class UnitTestSuite:
    """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""

    def test_pose_estimation_preprocessing(self):
        """í¬ì¦ˆ ì¶”ì • ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # Given
        estimator = RTMOEstimator(mock_config)
        input_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)

        # When
        processed = estimator.preprocess(input_frame)

        # Then
        assert processed.shape == (1, 3, 640, 640)
        assert processed.dtype == torch.float32
        assert 0 <= processed.min() <= processed.max() <= 1

    def test_tracking_association(self):
        """ì¶”ì  ì—°ê´€ì„± í…ŒìŠ¤íŠ¸"""
        # Given
        tracker = ByteTrackerWrapper(mock_config)
        detections = [
            Mock(bbox=[100, 100, 200, 200], confidence=0.8),
            Mock(bbox=[300, 300, 400, 400], confidence=0.9)
        ]

        # When
        tracked_objects = tracker.track(detections)

        # Then
        assert len(tracked_objects) == 2
        assert all(obj.track_id is not None for obj in tracked_objects)
        assert all(obj.track_state in ['New', 'Tracked'] for obj in tracked_objects)

    def test_classification_window_processing(self):
        """ë¶„ë¥˜ ìœˆë„ìš° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # Given
        processor = SlidingWindowProcessor(mock_config)
        mock_persons = [Mock() for _ in range(5)]

        # When
        for i in range(100):  # ìœˆë„ìš° í¬ê¸°ë§Œí¼ ì¶”ê°€
            processor.add_frame(mock_persons)

        # Then
        assert processor.is_ready()
        window_data = processor.get_window()
        assert window_data.tensor.shape == (4, 100, 17, 2)  # [M, T, V, C]

class IntegrationTestSuite:
    """í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""

    @pytest.mark.integration
    async def test_pose_estimation_to_tracking_integration(self):
        """í¬ì¦ˆ ì¶”ì •-ì¶”ì  í†µí•© í…ŒìŠ¤íŠ¸"""
        # Given
        pose_estimator = RTMOEstimator(test_config)
        tracker = ByteTrackerWrapper(test_config)
        test_video_path = "test_data/sample_video.mp4"

        # When
        cap = cv2.VideoCapture(test_video_path)
        frame_count = 0
        tracked_persons_history = []

        while frame_count < 30:  # 30í”„ë ˆì„ í…ŒìŠ¤íŠ¸
            ret, frame = cap.read()
            if not ret:
                break

            # í¬ì¦ˆ ì¶”ì •
            persons = await pose_estimator.estimate(frame)

            # ì¶”ì 
            tracked_persons = tracker.track(persons)
            tracked_persons_history.append(tracked_persons)

            frame_count += 1

        # Then
        assert frame_count == 30
        assert len(tracked_persons_history) == 30

        # ID ì¼ê´€ì„± ê²€ì¦
        id_consistency = self._calculate_id_consistency(tracked_persons_history)
        assert id_consistency > 0.85  # 85% ì´ìƒ ID ì¼ê´€ì„±

    @pytest.mark.integration
    async def test_end_to_end_pipeline(self):
        """ì¢…ë‹¨ê°„ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        # Given
        pipeline = DualServicePipeline(test_config)
        test_frame = self._load_test_frame()

        # When
        result = await pipeline.process_frame(test_frame)

        # Then
        assert result is not None
        assert 'poses' in result
        assert 'processing_time' in result
        assert result['processing_time'] < 0.1  # 100ms ì´í•˜

    def _calculate_id_consistency(self, tracked_history: List[List]) -> float:
        """ID ì¼ê´€ì„± ê³„ì‚°"""
        if len(tracked_history) < 2:
            return 1.0

        total_transitions = 0
        consistent_transitions = 0

        for i in range(1, len(tracked_history)):
            prev_frame = tracked_history[i-1]
            curr_frame = tracked_history[i]

            for curr_person in curr_frame:
                # ì´ì „ í”„ë ˆì„ì—ì„œ ë™ì¼í•œ ID ì°¾ê¸°
                matching_person = None
                for prev_person in prev_frame:
                    if prev_person.track_id == curr_person.track_id:
                        matching_person = prev_person
                        break

                if matching_person:
                    # ê³µê°„ì  ì¼ê´€ì„± ê²€ì¦
                    distance = self._calculate_bbox_distance(
                        matching_person.bbox, curr_person.bbox
                    )
                    if distance < 50:  # í”½ì…€ ë‹¨ìœ„ ì„ê³„ê°’
                        consistent_transitions += 1

                total_transitions += 1

        return consistent_transitions / total_transitions if total_transitions > 0 else 1.0

class SystemTestSuite:
    """ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""

    @pytest.mark.system
    async def test_realtime_processing_performance(self):
        """ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # Given
        realtime_mode = RealtimeMode(performance_test_config)
        test_video = "test_data/performance_test_video.mp4"

        # When
        performance_metrics = []
        start_time = time.time()

        async with realtime_mode.start_processing(test_video) as processor:
            for _ in range(300):  # 10ì´ˆê°„ 30 FPS í…ŒìŠ¤íŠ¸
                frame_start = time.time()
                result = await processor.process_next_frame()
                frame_end = time.time()

                processing_time = frame_end - frame_start
                performance_metrics.append(processing_time)

        end_time = time.time()
        total_time = end_time - start_time

        # Then
        avg_processing_time = np.mean(performance_metrics)
        fps = len(performance_metrics) / total_time

        assert avg_processing_time < 0.033  # 30 FPS ìœ ì§€ (33ms ì´í•˜)
        assert fps >= 28  # ìµœì†Œ 28 FPS
        assert np.percentile(performance_metrics, 95) < 0.05  # 95% íƒ€ì¼ 50ms ì´í•˜

    @pytest.mark.system
    def test_memory_leak_detection(self):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        # Given
        import psutil
        process = psutil.Process()
        initial_memory = process.memory_info().rss

        pipeline = DualServicePipeline(test_config)
        test_frames = [self._generate_random_frame() for _ in range(1000)]

        # When
        for frame in test_frames:
            result = pipeline.process_frame(frame)
            del result  # ëª…ì‹œì  ì‚­ì œ

        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        import gc
        gc.collect()

        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory

        # Then
        memory_increase_mb = memory_increase / (1024 * 1024)
        assert memory_increase_mb < 100  # 100MB ì´í•˜ ì¦ê°€

    @pytest.mark.system
    async def test_fault_tolerance(self):
        """ì¥ì•  í—ˆìš©ì„± í…ŒìŠ¤íŠ¸"""
        # Given
        pipeline = DualServicePipeline(fault_tolerance_config)

        # When & Then
        # 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œë®¬ë ˆì´ì…˜
        with patch('torch.cuda.OutOfMemoryError'):
            result = await pipeline.process_frame(test_frame)
            assert result is not None  # ê·¸ë ˆì´ìŠ¤í’€ ë””ê·¸ë¼ë°ì´ì…˜

        # 2. ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
        with patch.object(pipeline.pose_estimator, 'estimate', side_effect=Exception("Model error")):
            result = await pipeline.process_frame(test_frame)
            assert result['error_recovered'] is True

        # 3. ë„¤íŠ¸ì›Œí¬ ì¥ì•  ì‹œë®¬ë ˆì´ì…˜
        with patch('requests.post', side_effect=ConnectionError()):
            await pipeline.send_notification(mock_event)
            # ë¡œì»¬ íì— ì €ì¥ë˜ì–´ì•¼ í•¨
            assert len(pipeline.notification_queue) > 0

class PerformanceTestSuite:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""

    @pytest.mark.performance
    def test_load_testing(self):
        """ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        # Given
        concurrent_requests = 10
        requests_per_client = 100

        async def client_simulation(client_id: int):
            client = RecognizerClient(test_api_url)
            response_times = []

            for i in range(requests_per_client):
                start_time = time.time()
                result = await client.estimate_pose(test_image)
                end_time = time.time()

                response_times.append(end_time - start_time)

            return response_times

        # When
        loop = asyncio.get_event_loop()
        tasks = [client_simulation(i) for i in range(concurrent_requests)]
        results = loop.run_until_complete(asyncio.gather(*tasks))

        # Then
        all_response_times = [time for client_times in results for time in client_times]

        avg_response_time = np.mean(all_response_times)
        p95_response_time = np.percentile(all_response_times, 95)
        throughput = len(all_response_times) / max(all_response_times)

        assert avg_response_time < 0.5  # í‰ê·  ì‘ë‹µì‹œê°„ 500ms ì´í•˜
        assert p95_response_time < 1.0  # 95% íƒ€ì¼ 1ì´ˆ ì´í•˜
        assert throughput >= 20  # ì´ˆë‹¹ 20 ìš”ì²­ ì´ìƒ

    @pytest.mark.performance
    def test_stress_testing(self):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        # ì ì§„ì  ë¶€í•˜ ì¦ê°€ë¡œ í•œê³„ì  íƒì§€
        max_concurrent_users = 100
        step_size = 10
        step_duration = 60  # ê° ë‹¨ê³„ë§ˆë‹¤ 60ì´ˆ

        performance_data = []

        for concurrent_users in range(step_size, max_concurrent_users + 1, step_size):
            # ê° ë‹¨ê³„ë³„ ì„±ëŠ¥ ì¸¡ì •
            step_performance = self._measure_performance_at_load(
                concurrent_users, step_duration
            )
            performance_data.append(step_performance)

            # ì„±ëŠ¥ ì €í•˜ ì„ê³„ì  í™•ì¸
            if step_performance['error_rate'] > 0.05:  # 5% ì´ˆê³¼ ì—ëŸ¬ìœ¨
                break

        # ì•ˆì •ì ì¸ ìµœëŒ€ ì²˜ë¦¬ëŸ‰ ê¸°ë¡
        stable_performance = [p for p in performance_data if p['error_rate'] <= 0.01]
        max_stable_load = max(p['concurrent_users'] for p in stable_performance)

        assert max_stable_load >= 50  # ìµœì†Œ 50 ë™ì‹œ ì‚¬ìš©ì ì§€ì›
```

### 9.2 AI ëª¨ë¸ í’ˆì§ˆ ë³´ì¦

#### 9.2.1 ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦ í”„ë ˆì„ì›Œí¬

```python
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns

class ModelQualityAssurance:
    """AI ëª¨ë¸ í’ˆì§ˆ ë³´ì¦"""

    def __init__(self, test_dataset_path: str):
        self.test_dataset = self._load_test_dataset(test_dataset_path)
        self.ground_truth = self._load_ground_truth()
        self.quality_thresholds = {
            'accuracy': 0.90,
            'precision': 0.85,
            'recall': 0.85,
            'f1_score': 0.85
        }

    def validate_pose_estimation_model(self, model: RTMOEstimator) -> Dict[str, float]:
        """í¬ì¦ˆ ì¶”ì • ëª¨ë¸ ê²€ì¦"""
        predictions = []
        ground_truth_poses = []

        for test_sample in self.test_dataset['pose_estimation']:
            image = test_sample['image']
            gt_poses = test_sample['poses']

            # ëª¨ë¸ ì˜ˆì¸¡
            predicted_poses = model.estimate(image)

            # OKS (Object Keypoint Similarity) ê³„ì‚°
            oks_scores = self._calculate_oks(predicted_poses, gt_poses)
            predictions.extend(oks_scores)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_oks = np.mean(predictions)
        detection_accuracy = len([s for s in predictions if s > 0.5]) / len(predictions)

        metrics = {
            'average_oks': avg_oks,
            'detection_accuracy': detection_accuracy,
            'keypoint_accuracy': self._calculate_keypoint_accuracy(predictions)
        }

        # í’ˆì§ˆ ê¸°ì¤€ ê²€ì¦
        self._validate_quality_thresholds(metrics, 'pose_estimation')

        return metrics

    def validate_action_classification_model(self, model: STGCNClassifier) -> Dict[str, float]:
        """ë™ì‘ ë¶„ë¥˜ ëª¨ë¸ ê²€ì¦"""
        y_true = []
        y_pred = []
        confidence_scores = []

        for test_sample in self.test_dataset['action_classification']:
            window_data = test_sample['window_data']
            true_action = test_sample['action']

            # ëª¨ë¸ ì˜ˆì¸¡
            result = model.classify(window_data)
            predicted_action = result.action_type
            confidence = result.confidence

            y_true.append(true_action)
            y_pred.append(predicted_action)
            confidence_scores.append(confidence)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_true, y_pred, average='weighted'
        )

        # í˜¼ë™ í–‰ë ¬ ìƒì„±
        cm = confusion_matrix(y_true, y_pred)

        metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'avg_confidence': np.mean(confidence_scores),
            'confusion_matrix': cm.tolist()
        }

        # í’ˆì§ˆ ê¸°ì¤€ ê²€ì¦
        self._validate_quality_thresholds(metrics, 'action_classification')

        return metrics

    def _calculate_oks(self, predicted_poses: List[Person],
                      ground_truth_poses: List[Person]) -> List[float]:
        """Object Keypoint Similarity ê³„ì‚°"""
        oks_scores = []

        for pred_pose in predicted_poses:
            best_oks = 0.0

            for gt_pose in ground_truth_poses:
                oks = self._compute_oks_single(pred_pose.keypoints, gt_pose.keypoints)
                best_oks = max(best_oks, oks)

            oks_scores.append(best_oks)

        return oks_scores

    def _compute_oks_single(self, pred_keypoints: np.ndarray,
                           gt_keypoints: np.ndarray) -> float:
        """ë‹¨ì¼ í¬ì¦ˆì— ëŒ€í•œ OKS ê³„ì‚°"""
        # COCO í‚¤í¬ì¸íŠ¸ ì‹œê·¸ë§ˆ ê°’
        sigmas = np.array([
            .26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89
        ]) / 10.0

        # ë°”ìš´ë”© ë°•ìŠ¤ ë©´ì  ê³„ì‚° (ì •ê·œí™”ìš©)
        x_coords = gt_keypoints[:, 0]
        y_coords = gt_keypoints[:, 1]
        area = (x_coords.max() - x_coords.min()) * (y_coords.max() - y_coords.min())

        # ê±°ë¦¬ ê³„ì‚°
        dx = pred_keypoints[:, 0] - gt_keypoints[:, 0]
        dy = pred_keypoints[:, 1] - gt_keypoints[:, 1]
        distances = np.sqrt(dx**2 + dy**2)

        # OKS ê³„ì‚°
        visibility = gt_keypoints[:, 2] > 0
        e = distances / (2 * sigmas * np.sqrt(area) + np.finfo(float).eps)
        oks_per_keypoint = np.exp(-e**2 / 2) * visibility

        return oks_per_keypoint.sum() / visibility.sum() if visibility.sum() > 0 else 0

    def generate_quality_report(self, model_metrics: Dict[str, Dict]) -> str:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        report = "# AI Model Quality Assessment Report\n\n"

        for model_name, metrics in model_metrics.items():
            report += f"## {model_name.title()} Model\n\n"

            for metric_name, value in metrics.items():
                if metric_name == 'confusion_matrix':
                    continue  # ë³„ë„ ì²˜ë¦¬

                if isinstance(value, float):
                    report += f"- **{metric_name.title()}**: {value:.3f}\n"
                else:
                    report += f"- **{metric_name.title()}**: {value}\n"

            # í˜¼ë™ í–‰ë ¬ ì‹œê°í™” ìƒì„±
            if 'confusion_matrix' in metrics:
                self._save_confusion_matrix_plot(
                    metrics['confusion_matrix'],
                    f"{model_name}_confusion_matrix.png"
                )
                report += f"- **Confusion Matrix**: ![{model_name}_confusion_matrix.png]\n"

            report += "\n"

        return report

class ContinuousQualityMonitoring:
    """ì§€ì†ì  í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""

    def __init__(self, model_registry: Dict[str, Any]):
        self.model_registry = model_registry
        self.quality_history = {}
        self.alert_thresholds = {
            'accuracy_drop': 0.05,  # 5% ì •í™•ë„ í•˜ë½
            'confidence_drop': 0.10,  # 10% ì‹ ë¢°ë„ í•˜ë½
            'latency_increase': 0.20  # 20% ì§€ì—°ì‹œê°„ ì¦ê°€
        }

    async def monitor_model_drift(self):
        """ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§"""
        while True:
            for model_name, model in self.model_registry.items():
                # í˜„ì¬ ì„±ëŠ¥ ì¸¡ì •
                current_metrics = await self._measure_current_performance(model)

                # íˆìŠ¤í† ë¦¬ì™€ ë¹„êµ
                if model_name in self.quality_history:
                    drift_detected = self._detect_performance_drift(
                        model_name, current_metrics
                    )

                    if drift_detected:
                        await self._handle_model_drift(model_name, drift_detected)

                # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                self._update_quality_history(model_name, current_metrics)

            await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

    def _detect_performance_drift(self, model_name: str,
                                current_metrics: Dict) -> Dict[str, float]:
        """ì„±ëŠ¥ ë“œë¦¬í”„íŠ¸ ê°ì§€"""
        historical_metrics = self.quality_history[model_name]
        baseline = self._calculate_baseline_metrics(historical_metrics)

        drift_detected = {}

        for metric, current_value in current_metrics.items():
            if metric in baseline:
                baseline_value = baseline[metric]
                relative_change = abs(current_value - baseline_value) / baseline_value

                if metric in ['accuracy', 'f1_score'] and current_value < baseline_value:
                    # ì„±ëŠ¥ í•˜ë½ ê°ì§€
                    if relative_change > self.alert_thresholds['accuracy_drop']:
                        drift_detected[metric] = relative_change

                elif metric == 'avg_confidence' and current_value < baseline_value:
                    # ì‹ ë¢°ë„ í•˜ë½ ê°ì§€
                    if relative_change > self.alert_thresholds['confidence_drop']:
                        drift_detected[metric] = relative_change

                elif metric == 'avg_latency' and current_value > baseline_value:
                    # ì§€ì—°ì‹œê°„ ì¦ê°€ ê°ì§€
                    if relative_change > self.alert_thresholds['latency_increase']:
                        drift_detected[metric] = relative_change

        return drift_detected

    async def _handle_model_drift(self, model_name: str, drift_info: Dict[str, float]):
        """ëª¨ë¸ ë“œë¦¬í”„íŠ¸ ì²˜ë¦¬"""
        # ì•Œë¦¼ ë°œì†¡
        await self._send_drift_alert(model_name, drift_info)

        # ìë™ ë³µêµ¬ ì‹œë„
        recovery_actions = {
            'accuracy': self._trigger_model_retraining,
            'avg_confidence': self._adjust_confidence_thresholds,
            'avg_latency': self._optimize_model_performance
        }

        for metric, change in drift_info.items():
            if metric in recovery_actions:
                await recovery_actions[metric](model_name, change)
```

### 9.3 ìë™í™”ëœ í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸

#### 9.3.1 CI/CD í…ŒìŠ¤íŠ¸ ìë™í™”

```yaml
# .github/workflows/quality-assurance.yml
name: Quality Assurance Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10]

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-test.txt

    - name: Run unit tests with coverage
      run: |
        pytest tests/unit/ \
          --cov=recognizer \
          --cov-report=xml \
          --cov-report=html \
          --cov-fail-under=85

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests

    services:
      redis:
        image: redis:6.2
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Build test image
      run: |
        docker build -t recognizer:test -f Dockerfile.test .

    - name: Run integration tests
      run: |
        docker run --rm \
          --network host \
          -v ${{ github.workspace }}/test-results:/app/test-results \
          recognizer:test \
          pytest tests/integration/ \
            --junitxml=test-results/integration.xml \
            --html=test-results/integration.html

  model-quality-tests:
    runs-on: ubuntu-latest
    needs: integration-tests

    steps:
    - uses: actions/checkout@v3

    - name: Download test models
      run: |
        mkdir -p models/test/
        # í…ŒìŠ¤íŠ¸ìš© ê²½ëŸ‰ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        wget -O models/test/rtmo_test.onnx https://example.com/rtmo_test.onnx
        wget -O models/test/stgcn_test.onnx https://example.com/stgcn_test.onnx

    - name: Run model quality tests
      run: |
        python tests/model_quality/validate_models.py \
          --model-dir models/test/ \
          --test-data tests/data/quality_test_dataset/ \
          --output-report model_quality_report.json

    - name: Upload model quality report
      uses: actions/upload-artifact@v3
      with:
        name: model-quality-report
        path: model_quality_report.json

  performance-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]

    steps:
    - uses: actions/checkout@v3

    - name: Set up performance test environment
      run: |
        docker-compose -f docker-compose.perf-test.yml up -d
        sleep 30  # ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸°

    - name: Run performance tests
      run: |
        python tests/performance/load_test.py \
          --target-url http://localhost:8000 \
          --concurrent-users 50 \
          --duration 300 \
          --output-report performance_report.json

    - name: Analyze performance results
      run: |
        python tests/performance/analyze_results.py \
          --report performance_report.json \
          --thresholds tests/performance/performance_thresholds.json

    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: performance_report.json

  security-tests:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Run security scan with Bandit
      run: |
        bandit -r recognizer/ -f json -o security_report.json

    - name: Run dependency vulnerability scan
      run: |
        safety check --json --output safety_report.json

    - name: OWASP ZAP security scan
      run: |
        docker run -v $(pwd):/zap/wrk/:rw \
          -t owasp/zap2docker-stable \
          zap-baseline.py \
          -t http://localhost:8000 \
          -J zap_report.json

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          security_report.json
          safety_report.json
          zap_report.json

  quality-gate:
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, model-quality-tests, performance-tests, security-tests]

    steps:
    - name: Download all reports
      uses: actions/download-artifact@v3

    - name: Quality Gate Assessment
      run: |
        python tools/quality_gate.py \
          --coverage-report coverage.xml \
          --model-quality-report model-quality-report/model_quality_report.json \
          --performance-report performance-report/performance_report.json \
          --security-reports security-reports/ \
          --quality-criteria quality_criteria.yml \
          --fail-on-quality-gate-failure
```

---

## 10. ë°°í¬ ë° ìš´ì˜ ì„¤ê³„

### 10.1 ì»¨í…Œì´ë„ˆí™” ë° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

#### 10.1.1 Docker ì»¨í…Œì´ë„ˆ ì„¤ê³„

```dockerfile
# Dockerfile.production
FROM nvidia/cuda:11.8-devel-ubuntu20.04

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    python3.9 \
    python3.9-pip \
    python3.9-dev \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    wget \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Python í™˜ê²½ ì„¤ì •
RUN ln -s /usr/bin/python3.9 /usr/bin/python
RUN python -m pip install --upgrade pip

# ì‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ONNX Runtime GPU ì„¤ì¹˜
RUN pip install onnxruntime-gpu==1.15.1

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY recognizer/ ./recognizer/
COPY configs/ ./configs/
COPY models/ ./models/

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
COPY scripts/download_models.sh .
RUN chmod +x download_models.sh && ./download_models.sh

# ê¶Œí•œ ì„¤ì •
RUN useradd -m -u 1000 recognizer && \
    chown -R recognizer:recognizer /app
USER recognizer

# í—¬ìŠ¤ì²´í¬ ì„¤ì •
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ ì„¤ì •
COPY scripts/entrypoint.sh .
ENTRYPOINT ["./entrypoint.sh"]

# ê¸°ë³¸ ëª…ë ¹ì–´
CMD ["python", "-m", "recognizer.main", "--mode", "inference.realtime"]
```

#### 10.1.2 Kubernetes ë°°í¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: recognizer
  labels:
    name: recognizer

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: recognizer-config
  namespace: recognizer
data:
  config.yaml: |
    mode: inference.realtime
    gpu:
      enabled: true
      device_ids: [0]
    logging:
      level: INFO
      format: json
    performance:
      target_fps: 30
      max_latency_ms: 100

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: recognizer-secrets
  namespace: recognizer
type: Opaque
data:
  api_key: <base64_encoded_api_key>
  jwt_secret: <base64_encoded_jwt_secret>

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: recognizer-api
  namespace: recognizer
  labels:
    app: recognizer-api
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: recognizer-api
  template:
    metadata:
      labels:
        app: recognizer-api
    spec:
      nodeSelector:
        node-type: gpu
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

      containers:
      - name: recognizer-api
        image: recognizer:latest
        imagePullPolicy: Always

        ports:
        - containerPort: 8000
          name: http
        - containerPort: 9090
          name: metrics

        env:
        - name: CONFIG_PATH
          value: "/app/config/config.yaml"
        - name: API_KEY
          valueFrom:
            secretKeyRef:
              name: recognizer-secrets
              key: api_key

        resources:
          requests:
            cpu: "2"
            memory: "8Gi"
            nvidia.com/gpu: 1
          limits:
            cpu: "4"
            memory: "16Gi"
            nvidia.com/gpu: 1

        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: model-cache
          mountPath: /app/models
        - name: temp-storage
          mountPath: /tmp

        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

      volumes:
      - name: config-volume
        configMap:
          name: recognizer-config
      - name: model-cache
        persistentVolumeClaim:
          claimName: model-cache-pvc
      - name: temp-storage
        emptyDir:
          sizeLimit: 10Gi

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: recognizer-api-service
  namespace: recognizer
  labels:
    app: recognizer-api
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8000
    protocol: TCP
    name: http
  - port: 9090
    targetPort: 9090
    protocol: TCP
    name: metrics
  selector:
    app: recognizer-api

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: recognizer-ingress
  namespace: recognizer
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  tls:
  - hosts:
    - api.recognizer.example.com
    secretName: recognizer-tls
  rules:
  - host: api.recognizer.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: recognizer-api-service
            port:
              number: 80

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: recognizer-hpa
  namespace: recognizer
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: recognizer-api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60
```

### 10.2 ëª¨ë‹ˆí„°ë§ ë° ê´€ì°°ì„±

#### 10.2.1 í†µí•© ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ

```yaml
# monitoring/prometheus.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    rule_files:
      - "/etc/prometheus/rules/*.yml"

    scrape_configs:
    - job_name: 'recognizer-api'
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
          names:
          - recognizer
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_name]
        action: keep
        regex: recognizer-api-service
      - source_labels: [__meta_kubernetes_endpoint_port_name]
        action: keep
        regex: metrics

    - job_name: 'gpu-metrics'
      static_configs:
      - targets: ['nvidia-dcgm-exporter:9400']

    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true

---
# monitoring/alerting-rules.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: monitoring
data:
  recognizer.yml: |
    groups:
    - name: recognizer.rules
      rules:

      # API ì‘ë‹µì‹œê°„ ì•Œë¦¼
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="recognizer-api"}[5m])) > 1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High API latency detected"
          description: "95th percentile latency is {{ $value }}s"

      # GPU ì‚¬ìš©ë¥  ì•Œë¦¼
      - alert: HighGPUUtilization
        expr: nvidia_gpu_utilization_gpu > 95
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High GPU utilization"
          description: "GPU utilization is {{ $value }}%"

      # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì•Œë¦¼
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value }}%"

      # ëª¨ë¸ ì •í™•ë„ ì €í•˜ ì•Œë¦¼
      - alert: ModelAccuracyDrop
        expr: model_accuracy < 0.85
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "Model accuracy dropped below threshold"
          description: "Model accuracy is {{ $value }}"

      # ì„œë¹„ìŠ¤ ë‹¤ìš´ ì•Œë¦¼
      - alert: ServiceDown
        expr: up{job="recognizer-api"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Recognizer service is down"
          description: "Service {{ $labels.instance }} has been down for more than 1 minute"
```

#### 10.2.2 ë¡œê¹… ë° ì¶”ì  ì‹œìŠ¤í…œ

```python
import logging
import structlog
from pythonjsonlogger import jsonlogger
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

class ObservabilityManager:
    """ê´€ì°°ì„± ê´€ë¦¬ì"""

    def __init__(self, config: ObservabilityConfig):
        self.config = config
        self.setup_logging()
        self.setup_tracing()
        self.setup_metrics()

    def setup_logging(self):
        """êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •"""
        # JSON ë¡œê±° ì„¤ì •
        json_handler = logging.StreamHandler()
        formatter = jsonlogger.JsonFormatter(
            fmt='%(asctime)s %(name)s %(levelname)s %(message)s'
        )
        json_handler.setFormatter(formatter)

        # êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )

        # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
        logging.basicConfig(
            level=self.config.log_level,
            handlers=[json_handler]
        )

    def setup_tracing(self):
        """ë¶„ì‚° ì¶”ì  ì„¤ì •"""
        # Tracer Provider ì„¤ì •
        trace.set_tracer_provider(TracerProvider())
        tracer = trace.get_tracer(__name__)

        # Jaeger Exporter ì„¤ì •
        jaeger_exporter = JaegerExporter(
            agent_host_name=self.config.jaeger_host,
            agent_port=self.config.jaeger_port,
        )

        # Span Processor ì„¤ì •
        span_processor = BatchSpanProcessor(jaeger_exporter)
        trace.get_tracer_provider().add_span_processor(span_processor)

        # ìë™ ê³„ì¸¡ ì„¤ì •
        FastAPIInstrumentor.instrument()
        RequestsInstrumentor().instrument()

    def setup_metrics(self):
        """ë©”íŠ¸ë¦­ ì„¤ì •"""
        from prometheus_client import Counter, Histogram, Gauge, start_http_server

        # ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ì •ì˜
        self.metrics = {
            'api_requests_total': Counter(
                'api_requests_total',
                'Total API requests',
                ['method', 'endpoint', 'status']
            ),
            'api_request_duration': Histogram(
                'api_request_duration_seconds',
                'API request duration',
                ['method', 'endpoint']
            ),
            'pose_estimation_duration': Histogram(
                'pose_estimation_duration_seconds',
                'Pose estimation duration'
            ),
            'tracking_duration': Histogram(
                'tracking_duration_seconds',
                'Tracking duration'
            ),
            'classification_duration': Histogram(
                'classification_duration_seconds',
                'Classification duration'
            ),
            'active_sessions': Gauge(
                'active_realtime_sessions',
                'Number of active realtime sessions'
            ),
            'model_accuracy': Gauge(
                'model_accuracy',
                'Current model accuracy',
                ['model_name']
            ),
            'gpu_memory_usage': Gauge(
                'gpu_memory_usage_bytes',
                'GPU memory usage',
                ['gpu_id']
            )
        }

        # ë©”íŠ¸ë¦­ ì„œë²„ ì‹œì‘
        start_http_server(self.config.metrics_port)

class PerformanceTracker:
    """ì„±ëŠ¥ ì¶”ì ê¸°"""

    def __init__(self, metrics: Dict):
        self.metrics = metrics
        self.logger = structlog.get_logger()

    @contextmanager
    def track_operation(self, operation_name: str, **labels):
        """ì‘ì—… ì¶”ì  ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
        start_time = time.time()

        with trace.get_tracer(__name__).start_as_current_span(operation_name) as span:
            # ìŠ¤íŒ¬ì— ë¼ë²¨ ì¶”ê°€
            for key, value in labels.items():
                span.set_attribute(key, value)

            try:
                yield span

            except Exception as e:
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                raise

            finally:
                duration = time.time() - start_time

                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                if f'{operation_name}_duration' in self.metrics:
                    self.metrics[f'{operation_name}_duration'].observe(duration)

                # ë¡œê·¸ ê¸°ë¡
                self.logger.info(
                    f"{operation_name}_completed",
                    duration=duration,
                    **labels
                )

    def track_model_performance(self, model_name: str, accuracy: float, latency: float):
        """ëª¨ë¸ ì„±ëŠ¥ ì¶”ì """
        self.metrics['model_accuracy'].labels(model_name=model_name).set(accuracy)

        self.logger.info(
            "model_performance_updated",
            model_name=model_name,
            accuracy=accuracy,
            latency=latency
        )

    def track_resource_usage(self):
        """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¶”ì """
        import psutil
        import GPUtil

        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        gpus = GPUtil.getGPUs()
        for i, gpu in enumerate(gpus):
            memory_used = gpu.memoryUsed * 1024 * 1024  # MB to bytes
            self.metrics['gpu_memory_usage'].labels(gpu_id=str(i)).set(memory_used)

        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë¡œê¹…
        self.logger.info(
            "resource_usage",
            cpu_percent=psutil.cpu_percent(),
            memory_percent=psutil.virtual_memory().percent,
            gpu_count=len(gpus),
            gpu_utilization=[gpu.load * 100 for gpu in gpus]
        )
```

### 10.3 ë°°í¬ ìë™í™” ë° GitOps

#### 10.3.1 GitOps ë°°í¬ íŒŒì´í”„ë¼ì¸

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    tags:
      - 'v*'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=tag
          type=sha,prefix={{branch}}-

    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        file: ./Dockerfile.production
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    environment: staging

    steps:
    - name: Checkout GitOps repository
      uses: actions/checkout@v3
      with:
        repository: company/recognizer-gitops
        token: ${{ secrets.GITOPS_TOKEN }}
        path: gitops

    - name: Update staging manifests
      run: |
        cd gitops/environments/staging
        yq eval '.spec.template.spec.containers[0].image = "${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}"' -i deployment.yaml

    - name: Commit and push changes
      run: |
        cd gitops
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git add .
        git commit -m "Deploy ${{ github.ref_name }} to staging"
        git push

  integration-tests-staging:
    needs: deploy-staging
    runs-on: ubuntu-latest

    steps:
    - name: Wait for deployment
      run: sleep 120  # ë°°í¬ ì™„ë£Œ ëŒ€ê¸°

    - name: Run integration tests against staging
      run: |
        pytest tests/integration/ \
          --target-url https://staging-api.recognizer.example.com \
          --api-key ${{ secrets.STAGING_API_KEY }}

  deploy-production:
    needs: integration-tests-staging
    runs-on: ubuntu-latest
    environment: production
    if: startsWith(github.ref, 'refs/tags/v')

    steps:
    - name: Checkout GitOps repository
      uses: actions/checkout@v3
      with:
        repository: company/recognizer-gitops
        token: ${{ secrets.GITOPS_TOKEN }}
        path: gitops

    - name: Update production manifests
      run: |
        cd gitops/environments/production
        yq eval '.spec.template.spec.containers[0].image = "${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}"' -i deployment.yaml

    - name: Commit and push changes
      run: |
        cd gitops
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        git add .
        git commit -m "Deploy ${{ github.ref_name }} to production"
        git push

  smoke-tests-production:
    needs: deploy-production
    runs-on: ubuntu-latest

    steps:
    - name: Wait for deployment
      run: sleep 180  # í”„ë¡œë•ì…˜ ë°°í¬ ì™„ë£Œ ëŒ€ê¸°

    - name: Run smoke tests
      run: |
        pytest tests/smoke/ \
          --target-url https://api.recognizer.example.com \
          --api-key ${{ secrets.PRODUCTION_API_KEY }}

    - name: Notify deployment success
      if: success()
      uses: 8398a7/action-slack@v3
      with:
        status: success
        text: "ğŸš€ Recognizer ${{ github.ref_name }} successfully deployed to production!"
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

    - name: Rollback on failure
      if: failure()
      run: |
        # ìë™ ë¡¤ë°± ë¡œì§
        kubectl rollout undo deployment/recognizer-api -n recognizer
```

---

## 11. í™•ì¥ì„± ë° ìœ ì§€ë³´ìˆ˜ ì„¤ê³„

### 11.1 ìˆ˜í‰ì  í™•ì¥ ì„¤ê³„

#### 11.1.1 ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ í™•ì¥ ì „ëµ

```mermaid
graph TD
    subgraph "Load Balancer Layer"
        LB1[API Gateway]
        LB2[Service Mesh]
        LB3[Traffic Distribution]
    end

    subgraph "Service Layer - Auto Scaling"
        S1[Pose Estimation Service<br/>Min: 3, Max: 20]
        S2[Tracking Service<br/>Min: 2, Max: 15]
        S3[Classification Service<br/>Min: 3, Max: 25]
        S4[Event Management Service<br/>Min: 2, Max: 10]
    end

    subgraph "Data Layer - Sharding"
        D1[Redis Cluster<br/>Session Data]
        D2[PostgreSQL<br/>Read Replicas]
        D3[Object Storage<br/>Distributed]
    end

    subgraph "Message Queue - Partitioning"
        M1[Kafka Partitions<br/>Video Streams]
        M2[Redis Streams<br/>Real-time Events]
    end

    LB1 --> S1
    LB2 --> S2
    LB3 --> S3
    LB1 --> S4

    S1 --> D1
    S2 --> D2
    S3 --> D3
    S4 --> M1

    M1 --> M2
```

#### 11.1.2 ìë™ ìŠ¤ì¼€ì¼ë§ êµ¬í˜„

```python
from kubernetes import client, config
from typing import Dict, List
import asyncio
import numpy as np

class AutoScalingManager:
    """ìë™ ìŠ¤ì¼€ì¼ë§ ê´€ë¦¬ì"""

    def __init__(self, kube_config_path: str = None):
        if kube_config_path:
            config.load_kube_config(kube_config_path)
        else:
            config.load_incluster_config()

        self.apps_v1 = client.AppsV1Api()
        self.custom_objects_api = client.CustomObjectsApi()

        self.scaling_policies = {
            'pose-estimation': {
                'min_replicas': 3,
                'max_replicas': 20,
                'target_cpu': 70,
                'target_memory': 80,
                'scale_up_threshold': 0.8,
                'scale_down_threshold': 0.3
            },
            'classification': {
                'min_replicas': 3,
                'max_replicas': 25,
                'target_cpu': 75,
                'target_gpu': 80,
                'scale_up_threshold': 0.85,
                'scale_down_threshold': 0.25
            }
        }

    async def monitor_and_scale(self):
        """ëª¨ë‹ˆí„°ë§ ë° ìŠ¤ì¼€ì¼ë§ ë£¨í”„"""
        while True:
            for service_name, policy in self.scaling_policies.items():
                try:
                    current_metrics = await self._get_service_metrics(service_name)
                    scaling_decision = self._calculate_scaling_decision(
                        service_name, current_metrics, policy
                    )

                    if scaling_decision['action'] != 'none':
                        await self._execute_scaling(service_name, scaling_decision)

                except Exception as e:
                    logger.error(f"Scaling error for {service_name}: {e}")

            await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬

    async def _get_service_metrics(self, service_name: str) -> Dict[str, float]:
        """ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        # Prometheusì—ì„œ ë©”íŠ¸ë¦­ ì¡°íšŒ
        query_params = {
            'cpu_usage': f'avg(rate(container_cpu_usage_seconds_total{{pod=~"{service_name}-.*"}}[5m]))',
            'memory_usage': f'avg(container_memory_working_set_bytes{{pod=~"{service_name}-.*"}})',
            'gpu_usage': f'avg(nvidia_gpu_utilization_gpu{{pod=~"{service_name}-.*"}})',
            'request_rate': f'sum(rate(http_requests_total{{service="{service_name}"}}[5m]))',
            'response_time': f'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{{service="{service_name}"}}[5m]))'
        }

        metrics = {}
        for metric_name, query in query_params.items():
            try:
                result = await self._query_prometheus(query)
                metrics[metric_name] = float(result[0]['value'][1]) if result else 0.0
            except:
                metrics[metric_name] = 0.0

        return metrics

    def _calculate_scaling_decision(self, service_name: str,
                                  metrics: Dict[str, float],
                                  policy: Dict) -> Dict[str, any]:
        """ìŠ¤ì¼€ì¼ë§ ê²°ì • ê³„ì‚°"""
        current_replicas = self._get_current_replicas(service_name)

        # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ê¸°ë°˜ ìŠ¤ì¼€ì¼ íŒ©í„° ê³„ì‚°
        scale_factors = []

        if 'target_cpu' in policy:
            cpu_factor = metrics['cpu_usage'] / (policy['target_cpu'] / 100)
            scale_factors.append(cpu_factor)

        if 'target_memory' in policy:
            memory_factor = metrics['memory_usage'] / (policy['target_memory'] / 100)
            scale_factors.append(memory_factor)

        if 'target_gpu' in policy and metrics['gpu_usage'] > 0:
            gpu_factor = metrics['gpu_usage'] / (policy['target_gpu'] / 100)
            scale_factors.append(gpu_factor)

        if not scale_factors:
            return {'action': 'none'}

        avg_scale_factor = np.mean(scale_factors)
        desired_replicas = int(current_replicas * avg_scale_factor)

        # ì •ì±… ì œì•½ ì ìš©
        desired_replicas = max(policy['min_replicas'],
                              min(policy['max_replicas'], desired_replicas))

        # ìŠ¤ì¼€ì¼ë§ ì„ê³„ê°’ í™•ì¸
        if desired_replicas > current_replicas:
            if avg_scale_factor > policy['scale_up_threshold']:
                return {
                    'action': 'scale_up',
                    'current_replicas': current_replicas,
                    'desired_replicas': desired_replicas,
                    'scale_factor': avg_scale_factor
                }
        elif desired_replicas < current_replicas:
            if avg_scale_factor < policy['scale_down_threshold']:
                return {
                    'action': 'scale_down',
                    'current_replicas': current_replicas,
                    'desired_replicas': desired_replicas,
                    'scale_factor': avg_scale_factor
                }

        return {'action': 'none'}

    async def _execute_scaling(self, service_name: str, decision: Dict):
        """ìŠ¤ì¼€ì¼ë§ ì‹¤í–‰"""
        try:
            # Deployment ì—…ë°ì´íŠ¸
            body = {'spec': {'replicas': decision['desired_replicas']}}

            await self.apps_v1.patch_namespaced_deployment_scale(
                name=f"{service_name}-deployment",
                namespace="recognizer",
                body=body
            )

            logger.info(
                f"Scaled {service_name} from {decision['current_replicas']} "
                f"to {decision['desired_replicas']} replicas"
            )

        except Exception as e:
            logger.error(f"Failed to scale {service_name}: {e}")

class ServiceMeshManager:
    """ì„œë¹„ìŠ¤ ë©”ì‹œ ê´€ë¦¬ì"""

    def __init__(self):
        self.traffic_policies = {
            'canary_deployment': {
                'stable_weight': 90,
                'canary_weight': 10,
                'success_threshold': 0.99,
                'latency_threshold': 500  # ms
            },
            'circuit_breaker': {
                'failure_threshold': 5,
                'recovery_timeout': 30,
                'success_threshold': 3
            },
            'retry_policy': {
                'max_retries': 3,
                'timeout': '10s',
                'backoff': 'exponential'
            }
        }

    async def configure_traffic_splitting(self, service_name: str,
                                        stable_version: str,
                                        canary_version: str):
        """íŠ¸ë˜í”½ ë¶„í•  ì„¤ì •"""
        virtual_service = {
            'apiVersion': 'networking.istio.io/v1beta1',
            'kind': 'VirtualService',
            'metadata': {
                'name': f'{service_name}-vs',
                'namespace': 'recognizer'
            },
            'spec': {
                'hosts': [service_name],
                'http': [{
                    'match': [{'headers': {'canary': {'exact': 'true'}}}],
                    'route': [{'destination': {
                        'host': service_name,
                        'subset': canary_version
                    }}]
                }, {
                    'route': [
                        {
                            'destination': {
                                'host': service_name,
                                'subset': stable_version
                            },
                            'weight': self.traffic_policies['canary_deployment']['stable_weight']
                        },
                        {
                            'destination': {
                                'host': service_name,
                                'subset': canary_version
                            },
                            'weight': self.traffic_policies['canary_deployment']['canary_weight']
                        }
                    ]
                }]
            }
        }

        await self._apply_istio_config(virtual_service)

    async def setup_circuit_breaker(self, service_name: str):
        """ì„œí‚· ë¸Œë ˆì´ì»¤ ì„¤ì •"""
        destination_rule = {
            'apiVersion': 'networking.istio.io/v1beta1',
            'kind': 'DestinationRule',
            'metadata': {
                'name': f'{service_name}-dr',
                'namespace': 'recognizer'
            },
            'spec': {
                'host': service_name,
                'trafficPolicy': {
                    'connectionPool': {
                        'tcp': {'maxConnections': 100},
                        'http': {
                            'http1MaxPendingRequests': 10,
                            'maxRequestsPerConnection': 10
                        }
                    },
                    'outlierDetection': {
                        'consecutiveErrors': self.traffic_policies['circuit_breaker']['failure_threshold'],
                        'interval': '30s',
                        'baseEjectionTime': f"{self.traffic_policies['circuit_breaker']['recovery_timeout']}s",
                        'maxEjectionPercent': 50
                    }
                }
            }
        }

        await self._apply_istio_config(destination_rule)
```

### 11.2 ìˆ˜ì§ì  í™•ì¥ ë° ë¦¬ì†ŒìŠ¤ ìµœì í™”

#### 11.2.1 ì ì‘í˜• ë¦¬ì†ŒìŠ¤ ê´€ë¦¬

```python
class AdaptiveResourceManager:
    """ì ì‘í˜• ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì"""

    def __init__(self):
        self.resource_profiles = {
            'low_load': {
                'cpu_limit': '2',
                'memory_limit': '4Gi',
                'gpu_memory_fraction': 0.5,
                'batch_size': 4,
                'worker_threads': 2
            },
            'medium_load': {
                'cpu_limit': '4',
                'memory_limit': '8Gi',
                'gpu_memory_fraction': 0.7,
                'batch_size': 8,
                'worker_threads': 4
            },
            'high_load': {
                'cpu_limit': '8',
                'memory_limit': '16Gi',
                'gpu_memory_fraction': 0.9,
                'batch_size': 16,
                'worker_threads': 8
            }
        }

    async def optimize_resources(self, service_metrics: Dict[str, float]):
        """ë¦¬ì†ŒìŠ¤ ìµœì í™”"""
        current_load = self._classify_load_level(service_metrics)
        optimal_profile = self.resource_profiles[current_load]

        # ë™ì  ë¦¬ì†ŒìŠ¤ ì¡°ì •
        await self._adjust_resources(optimal_profile)

        # ì• í”Œë¦¬ì¼€ì´ì…˜ íŒŒë¼ë¯¸í„° ì¡°ì •
        await self._adjust_application_params(optimal_profile)

    def _classify_load_level(self, metrics: Dict[str, float]) -> str:
        """ë¶€í•˜ ìˆ˜ì¤€ ë¶„ë¥˜"""
        cpu_usage = metrics.get('cpu_usage', 0)
        memory_usage = metrics.get('memory_usage', 0)
        request_rate = metrics.get('request_rate', 0)

        # ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¶€í•˜ ì ìˆ˜ ê³„ì‚°
        load_score = (
            cpu_usage * 0.4 +
            memory_usage * 0.3 +
            min(request_rate / 100, 1.0) * 0.3
        )

        if load_score > 0.8:
            return 'high_load'
        elif load_score > 0.5:
            return 'medium_load'
        else:
            return 'low_load'

    async def _adjust_resources(self, profile: Dict[str, any]):
        """Kubernetes ë¦¬ì†ŒìŠ¤ ì¡°ì •"""
        # VPA (Vertical Pod Autoscaler) ì„¤ì • ì—…ë°ì´íŠ¸
        vpa_config = {
            'apiVersion': 'autoscaling.k8s.io/v1',
            'kind': 'VerticalPodAutoscaler',
            'metadata': {
                'name': 'recognizer-vpa',
                'namespace': 'recognizer'
            },
            'spec': {
                'targetRef': {
                    'apiVersion': 'apps/v1',
                    'kind': 'Deployment',
                    'name': 'recognizer-api'
                },
                'updatePolicy': {
                    'updateMode': 'Auto'
                },
                'resourcePolicy': {
                    'containerPolicies': [{
                        'containerName': 'recognizer-api',
                        'maxAllowed': {
                            'cpu': profile['cpu_limit'],
                            'memory': profile['memory_limit']
                        }
                    }]
                }
            }
        }

        await self._apply_kubernetes_config(vpa_config)

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™”ê¸°"""

    def __init__(self):
        self.memory_pools = {
            'frame_buffer': MemoryPool(size_mb=100),
            'tensor_cache': MemoryPool(size_mb=200),
            'result_buffer': MemoryPool(size_mb=50)
        }

    def optimize_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”"""
        # 1. ë¯¸ì‚¬ìš© ë©”ëª¨ë¦¬ í’€ ì •ë¦¬
        self._cleanup_unused_pools()

        # 2. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”
        self._optimize_garbage_collection()

        # 3. ìºì‹œ í¬ê¸° ë™ì  ì¡°ì •
        self._adjust_cache_sizes()

    def _cleanup_unused_pools(self):
        """ë¯¸ì‚¬ìš© ë©”ëª¨ë¦¬ í’€ ì •ë¦¬"""
        for pool_name, pool in self.memory_pools.items():
            if pool.usage_ratio() < 0.1:  # ì‚¬ìš©ë¥  10% ë¯¸ë§Œ
                pool.shrink(factor=0.5)

    def _optimize_garbage_collection(self):
        """ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”"""
        import gc

        # ì„¸ëŒ€ë³„ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„ê³„ê°’ ì¡°ì •
        current_thresholds = gc.get_threshold()
        optimized_thresholds = (
            current_thresholds[0],
            current_thresholds[1] // 2,  # ë” ìì£¼ ìˆ˜ì§‘
            current_thresholds[2] // 2
        )
        gc.set_threshold(*optimized_thresholds)

        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        logger.info(f"Garbage collection freed {collected} objects")

class GPUResourceManager:
    """GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì"""

    def __init__(self):
        self.gpu_schedulers = {
            'pose_estimation': GPUScheduler(priority=1),
            'classification': GPUScheduler(priority=2),
            'tracking': GPUScheduler(priority=3)
        }

    async def optimize_gpu_allocation(self):
        """GPU í• ë‹¹ ìµœì í™”"""
        # GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§
        gpu_metrics = self._get_gpu_metrics()

        # ë™ì  ë©”ëª¨ë¦¬ ë¶„í• 
        await self._rebalance_gpu_memory(gpu_metrics)

        # ì‘ì—… í ìµœì í™”
        await self._optimize_work_queues()

    def _get_gpu_metrics(self) -> Dict[str, float]:
        """GPU ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        import GPUtil

        gpus = GPUtil.getGPUs()
        metrics = {}

        for i, gpu in enumerate(gpus):
            metrics[f'gpu_{i}'] = {
                'utilization': gpu.load * 100,
                'memory_used': gpu.memoryUsed,
                'memory_total': gpu.memoryTotal,
                'temperature': gpu.temperature
            }

        return metrics

    async def _rebalance_gpu_memory(self, gpu_metrics: Dict):
        """GPU ë©”ëª¨ë¦¬ ì¬ë¶„í• """
        for gpu_id, metrics in gpu_metrics.items():
            memory_usage = metrics['memory_used'] / metrics['memory_total']

            if memory_usage > 0.9:  # 90% ì´ìƒ ì‚¬ìš© ì‹œ
                # ë©”ëª¨ë¦¬ ì••ë°• ì™„í™”
                await self._reduce_batch_sizes(gpu_id)
                await self._enable_gradient_checkpointing(gpu_id)
            elif memory_usage < 0.5:  # 50% ë¯¸ë§Œ ì‚¬ìš© ì‹œ
                # ë©”ëª¨ë¦¬ í™œìš©ë„ í–¥ìƒ
                await self._increase_batch_sizes(gpu_id)
                await self._enable_model_parallelism(gpu_id)
```

### 11.3 ìœ ì§€ë³´ìˆ˜ì„± ì„¤ê³„

#### 11.3.1 ëª¨ë“ˆí™” ë° í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Type
import importlib
import inspect

class PluginInterface(ABC):
    """í”ŒëŸ¬ê·¸ì¸ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def initialize(self, config: Dict[str, Any]) -> None:
        """í”ŒëŸ¬ê·¸ì¸ ì´ˆê¸°í™”"""
        pass

    @abstractmethod
    def execute(self, input_data: Any) -> Any:
        """í”ŒëŸ¬ê·¸ì¸ ì‹¤í–‰"""
        pass

    @abstractmethod
    def cleanup(self) -> None:
        """í”ŒëŸ¬ê·¸ì¸ ì •ë¦¬"""
        pass

    @property
    @abstractmethod
    def name(self) -> str:
        """í”ŒëŸ¬ê·¸ì¸ ì´ë¦„"""
        pass

    @property
    @abstractmethod
    def version(self) -> str:
        """í”ŒëŸ¬ê·¸ì¸ ë²„ì „"""
        pass

class PoseEstimatorPlugin(PluginInterface):
    """í¬ì¦ˆ ì¶”ì •ê¸° í”ŒëŸ¬ê·¸ì¸ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def estimate_poses(self, frame: np.ndarray) -> List[Person]:
        """í¬ì¦ˆ ì¶”ì • ìˆ˜í–‰"""
        pass

class ActionClassifierPlugin(PluginInterface):
    """ë™ì‘ ë¶„ë¥˜ê¸° í”ŒëŸ¬ê·¸ì¸ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def classify_action(self, window_data: WindowData) -> ClassificationResult:
        """ë™ì‘ ë¶„ë¥˜ ìˆ˜í–‰"""
        pass

class PluginManager:
    """í”ŒëŸ¬ê·¸ì¸ ê´€ë¦¬ì"""

    def __init__(self):
        self.plugins: Dict[str, PluginInterface] = {}
        self.plugin_configs: Dict[str, Dict] = {}
        self.plugin_metadata: Dict[str, Dict] = {}

    def register_plugin(self, plugin_class: Type[PluginInterface],
                       config: Dict[str, Any] = None) -> None:
        """í”ŒëŸ¬ê·¸ì¸ ë“±ë¡"""
        try:
            # í”ŒëŸ¬ê·¸ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            plugin_instance = plugin_class()

            # í”ŒëŸ¬ê·¸ì¸ ê²€ì¦
            self._validate_plugin(plugin_instance)

            # í”ŒëŸ¬ê·¸ì¸ ì´ˆê¸°í™”
            plugin_instance.initialize(config or {})

            # ë“±ë¡
            plugin_name = plugin_instance.name
            self.plugins[plugin_name] = plugin_instance
            self.plugin_configs[plugin_name] = config
            self.plugin_metadata[plugin_name] = {
                'class': plugin_class.__name__,
                'module': plugin_class.__module__,
                'version': plugin_instance.version,
                'registered_at': time.time()
            }

            logger.info(f"Plugin '{plugin_name}' registered successfully")

        except Exception as e:
            logger.error(f"Failed to register plugin {plugin_class.__name__}: {e}")
            raise

    def unregister_plugin(self, plugin_name: str) -> None:
        """í”ŒëŸ¬ê·¸ì¸ ë“±ë¡ í•´ì œ"""
        if plugin_name in self.plugins:
            try:
                self.plugins[plugin_name].cleanup()
                del self.plugins[plugin_name]
                del self.plugin_configs[plugin_name]
                del self.plugin_metadata[plugin_name]

                logger.info(f"Plugin '{plugin_name}' unregistered successfully")

            except Exception as e:
                logger.error(f"Failed to unregister plugin {plugin_name}: {e}")

    def get_plugin(self, plugin_name: str) -> PluginInterface:
        """í”ŒëŸ¬ê·¸ì¸ ì¡°íšŒ"""
        if plugin_name not in self.plugins:
            raise ValueError(f"Plugin '{plugin_name}' not found")

        return self.plugins[plugin_name]

    def list_plugins(self) -> Dict[str, Dict]:
        """í”ŒëŸ¬ê·¸ì¸ ëª©ë¡ ì¡°íšŒ"""
        return self.plugin_metadata.copy()

    def reload_plugin(self, plugin_name: str) -> None:
        """í”ŒëŸ¬ê·¸ì¸ ì¬ë¡œë“œ"""
        if plugin_name not in self.plugins:
            raise ValueError(f"Plugin '{plugin_name}' not found")

        # ê¸°ì¡´ í”ŒëŸ¬ê·¸ì¸ ì •ë³´ ë°±ì—…
        metadata = self.plugin_metadata[plugin_name]
        config = self.plugin_configs[plugin_name]

        # í”ŒëŸ¬ê·¸ì¸ ì–¸ë¡œë“œ
        self.unregister_plugin(plugin_name)

        # ëª¨ë“ˆ ì¬ë¡œë“œ
        module = importlib.import_module(metadata['module'])
        importlib.reload(module)

        # í”ŒëŸ¬ê·¸ì¸ í´ë˜ìŠ¤ ì¬ë¡œë“œ
        plugin_class = getattr(module, metadata['class'])

        # í”ŒëŸ¬ê·¸ì¸ ì¬ë“±ë¡
        self.register_plugin(plugin_class, config)

    def _validate_plugin(self, plugin: PluginInterface) -> None:
        """í”ŒëŸ¬ê·¸ì¸ ê²€ì¦"""
        # í•„ìˆ˜ ë©”ì„œë“œ êµ¬í˜„ í™•ì¸
        required_methods = ['initialize', 'execute', 'cleanup']
        for method_name in required_methods:
            if not hasattr(plugin, method_name):
                raise ValueError(f"Plugin must implement {method_name} method")

        # í”ŒëŸ¬ê·¸ì¸ ì´ë¦„ ë° ë²„ì „ í™•ì¸
        if not plugin.name or not plugin.version:
            raise ValueError("Plugin must have name and version")

class ConfigurationManager:
    """ì„¤ì • ê´€ë¦¬ì"""

    def __init__(self):
        self.configs: Dict[str, Any] = {}
        self.config_watchers: Dict[str, List[Callable]] = {}

    def load_config(self, config_path: str) -> Dict[str, Any]:
        """ì„¤ì • ë¡œë“œ"""
        import yaml

        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)

        # í™˜ê²½ ë³€ìˆ˜ ì¹˜í™˜
        config = self._substitute_env_vars(config)

        # ì„¤ì • ê²€ì¦
        self._validate_config(config)

        self.configs.update(config)
        return config

    def get_config(self, key: str, default: Any = None) -> Any:
        """ì„¤ì • ê°’ ì¡°íšŒ"""
        keys = key.split('.')
        value = self.configs

        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default

        return value

    def update_config(self, key: str, value: Any) -> None:
        """ì„¤ì • ê°’ ì—…ë°ì´íŠ¸"""
        keys = key.split('.')
        config = self.configs

        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            config = config[k]

        old_value = config.get(keys[-1])
        config[keys[-1]] = value

        # ì„¤ì • ë³€ê²½ ì•Œë¦¼
        self._notify_config_change(key, old_value, value)

    def watch_config(self, key: str, callback: Callable[[str, Any, Any], None]) -> None:
        """ì„¤ì • ë³€ê²½ ê°ì‹œ"""
        if key not in self.config_watchers:
            self.config_watchers[key] = []

        self.config_watchers[key].append(callback)

    def _notify_config_change(self, key: str, old_value: Any, new_value: Any) -> None:
        """ì„¤ì • ë³€ê²½ ì•Œë¦¼"""
        if key in self.config_watchers:
            for callback in self.config_watchers[key]:
                try:
                    callback(key, old_value, new_value)
                except Exception as e:
                    logger.error(f"Config watcher error: {e}")

    def _substitute_env_vars(self, config: Dict) -> Dict:
        """í™˜ê²½ ë³€ìˆ˜ ì¹˜í™˜"""
        import os
        import re

        def substitute_value(value):
            if isinstance(value, str):
                # ${VAR_NAME} ë˜ëŠ” $VAR_NAME íŒ¨í„´ ì°¾ê¸°
                pattern = r'\$\{([^}]+)\}|\$([A-Za-z_][A-Za-z0-9_]*)'

                def replace_match(match):
                    var_name = match.group(1) or match.group(2)
                    return os.getenv(var_name, match.group(0))

                return re.sub(pattern, replace_match, value)
            elif isinstance(value, dict):
                return {k: substitute_value(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [substitute_value(item) for item in value]
            else:
                return value

        return substitute_value(config)

class VersionManager:
    """ë²„ì „ ê´€ë¦¬ì"""

    def __init__(self):
        self.version_history: List[Dict] = []
        self.current_version = "2.0.0"

    def create_migration_plan(self, from_version: str, to_version: str) -> List[Dict]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš ìƒì„±"""
        migration_steps = []

        # ë²„ì „ë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ë‹¨ê³„ ì •ì˜
        version_migrations = {
            "1.0.0": {
                "2.0.0": [
                    {
                        "type": "schema_migration",
                        "description": "Update PKL data structure",
                        "script": "migrations/v1_to_v2_schema.py"
                    },
                    {
                        "type": "config_migration",
                        "description": "Migrate configuration format",
                        "script": "migrations/v1_to_v2_config.py"
                    },
                    {
                        "type": "model_migration",
                        "description": "Update model format",
                        "script": "migrations/v1_to_v2_models.py"
                    }
                ]
            }
        }

        if from_version in version_migrations:
            if to_version in version_migrations[from_version]:
                migration_steps = version_migrations[from_version][to_version]

        return migration_steps

    async def execute_migration(self, migration_steps: List[Dict]) -> bool:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        try:
            for step in migration_steps:
                logger.info(f"Executing migration: {step['description']}")

                # ë°±ì—… ìƒì„±
                backup_id = await self._create_backup()

                try:
                    # ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
                    await self._execute_migration_script(step['script'])

                    # ê²€ì¦
                    if await self._validate_migration_step(step):
                        logger.info(f"Migration step completed: {step['description']}")
                    else:
                        raise Exception(f"Migration validation failed: {step['description']}")

                except Exception as e:
                    # ë¡¤ë°±
                    logger.error(f"Migration failed: {e}")
                    await self._rollback_to_backup(backup_id)
                    return False

            return True

        except Exception as e:
            logger.error(f"Migration execution failed: {e}")
            return False
```

---

## 12. ìœ„í—˜ ê´€ë¦¬ ë° ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš

### 12.1 ìœ„í—˜ í‰ê°€ ë° ëŒ€ì‘ ì „ëµ

#### 12.1.1 ìœ„í—˜ ë§¤íŠ¸ë¦­ìŠ¤

```mermaid
graph TD
    subgraph "Risk Categories"
        RC1[Technical Risks<br/>High Impact, Medium Probability]
        RC2[Operational Risks<br/>Medium Impact, High Probability]
        RC3[Security Risks<br/>High Impact, Low Probability]
        RC4[Performance Risks<br/>Medium Impact, Medium Probability]
        RC5[Data Risks<br/>High Impact, Low Probability]
    end

    subgraph "Mitigation Strategies"
        MS1[Redundancy & Backup]
        MS2[Monitoring & Alerting]
        MS3[Security Hardening]
        MS4[Performance Optimization]
        MS5[Data Protection]
    end

    subgraph "Contingency Plans"
        CP1[Disaster Recovery]
        CP2[Incident Response]
        CP3[Business Continuity]
        CP4[Emergency Procedures]
    end

    RC1 --> MS1
    RC2 --> MS2
    RC3 --> MS3
    RC4 --> MS4
    RC5 --> MS5

    MS1 --> CP1
    MS2 --> CP2
    MS3 --> CP3
    MS4 --> CP4
    MS5 --> CP1
```

#### 12.1.2 ìœ„í—˜ ê´€ë¦¬ í”„ë ˆì„ì›Œí¬

```python
from enum import Enum
from dataclasses import dataclass
from typing import List, Dict, Callable
import asyncio

class RiskLevel(Enum):
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

class RiskCategory(Enum):
    TECHNICAL = "technical"
    OPERATIONAL = "operational"
    SECURITY = "security"
    PERFORMANCE = "performance"
    DATA = "data"
    BUSINESS = "business"

@dataclass
class Risk:
    id: str
    name: str
    description: str
    category: RiskCategory
    probability: float  # 0.0 ~ 1.0
    impact: RiskLevel
    current_level: RiskLevel
    mitigation_strategies: List[str]
    contingency_plans: List[str]
    owner: str
    created_at: float
    updated_at: float

class RiskManager:
    """ìœ„í—˜ ê´€ë¦¬ì"""

    def __init__(self):
        self.risks: Dict[str, Risk] = {}
        self.risk_monitors: Dict[str, Callable] = {}
        self.mitigation_actions: Dict[str, Callable] = {}
        self.risk_thresholds = {
            RiskLevel.LOW: 0.2,
            RiskLevel.MEDIUM: 0.5,
            RiskLevel.HIGH: 0.8,
            RiskLevel.CRITICAL: 0.95
        }

    def register_risk(self, risk: Risk) -> None:
        """ìœ„í—˜ ë“±ë¡"""
        self.risks[risk.id] = risk
        logger.info(f"Risk registered: {risk.name} ({risk.id})")

    def assess_risk_level(self, probability: float, impact: RiskLevel) -> RiskLevel:
        """ìœ„í—˜ ìˆ˜ì¤€ í‰ê°€"""
        # í™•ë¥ ê³¼ ì˜í–¥ë„ë¥¼ ê²°í•©í•œ ìœ„í—˜ ì ìˆ˜ ê³„ì‚°
        impact_weights = {
            RiskLevel.LOW: 0.25,
            RiskLevel.MEDIUM: 0.5,
            RiskLevel.HIGH: 0.75,
            RiskLevel.CRITICAL: 1.0
        }

        risk_score = probability * impact_weights[impact]

        for level, threshold in sorted(self.risk_thresholds.items(),
                                     key=lambda x: x[1], reverse=True):
            if risk_score >= threshold:
                return level

        return RiskLevel.LOW

    async def monitor_risks(self):
        """ìœ„í—˜ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                for risk_id, risk in self.risks.items():
                    if risk_id in self.risk_monitors:
                        current_probability = await self.risk_monitors[risk_id]()
                        new_level = self.assess_risk_level(current_probability, risk.impact)

                        if new_level != risk.current_level:
                            await self._handle_risk_level_change(risk, new_level)

                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤ ëª¨ë‹ˆí„°ë§

            except Exception as e:
                logger.error(f"Risk monitoring error: {e}")
                await asyncio.sleep(60)

    async def _handle_risk_level_change(self, risk: Risk, new_level: RiskLevel):
        """ìœ„í—˜ ìˆ˜ì¤€ ë³€í™” ì²˜ë¦¬"""
        old_level = risk.current_level
        risk.current_level = new_level
        risk.updated_at = time.time()

        logger.warning(
            f"Risk level changed: {risk.name} from {old_level.name} to {new_level.name}"
        )

        # ìœ„í—˜ ìˆ˜ì¤€ì— ë”°ë¥¸ ëŒ€ì‘ ì¡°ì¹˜
        if new_level in [RiskLevel.HIGH, RiskLevel.CRITICAL]:
            await self._trigger_mitigation_actions(risk)
            await self._send_risk_alert(risk, old_level, new_level)

    async def _trigger_mitigation_actions(self, risk: Risk):
        """ì™„í™” ì¡°ì¹˜ ì‹¤í–‰"""
        for strategy in risk.mitigation_strategies:
            if strategy in self.mitigation_actions:
                try:
                    await self.mitigation_actions[strategy](risk)
                    logger.info(f"Mitigation action executed: {strategy} for {risk.name}")
                except Exception as e:
                    logger.error(f"Mitigation action failed: {strategy} - {e}")

# êµ¬ì²´ì ì¸ ìœ„í—˜ ëª¨ë‹ˆí„° êµ¬í˜„
class TechnicalRiskMonitor:
    """ê¸°ìˆ ì  ìœ„í—˜ ëª¨ë‹ˆí„°"""

    def __init__(self, performance_monitor: PerformanceMonitor):
        self.performance_monitor = performance_monitor

    async def monitor_gpu_failure_risk(self) -> float:
        """GPU ì¥ì•  ìœ„í—˜ ëª¨ë‹ˆí„°ë§"""
        gpu_metrics = self.performance_monitor.get_gpu_metrics()

        risk_factors = []

        for gpu_id, metrics in gpu_metrics.items():
            # ì˜¨ë„ ê¸°ë°˜ ìœ„í—˜ë„
            temp_risk = min(metrics['temperature'] / 85.0, 1.0)  # 85Â°C ê¸°ì¤€
            risk_factors.append(temp_risk)

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ê¸°ë°˜ ìœ„í—˜ë„
            memory_risk = metrics['memory_usage'] / 100.0
            risk_factors.append(memory_risk)

            # ì‚¬ìš©ë¥  ê¸°ë°˜ ìœ„í—˜ë„
            util_risk = metrics['utilization'] / 100.0
            risk_factors.append(util_risk)

        return min(np.mean(risk_factors), 1.0)

    async def monitor_model_accuracy_degradation_risk(self) -> float:
        """ëª¨ë¸ ì •í™•ë„ ì €í•˜ ìœ„í—˜ ëª¨ë‹ˆí„°ë§"""
        # ìµœê·¼ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ
        recent_accuracy = await self._get_recent_model_accuracy()
        baseline_accuracy = 0.90  # ê¸°ì¤€ ì •í™•ë„

        if recent_accuracy < baseline_accuracy:
            degradation_ratio = (baseline_accuracy - recent_accuracy) / baseline_accuracy
            return min(degradation_ratio * 2, 1.0)  # 50% ì €í•˜ ì‹œ ìµœëŒ€ ìœ„í—˜

        return 0.0

class OperationalRiskMonitor:
    """ìš´ì˜ ìœ„í—˜ ëª¨ë‹ˆí„°"""

    async def monitor_system_overload_risk(self) -> float:
        """ì‹œìŠ¤í…œ ê³¼ë¶€í•˜ ìœ„í—˜ ëª¨ë‹ˆí„°ë§"""
        system_metrics = await self._get_system_metrics()

        # CPU, ë©”ëª¨ë¦¬, ë””ìŠ¤í¬ ì‚¬ìš©ë¥  ê¸°ë°˜ ìœ„í—˜ë„
        cpu_risk = system_metrics['cpu_usage'] / 100.0
        memory_risk = system_metrics['memory_usage'] / 100.0
        disk_risk = system_metrics['disk_usage'] / 100.0

        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        overload_risk = (cpu_risk * 0.4 + memory_risk * 0.4 + disk_risk * 0.2)

        return min(overload_risk, 1.0)

    async def monitor_dependency_failure_risk(self) -> float:
        """ì˜ì¡´ì„± ì¥ì•  ìœ„í—˜ ëª¨ë‹ˆí„°ë§"""
        # ì™¸ë¶€ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        dependencies = [
            'redis_cluster',
            'postgresql',
            'object_storage',
            'monitoring_stack'
        ]

        failed_dependencies = 0

        for dep in dependencies:
            if not await self._check_dependency_health(dep):
                failed_dependencies += 1

        return failed_dependencies / len(dependencies)

class DisasterRecoveryManager:
    """ì¬í•´ ë³µêµ¬ ê´€ë¦¬ì"""

    def __init__(self):
        self.recovery_procedures = {
            'data_center_failure': self._handle_data_center_failure,
            'database_corruption': self._handle_database_corruption,
            'model_corruption': self._handle_model_corruption,
            'security_breach': self._handle_security_breach
        }

        self.rto_targets = {  # Recovery Time Objective
            'critical_services': 3600,  # 1ì‹œê°„
            'data_services': 7200,      # 2ì‹œê°„
            'analytics_services': 14400  # 4ì‹œê°„
        }

        self.rpo_targets = {  # Recovery Point Objective
            'transactional_data': 300,   # 5ë¶„
            'analytics_data': 3600,      # 1ì‹œê°„
            'model_data': 86400          # 24ì‹œê°„
        }

    async def initiate_disaster_recovery(self, disaster_type: str) -> bool:
        """ì¬í•´ ë³µêµ¬ ì‹œì‘"""
        logger.critical(f"Initiating disaster recovery for: {disaster_type}")

        if disaster_type not in self.recovery_procedures:
            logger.error(f"No recovery procedure for disaster type: {disaster_type}")
            return False

        try:
            # ì¬í•´ ë³µêµ¬ ì ˆì°¨ ì‹¤í–‰
            recovery_success = await self.recovery_procedures[disaster_type]()

            if recovery_success:
                logger.info(f"Disaster recovery completed successfully for {disaster_type}")
                await self._verify_system_integrity()
            else:
                logger.error(f"Disaster recovery failed for {disaster_type}")
                await self._escalate_to_manual_recovery(disaster_type)

            return recovery_success

        except Exception as e:
            logger.error(f"Disaster recovery error for {disaster_type}: {e}")
            return False

    async def _handle_data_center_failure(self) -> bool:
        """ë°ì´í„°ì„¼í„° ì¥ì•  ì²˜ë¦¬"""
        # 1. íŠ¸ë˜í”½ì„ ë°±ì—… ë°ì´í„°ì„¼í„°ë¡œ ì „í™˜
        await self._switch_traffic_to_backup_dc()

        # 2. ë°ì´í„°ë² ì´ìŠ¤ í˜ì¼ì˜¤ë²„
        await self._failover_database()

        # 3. ì„œë¹„ìŠ¤ ì¬ì‹œì‘
        await self._restart_critical_services()

        # 4. ìƒíƒœ í™•ì¸
        return await self._verify_backup_dc_health()

    async def _handle_database_corruption(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ì†ìƒ ì²˜ë¦¬"""
        # 1. ì„œë¹„ìŠ¤ ì¤‘ì§€
        await self._stop_write_operations()

        # 2. ìµœì‹  ë°±ì—…ì—ì„œ ë³µì›
        await self._restore_from_latest_backup()

        # 3. íŠ¸ëœì­ì…˜ ë¡œê·¸ ë³µì›
        await self._restore_transaction_logs()

        # 4. ë¬´ê²°ì„± ê²€ì¦
        return await self._verify_data_integrity()

    async def _handle_model_corruption(self) -> bool:
        """ëª¨ë¸ ì†ìƒ ì²˜ë¦¬"""
        # 1. ì†ìƒëœ ëª¨ë¸ ê²©ë¦¬
        await self._quarantine_corrupted_models()

        # 2. ë°±ì—… ëª¨ë¸ë¡œ êµì²´
        await self._restore_models_from_backup()

        # 3. ëª¨ë¸ ê²€ì¦
        return await self._validate_restored_models()

class BusinessContinuityManager:
    """ì‚¬ì—… ì—°ì†ì„± ê´€ë¦¬ì"""

    def __init__(self):
        self.continuity_plans = {
            'pandemic': self._pandemic_continuity_plan,
            'natural_disaster': self._natural_disaster_plan,
            'cyber_attack': self._cyber_attack_plan,
            'supply_chain_disruption': self._supply_chain_plan
        }

    async def activate_continuity_plan(self, scenario: str) -> bool:
        """ì—°ì†ì„± ê³„íš í™œì„±í™”"""
        if scenario not in self.continuity_plans:
            logger.error(f"No continuity plan for scenario: {scenario}")
            return False

        try:
            logger.info(f"Activating business continuity plan: {scenario}")

            # ì—°ì†ì„± ê³„íš ì‹¤í–‰
            plan_result = await self.continuity_plans[scenario]()

            if plan_result:
                logger.info(f"Business continuity plan activated successfully: {scenario}")
                await self._monitor_continuity_effectiveness(scenario)
            else:
                logger.error(f"Business continuity plan failed: {scenario}")

            return plan_result

        except Exception as e:
            logger.error(f"Business continuity plan error: {e}")
            return False

    async def _pandemic_continuity_plan(self) -> bool:
        """íŒ¬ë°ë¯¹ ì—°ì†ì„± ê³„íš"""
        # ì›ê²© ìš´ì˜ ëª¨ë“œ í™œì„±í™”
        await self._enable_remote_operations()

        # í´ë¼ìš°ë“œ ë¦¬ì†ŒìŠ¤ í™•ì¥
        await self._scale_cloud_resources()

        # ë¹„ì ‘ì´‰ ì„œë¹„ìŠ¤ ê°•í™”
        await self._enhance_contactless_services()

        return True

    async def _cyber_attack_plan(self) -> bool:
        """ì‚¬ì´ë²„ ê³µê²© ëŒ€ì‘ ê³„íš"""
        # ì‹œìŠ¤í…œ ê²©ë¦¬
        await self._isolate_affected_systems()

        # ë³´ì•ˆ ê°•í™”
        await self._implement_emergency_security_measures()

        # ë°±ì—… ì‹œìŠ¤í…œ í™œì„±í™”
        await self._activate_backup_systems()

        return True
```

---

## ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„

### ì¢…í•© ë¬¸ì„œ ì™„ì„± í˜„í™©

Part 3ì—ì„œëŠ” Recognizer ì‹œìŠ¤í…œì˜ í’ˆì§ˆ ë³´ì¦, ë°°í¬/ìš´ì˜, í™•ì¥ì„±, ê·¸ë¦¬ê³  ìœ„í—˜ ê´€ë¦¬ì— ëŒ€í•œ í¬ê´„ì ì¸ ì„¤ê³„ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.

**ì „ì²´ 3ë¶€ì‘ ì™„ì„± ë‚´ìš©:**

**Part 1: ì‹œìŠ¤í…œ ê¸°ë°˜ ì„¤ê³„**
- ì‹œìŠ¤í…œ ê°œìš” ë° ìš”êµ¬ì‚¬í•­ ë¶„ì„
- ì „ì²´ ì•„í‚¤í…ì²˜ ë° ê¸°ìˆ  ìŠ¤íƒ ì„ ì •
- ì‹œìŠ¤í…œ ì œì•½ì‚¬í•­ ë° ì´í•´ê´€ê³„ì ë¶„ì„

**Part 2: ìƒì„¸ êµ¬í˜„ ì„¤ê³„**
- ëª¨ë“ˆë³„ ìƒì„¸ ì„¤ê³„ (í¬ì¦ˆ ì¶”ì •, ì¶”ì , ë¶„ë¥˜, ì´ë²¤íŠ¸ ê´€ë¦¬)
- ë°ì´í„° ì•„í‚¤í…ì²˜ ë° í”Œë¡œìš° ì„¤ê³„
- API ë° ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„
- ë³´ì•ˆ ë° ì„±ëŠ¥ ìµœì í™” ì„¤ê³„

**Part 3: ìš´ì˜ ë° ì§€ì†ê°€ëŠ¥ì„± ì„¤ê³„**
- í’ˆì§ˆ ë³´ì¦ ë° í…ŒìŠ¤íŠ¸ ì „ëµ
- ì»¨í…Œì´ë„ˆí™” ë° ë°°í¬ ìë™í™”
- í™•ì¥ì„± ë° ìœ ì§€ë³´ìˆ˜ ì„¤ê³„
- ìœ„í—˜ ê´€ë¦¬ ë° ì¬í•´ ë³µêµ¬ ê³„íš

### ì£¼ìš” ë‹¬ì„± ì„±ê³¼

1. **ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ì•„í‚¤í…ì²˜**: 24/7 ìš´ì˜ ê°€ëŠ¥í•œ í”„ë¡œë•ì…˜ ë ˆë²¨ ì„¤ê³„
2. **í¬ê´„ì  í’ˆì§ˆ ê´€ë¦¬**: ë‹¨ìœ„-í†µí•©-ì‹œìŠ¤í…œ-ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì „ëµ
3. **ì™„ì „ ìë™í™”**: CI/CD, GitOps, ëª¨ë‹ˆí„°ë§ í†µí•© ìš´ì˜
4. **í™•ì¥ì„± ë³´ì¥**: ìˆ˜í‰/ìˆ˜ì§ í™•ì¥, í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜
5. **ìœ„í—˜ ê´€ë¦¬**: ì˜ˆë°©-ê°ì§€-ëŒ€ì‘-ë³µêµ¬ ì „ì²´ ìƒëª…ì£¼ê¸°

### êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë¡œë“œë§µ

**Phase 1 (0-3ê°œì›”): í•µì‹¬ ì‹œìŠ¤í…œ êµ¬ì¶•**
- ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- ë‹¨ìœ„/í†µí•© í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•
- ê¸°ë³¸ ì»¨í…Œì´ë„ˆí™” ë° ë°°í¬

**Phase 2 (3-6ê°œì›”): í’ˆì§ˆ ë° ìš´ì˜ ê°•í™”**
- í¬ê´„ì  í…ŒìŠ¤íŠ¸ ìë™í™”
- ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ìµœì í™” ë° í™•ì¥ì„± êµ¬í˜„

**Phase 3 (6-12ê°œì›”): ê³ ê¸‰ ê¸°ëŠ¥ ë° ì•ˆì •ì„±**
- í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜ êµ¬í˜„
- ì¬í•´ ë³µêµ¬ ì‹œìŠ¤í…œ êµ¬ì¶•
- ì—”í„°í”„ë¼ì´ì¦ˆ ë³´ì•ˆ ê°•í™”


<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"content": "old version ë¬¸ì„œë“¤ ë¶„ì„í•˜ì—¬ ìˆ˜ì¤€ íŒŒì•…", "status": "completed", "activeForm": "old version ë¬¸ì„œë“¤ ë¶„ì„ ì™„ë£Œ"}, {"content": "UML ë‹¤ì´ì–´ê·¸ë¨ ìƒì„¸ ì—…ë°ì´íŠ¸", "status": "completed", "activeForm": "UML ë‹¤ì´ì–´ê·¸ë¨ ìƒì„¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ"}, {"content": "ì•„í‚¤í…ì²˜ ë¬¸ì„œ ìƒì„¸ ì—…ë°ì´íŠ¸", "status": "completed", "activeForm": "ì•„í‚¤í…ì²˜ ë¬¸ì„œ ìƒì„¸ ì—…ë°ì´íŠ¸ ì™„ë£Œ"}, {"content": "ëª¨ë“œë³„ API ë¬¸ì„œ ì‘ì„±", "status": "completed", "activeForm": "ëª¨ë“œë³„ API ë¬¸ì„œ ì‘ì„± ì™„ë£Œ"}, {"content": "í†µí•© ì†Œí”„íŠ¸ì›¨ì–´ ì„¤ê³„ì„œ SDD ìˆ˜ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸", "status": "completed", "activeForm": "í†µí•© SDD ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì™„ë£Œ"}]